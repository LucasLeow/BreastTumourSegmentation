{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2e602a2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Libraries -------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
    "import os\n",
    "import h5py\n",
    "import keras\n",
    "import loss\n",
    "import Helper\n",
    "import allMetrics\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import UNetModel_3D_simplified\n",
    "import matplotlib.pyplot as plt\n",
    "from keras import backend as K\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau, ModelCheckpoint, CSVLogger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "60a708f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"2\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "31c737e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_num = 1\n",
    "\n",
    "train_fileName = f\"/media/dro/JHSeagate/FYP/jh_fyp_work/3D_UNet/trials/numpyDatasets/numPyArrays/train_Scans/resampled_trainDS{ds_num}.hdf5\"\n",
    "train_maskfileName = f\"/media/dro/JHSeagate/FYP/jh_fyp_work/3D_UNet/trials/numpyDatasets/numPyArrays/train_Mask_Scans/resampled_trainmaskDS{ds_num}.hdf5\"\n",
    "\n",
    "train_DatasetName = f\"ResampledtrainScans_DataSet{ds_num}\"\n",
    "train_maskDatasetName = f\"ResampledtrainMaskScans_DataSet{ds_num}\"\n",
    "\n",
    "valid_fileName = f\"/media/dro/JHSeagate/FYP/jh_fyp_work/3D_UNet/trials/numpyDatasets/numPyArrays/valid_Scans/resampled_validDS{ds_num}.hdf5\"\n",
    "valid_maskfileName = f\"/media/dro/JHSeagate/FYP/jh_fyp_work/3D_UNet/trials/numpyDatasets/numPyArrays/valid_Mask_Scans/resampled_validmaskDS{ds_num}.hdf5\"\n",
    "\n",
    "valid_DatasetName = f\"ResampledvalidScans_DataSet{ds_num}\"\n",
    "valid_maskDatasetName = f\"ResampledvalidMaskScans_DataSet{ds_num}\"\n",
    "\n",
    "test_fileName = f\"/media/dro/JHSeagate/FYP/jh_fyp_work/3D_UNet/trials/numpyDatasets/numPyArrays/test_Scans/resampled_testDS{ds_num}.hdf5\"\n",
    "test_maskfileName = f\"/media/dro/JHSeagate/FYP/jh_fyp_work/3D_UNet/trials/numpyDatasets/numPyArrays/test_Mask_Scans/resampled_testmaskDS{ds_num}.hdf5\"\n",
    "\n",
    "test_DatasetName = f\"ResampledtestScans_DataSet{ds_num}\"\n",
    "test_maskDatasetName = f\"ResampledtestMaskScans_DataSet{ds_num}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1cd4eb0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "with h5py.File(train_fileName, 'r') as hf: # File Dir\n",
    "    train_array = hf[train_DatasetName][:]\n",
    "    \n",
    "with h5py.File(train_maskfileName,'r') as hf:\n",
    "    train_mask_array = hf[train_maskDatasetName][:]\n",
    "    \n",
    "with h5py.File(valid_fileName, 'r') as hf: # File Dir\n",
    "    valid_array = hf[valid_DatasetName][:]\n",
    "    \n",
    "with h5py.File(valid_maskfileName,'r') as hf:\n",
    "    valid_mask_array = hf[valid_maskDatasetName][:]\n",
    "    \n",
    "with h5py.File(test_fileName, 'r') as hf: # File Dir\n",
    "    test_array = hf[test_DatasetName][:]\n",
    "    \n",
    "with h5py.File(test_maskfileName,'r') as hf:\n",
    "    test_mask_array = hf[test_maskDatasetName][:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "062bb7eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(6543, 64, 64, 64)\n",
      "(6543, 64, 64, 64)\n",
      "--\n",
      "(1596, 64, 64, 64)\n",
      "(1596, 64, 64, 64)\n",
      "--\n",
      "(2403, 64, 64, 64)\n",
      "(2403, 64, 64, 64)\n"
     ]
    }
   ],
   "source": [
    "print(train_array.shape)\n",
    "print(train_mask_array.shape)\n",
    "print('--')\n",
    "print(valid_array.shape)\n",
    "print(valid_mask_array.shape)\n",
    "print('--')\n",
    "print(test_array.shape)\n",
    "print(test_mask_array.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "73b391f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_scan = np.concatenate((train_array,valid_array,test_array))\n",
    "train_mask_scan = np.concatenate((train_mask_array,valid_mask_array,test_mask_array))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "da8c8df7",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_array = np.expand_dims(train_scan, axis=4)\n",
    "train_mask_array = np.expand_dims(train_mask_scan, axis=4)\n",
    "\n",
    "valid_array = np.expand_dims(test_array, axis=4)\n",
    "valid_mask_array = np.expand_dims(test_mask_array, axis=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a0748d0f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10542, 64, 64, 64, 1)\n",
      "(10542, 64, 64, 64, 1)\n",
      "--\n",
      "(2403, 64, 64, 64, 1)\n",
      "(2403, 64, 64, 64, 1)\n"
     ]
    }
   ],
   "source": [
    "print(train_array.shape)\n",
    "print(train_mask_array.shape)\n",
    "print('--')\n",
    "print(valid_array.shape)\n",
    "print(valid_mask_array.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1851207d",
   "metadata": {},
   "outputs": [],
   "source": [
    "LR = 0.001\n",
    "opt = tf.keras.optimizers.Nadam(LR)\n",
    "\n",
    "input_shape = (64,64,64,1)\n",
    "num_class = 1\n",
    "\n",
    "metrics = [allMetrics.dice_coef]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9aaf3672",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-03-15 13:41:02.759869: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-03-15 13:41:03.258025: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1525] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 37831 MB memory:  -> device: 0, name: A100-SXM4-40GB, pci bus id: 0000:81:00.0, compute capability: 8.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sigmoid\n",
      "Model: \"U-Net\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_1 (InputLayer)           [(None, 64, 64, 64,  0           []                               \n",
      "                                 1)]                                                              \n",
      "                                                                                                  \n",
      " conv3d (Conv3D)                (None, 64, 64, 64,   896         ['input_1[0][0]']                \n",
      "                                32)                                                               \n",
      "                                                                                                  \n",
      " batch_normalization (BatchNorm  (None, 64, 64, 64,   128        ['conv3d[0][0]']                 \n",
      " alization)                     32)                                                               \n",
      "                                                                                                  \n",
      " leaky_re_lu (LeakyReLU)        (None, 64, 64, 64,   0           ['batch_normalization[0][0]']    \n",
      "                                32)                                                               \n",
      "                                                                                                  \n",
      " conv3d_1 (Conv3D)              (None, 64, 64, 64,   27680       ['leaky_re_lu[0][0]']            \n",
      "                                32)                                                               \n",
      "                                                                                                  \n",
      " batch_normalization_1 (BatchNo  (None, 64, 64, 64,   128        ['conv3d_1[0][0]']               \n",
      " rmalization)                   32)                                                               \n",
      "                                                                                                  \n",
      " leaky_re_lu_1 (LeakyReLU)      (None, 64, 64, 64,   0           ['batch_normalization_1[0][0]']  \n",
      "                                32)                                                               \n",
      "                                                                                                  \n",
      " max_pooling3d (MaxPooling3D)   (None, 32, 32, 32,   0           ['leaky_re_lu_1[0][0]']          \n",
      "                                32)                                                               \n",
      "                                                                                                  \n",
      " conv3d_2 (Conv3D)              (None, 32, 32, 32,   55360       ['max_pooling3d[0][0]']          \n",
      "                                64)                                                               \n",
      "                                                                                                  \n",
      " batch_normalization_2 (BatchNo  (None, 32, 32, 32,   256        ['conv3d_2[0][0]']               \n",
      " rmalization)                   64)                                                               \n",
      "                                                                                                  \n",
      " leaky_re_lu_2 (LeakyReLU)      (None, 32, 32, 32,   0           ['batch_normalization_2[0][0]']  \n",
      "                                64)                                                               \n",
      "                                                                                                  \n",
      " conv3d_3 (Conv3D)              (None, 32, 32, 32,   110656      ['leaky_re_lu_2[0][0]']          \n",
      "                                64)                                                               \n",
      "                                                                                                  \n",
      " batch_normalization_3 (BatchNo  (None, 32, 32, 32,   256        ['conv3d_3[0][0]']               \n",
      " rmalization)                   64)                                                               \n",
      "                                                                                                  \n",
      " leaky_re_lu_3 (LeakyReLU)      (None, 32, 32, 32,   0           ['batch_normalization_3[0][0]']  \n",
      "                                64)                                                               \n",
      "                                                                                                  \n",
      " max_pooling3d_1 (MaxPooling3D)  (None, 16, 16, 16,   0          ['leaky_re_lu_3[0][0]']          \n",
      "                                64)                                                               \n",
      "                                                                                                  \n",
      " conv3d_4 (Conv3D)              (None, 16, 16, 16,   221312      ['max_pooling3d_1[0][0]']        \n",
      "                                128)                                                              \n",
      "                                                                                                  \n",
      " batch_normalization_4 (BatchNo  (None, 16, 16, 16,   512        ['conv3d_4[0][0]']               \n",
      " rmalization)                   128)                                                              \n",
      "                                                                                                  \n",
      " leaky_re_lu_4 (LeakyReLU)      (None, 16, 16, 16,   0           ['batch_normalization_4[0][0]']  \n",
      "                                128)                                                              \n",
      "                                                                                                  \n",
      " conv3d_5 (Conv3D)              (None, 16, 16, 16,   442496      ['leaky_re_lu_4[0][0]']          \n",
      "                                128)                                                              \n",
      "                                                                                                  \n",
      " batch_normalization_5 (BatchNo  (None, 16, 16, 16,   512        ['conv3d_5[0][0]']               \n",
      " rmalization)                   128)                                                              \n",
      "                                                                                                  \n",
      " leaky_re_lu_5 (LeakyReLU)      (None, 16, 16, 16,   0           ['batch_normalization_5[0][0]']  \n",
      "                                128)                                                              \n",
      "                                                                                                  \n",
      " max_pooling3d_2 (MaxPooling3D)  (None, 8, 8, 8, 128  0          ['leaky_re_lu_5[0][0]']          \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " conv3d_6 (Conv3D)              (None, 8, 8, 8, 256  884992      ['max_pooling3d_2[0][0]']        \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " batch_normalization_6 (BatchNo  (None, 8, 8, 8, 256  1024       ['conv3d_6[0][0]']               \n",
      " rmalization)                   )                                                                 \n",
      "                                                                                                  \n",
      " leaky_re_lu_6 (LeakyReLU)      (None, 8, 8, 8, 256  0           ['batch_normalization_6[0][0]']  \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " conv3d_7 (Conv3D)              (None, 8, 8, 8, 256  1769728     ['leaky_re_lu_6[0][0]']          \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " batch_normalization_7 (BatchNo  (None, 8, 8, 8, 256  1024       ['conv3d_7[0][0]']               \n",
      " rmalization)                   )                                                                 \n",
      "                                                                                                  \n",
      " leaky_re_lu_7 (LeakyReLU)      (None, 8, 8, 8, 256  0           ['batch_normalization_7[0][0]']  \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " max_pooling3d_3 (MaxPooling3D)  (None, 4, 4, 4, 256  0          ['leaky_re_lu_7[0][0]']          \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " conv3d_8 (Conv3D)              (None, 4, 4, 4, 512  3539456     ['max_pooling3d_3[0][0]']        \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " batch_normalization_8 (BatchNo  (None, 4, 4, 4, 512  2048       ['conv3d_8[0][0]']               \n",
      " rmalization)                   )                                                                 \n",
      "                                                                                                  \n",
      " leaky_re_lu_8 (LeakyReLU)      (None, 4, 4, 4, 512  0           ['batch_normalization_8[0][0]']  \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " conv3d_9 (Conv3D)              (None, 4, 4, 4, 512  7078400     ['leaky_re_lu_8[0][0]']          \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " batch_normalization_9 (BatchNo  (None, 4, 4, 4, 512  2048       ['conv3d_9[0][0]']               \n",
      " rmalization)                   )                                                                 \n",
      "                                                                                                  \n",
      " leaky_re_lu_9 (LeakyReLU)      (None, 4, 4, 4, 512  0           ['batch_normalization_9[0][0]']  \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " conv3d_transpose (Conv3DTransp  (None, 8, 8, 8, 256  1048832    ['leaky_re_lu_9[0][0]']          \n",
      " ose)                           )                                                                 \n",
      "                                                                                                  \n",
      " concatenate (Concatenate)      (None, 8, 8, 8, 512  0           ['conv3d_transpose[0][0]',       \n",
      "                                )                                 'leaky_re_lu_7[0][0]']          \n",
      "                                                                                                  \n",
      " conv3d_10 (Conv3D)             (None, 8, 8, 8, 256  3539200     ['concatenate[0][0]']            \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " batch_normalization_10 (BatchN  (None, 8, 8, 8, 256  1024       ['conv3d_10[0][0]']              \n",
      " ormalization)                  )                                                                 \n",
      "                                                                                                  \n",
      " leaky_re_lu_10 (LeakyReLU)     (None, 8, 8, 8, 256  0           ['batch_normalization_10[0][0]'] \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " conv3d_11 (Conv3D)             (None, 8, 8, 8, 256  1769728     ['leaky_re_lu_10[0][0]']         \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " batch_normalization_11 (BatchN  (None, 8, 8, 8, 256  1024       ['conv3d_11[0][0]']              \n",
      " ormalization)                  )                                                                 \n",
      "                                                                                                  \n",
      " leaky_re_lu_11 (LeakyReLU)     (None, 8, 8, 8, 256  0           ['batch_normalization_11[0][0]'] \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " conv3d_transpose_1 (Conv3DTran  (None, 16, 16, 16,   262272     ['leaky_re_lu_11[0][0]']         \n",
      " spose)                         128)                                                              \n",
      "                                                                                                  \n",
      " concatenate_1 (Concatenate)    (None, 16, 16, 16,   0           ['conv3d_transpose_1[0][0]',     \n",
      "                                256)                              'leaky_re_lu_5[0][0]']          \n",
      "                                                                                                  \n",
      " conv3d_12 (Conv3D)             (None, 16, 16, 16,   884864      ['concatenate_1[0][0]']          \n",
      "                                128)                                                              \n",
      "                                                                                                  \n",
      " batch_normalization_12 (BatchN  (None, 16, 16, 16,   512        ['conv3d_12[0][0]']              \n",
      " ormalization)                  128)                                                              \n",
      "                                                                                                  \n",
      " leaky_re_lu_12 (LeakyReLU)     (None, 16, 16, 16,   0           ['batch_normalization_12[0][0]'] \n",
      "                                128)                                                              \n",
      "                                                                                                  \n",
      " conv3d_13 (Conv3D)             (None, 16, 16, 16,   442496      ['leaky_re_lu_12[0][0]']         \n",
      "                                128)                                                              \n",
      "                                                                                                  \n",
      " batch_normalization_13 (BatchN  (None, 16, 16, 16,   512        ['conv3d_13[0][0]']              \n",
      " ormalization)                  128)                                                              \n",
      "                                                                                                  \n",
      " leaky_re_lu_13 (LeakyReLU)     (None, 16, 16, 16,   0           ['batch_normalization_13[0][0]'] \n",
      "                                128)                                                              \n",
      "                                                                                                  \n",
      " conv3d_transpose_2 (Conv3DTran  (None, 32, 32, 32,   65600      ['leaky_re_lu_13[0][0]']         \n",
      " spose)                         64)                                                               \n",
      "                                                                                                  \n",
      " concatenate_2 (Concatenate)    (None, 32, 32, 32,   0           ['conv3d_transpose_2[0][0]',     \n",
      "                                128)                              'leaky_re_lu_3[0][0]']          \n",
      "                                                                                                  \n",
      " conv3d_14 (Conv3D)             (None, 32, 32, 32,   221248      ['concatenate_2[0][0]']          \n",
      "                                64)                                                               \n",
      "                                                                                                  \n",
      " batch_normalization_14 (BatchN  (None, 32, 32, 32,   256        ['conv3d_14[0][0]']              \n",
      " ormalization)                  64)                                                               \n",
      "                                                                                                  \n",
      " leaky_re_lu_14 (LeakyReLU)     (None, 32, 32, 32,   0           ['batch_normalization_14[0][0]'] \n",
      "                                64)                                                               \n",
      "                                                                                                  \n",
      " conv3d_15 (Conv3D)             (None, 32, 32, 32,   110656      ['leaky_re_lu_14[0][0]']         \n",
      "                                64)                                                               \n",
      "                                                                                                  \n",
      " batch_normalization_15 (BatchN  (None, 32, 32, 32,   256        ['conv3d_15[0][0]']              \n",
      " ormalization)                  64)                                                               \n",
      "                                                                                                  \n",
      " leaky_re_lu_15 (LeakyReLU)     (None, 32, 32, 32,   0           ['batch_normalization_15[0][0]'] \n",
      "                                64)                                                               \n",
      "                                                                                                  \n",
      " conv3d_transpose_3 (Conv3DTran  (None, 64, 64, 64,   16416      ['leaky_re_lu_15[0][0]']         \n",
      " spose)                         32)                                                               \n",
      "                                                                                                  \n",
      " concatenate_3 (Concatenate)    (None, 64, 64, 64,   0           ['conv3d_transpose_3[0][0]',     \n",
      "                                64)                               'leaky_re_lu_1[0][0]']          \n",
      "                                                                                                  \n",
      " conv3d_16 (Conv3D)             (None, 64, 64, 64,   55328       ['concatenate_3[0][0]']          \n",
      "                                32)                                                               \n",
      "                                                                                                  \n",
      " batch_normalization_16 (BatchN  (None, 64, 64, 64,   128        ['conv3d_16[0][0]']              \n",
      " ormalization)                  32)                                                               \n",
      "                                                                                                  \n",
      " leaky_re_lu_16 (LeakyReLU)     (None, 64, 64, 64,   0           ['batch_normalization_16[0][0]'] \n",
      "                                32)                                                               \n",
      "                                                                                                  \n",
      " conv3d_17 (Conv3D)             (None, 64, 64, 64,   27680       ['leaky_re_lu_16[0][0]']         \n",
      "                                32)                                                               \n",
      "                                                                                                  \n",
      " batch_normalization_17 (BatchN  (None, 64, 64, 64,   128        ['conv3d_17[0][0]']              \n",
      " ormalization)                  32)                                                               \n",
      "                                                                                                  \n",
      " leaky_re_lu_17 (LeakyReLU)     (None, 64, 64, 64,   0           ['batch_normalization_17[0][0]'] \n",
      "                                32)                                                               \n",
      "                                                                                                  \n",
      " conv3d_18 (Conv3D)             (None, 64, 64, 64,   865         ['leaky_re_lu_17[0][0]']         \n",
      "                                1)                                                                \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 22,587,937\n",
      "Trainable params: 22,582,049\n",
      "Non-trainable params: 5,888\n",
      "__________________________________________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "model = UNetModel_3D_simplified.build_unet(input_shape, n_classes = num_class)\n",
    "\n",
    "model.compile(optimizer=opt, loss=loss.tversky_crossentropy, metrics=metrics)\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b0881f5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_path = '/media/dro/JHSeagate/FYP/jh_fyp_work/3D_UNet/Final/CSVLogs/resampled_5mm.csv'\n",
    "model_checkpoint_path = '/media/dro/JHSeagate/FYP/jh_fyp_work/3D_UNet/Final/SavedModels/resampled_5mm.hdf5'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "5502cce8",
   "metadata": {},
   "outputs": [],
   "source": [
    "my_callbacks = [\n",
    "    ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=8, mode = 'auto'),\n",
    "    EarlyStopping(monitor='val_loss', patience=50, restore_best_weights=True, mode = 'auto'),\n",
    "    CSVLogger(csv_path, separator=',', append=True),\n",
    "    ModelCheckpoint(filepath=model_checkpoint_path,\n",
    "                    monitor='val_loss',\n",
    "                    mode='auto',\n",
    "                    verbose=1,\n",
    "                    save_best_only= True)\n",
    "]\n",
    "#     CustomCallBack()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "caca8054",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/300\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-03-15 13:41:28.749935: I tensorflow/stream_executor/cuda/cuda_dnn.cc:366] Loaded cuDNN version 8301\n",
      "2023-03-15 13:41:29.596574: W tensorflow/stream_executor/gpu/asm_compiler.cc:111] *** WARNING *** You are using ptxas 11.0.221, which is older than 11.1. ptxas before 11.1 is known to miscompile XLA code, leading to incorrect results or invalid-address errors.\n",
      "\n",
      "You may not need to update to CUDA 11.1; cherry-picking the ptxas binary is often sufficient.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2109/2109 [==============================] - ETA: 0s - loss: 0.7413 - dice_coef: 0.5047\n",
      "Epoch 00001: val_loss improved from inf to 0.58953, saving model to /media/dro/JHSeagate/FYP/jh_fyp_work/3D_UNet/Final/SavedModels/resampled_5mm.hdf5\n",
      "2109/2109 [==============================] - 215s 98ms/step - loss: 0.7413 - dice_coef: 0.5047 - val_loss: 0.5895 - val_dice_coef: 0.5956 - lr: 0.0010\n",
      "Epoch 2/300\n",
      "2108/2109 [============================>.] - ETA: 0s - loss: 0.6343 - dice_coef: 0.6219\n",
      "Epoch 00002: val_loss did not improve from 0.58953\n",
      "2109/2109 [==============================] - 204s 97ms/step - loss: 0.6343 - dice_coef: 0.6219 - val_loss: 1.3843 - val_dice_coef: 0.0104 - lr: 0.0010\n",
      "Epoch 3/300\n",
      "2108/2109 [============================>.] - ETA: 0s - loss: 0.5921 - dice_coef: 0.6635\n",
      "Epoch 00003: val_loss did not improve from 0.58953\n",
      "2109/2109 [==============================] - 202s 96ms/step - loss: 0.5922 - dice_coef: 0.6633 - val_loss: 0.7224 - val_dice_coef: 0.4991 - lr: 0.0010\n",
      "Epoch 4/300\n",
      "2108/2109 [============================>.] - ETA: 0s - loss: 0.5663 - dice_coef: 0.6925\n",
      "Epoch 00004: val_loss did not improve from 0.58953\n",
      "2109/2109 [==============================] - 202s 96ms/step - loss: 0.5663 - dice_coef: 0.6924 - val_loss: 1.7338 - val_dice_coef: 0.0011 - lr: 0.0010\n",
      "Epoch 5/300\n",
      "2108/2109 [============================>.] - ETA: 0s - loss: 0.5480 - dice_coef: 0.7119\n",
      "Epoch 00005: val_loss did not improve from 0.58953\n",
      "2109/2109 [==============================] - 202s 96ms/step - loss: 0.5480 - dice_coef: 0.7119 - val_loss: 0.6097 - val_dice_coef: 0.5371 - lr: 0.0010\n",
      "Epoch 6/300\n",
      "2108/2109 [============================>.] - ETA: 0s - loss: 0.5289 - dice_coef: 0.7319\n",
      "Epoch 00006: val_loss improved from 0.58953 to 0.48979, saving model to /media/dro/JHSeagate/FYP/jh_fyp_work/3D_UNet/Final/SavedModels/resampled_5mm.hdf5\n",
      "2109/2109 [==============================] - 202s 96ms/step - loss: 0.5288 - dice_coef: 0.7320 - val_loss: 0.4898 - val_dice_coef: 0.6865 - lr: 0.0010\n",
      "Epoch 7/300\n",
      "2108/2109 [============================>.] - ETA: 0s - loss: 0.5187 - dice_coef: 0.7414\n",
      "Epoch 00007: val_loss improved from 0.48979 to 0.46899, saving model to /media/dro/JHSeagate/FYP/jh_fyp_work/3D_UNet/Final/SavedModels/resampled_5mm.hdf5\n",
      "2109/2109 [==============================] - 202s 96ms/step - loss: 0.5187 - dice_coef: 0.7414 - val_loss: 0.4690 - val_dice_coef: 0.7181 - lr: 0.0010\n",
      "Epoch 8/300\n",
      "2108/2109 [============================>.] - ETA: 0s - loss: 0.5050 - dice_coef: 0.7527\n",
      "Epoch 00008: val_loss did not improve from 0.46899\n",
      "2109/2109 [==============================] - 202s 96ms/step - loss: 0.5051 - dice_coef: 0.7524 - val_loss: 0.7584 - val_dice_coef: 0.5582 - lr: 0.0010\n",
      "Epoch 9/300\n",
      "2108/2109 [============================>.] - ETA: 0s - loss: 0.4931 - dice_coef: 0.7667\n",
      "Epoch 00009: val_loss improved from 0.46899 to 0.45552, saving model to /media/dro/JHSeagate/FYP/jh_fyp_work/3D_UNet/Final/SavedModels/resampled_5mm.hdf5\n",
      "2109/2109 [==============================] - 203s 96ms/step - loss: 0.4932 - dice_coef: 0.7666 - val_loss: 0.4555 - val_dice_coef: 0.7387 - lr: 0.0010\n",
      "Epoch 10/300\n",
      "2108/2109 [============================>.] - ETA: 0s - loss: 0.4872 - dice_coef: 0.7675\n",
      "Epoch 00010: val_loss did not improve from 0.45552\n",
      "2109/2109 [==============================] - 202s 96ms/step - loss: 0.4872 - dice_coef: 0.7675 - val_loss: 0.5616 - val_dice_coef: 0.5960 - lr: 0.0010\n",
      "Epoch 11/300\n",
      "2108/2109 [============================>.] - ETA: 0s - loss: 0.4662 - dice_coef: 0.7777\n",
      "Epoch 00011: val_loss improved from 0.45552 to 0.39622, saving model to /media/dro/JHSeagate/FYP/jh_fyp_work/3D_UNet/Final/SavedModels/resampled_5mm.hdf5\n",
      "2109/2109 [==============================] - 203s 96ms/step - loss: 0.4662 - dice_coef: 0.7776 - val_loss: 0.3962 - val_dice_coef: 0.7520 - lr: 0.0010\n",
      "Epoch 12/300\n",
      "2108/2109 [============================>.] - ETA: 0s - loss: 0.4453 - dice_coef: 0.7829\n",
      "Epoch 00012: val_loss did not improve from 0.39622\n",
      "2109/2109 [==============================] - 202s 96ms/step - loss: 0.4452 - dice_coef: 0.7827 - val_loss: 0.4069 - val_dice_coef: 0.7366 - lr: 0.0010\n",
      "Epoch 13/300\n",
      "2108/2109 [============================>.] - ETA: 0s - loss: 0.4457 - dice_coef: 0.7600\n",
      "Epoch 00013: val_loss did not improve from 0.39622\n",
      "2109/2109 [==============================] - 202s 96ms/step - loss: 0.4456 - dice_coef: 0.7600 - val_loss: 0.3973 - val_dice_coef: 0.7002 - lr: 0.0010\n",
      "Epoch 14/300\n",
      "2108/2109 [============================>.] - ETA: 0s - loss: 0.4255 - dice_coef: 0.7678\n",
      "Epoch 00014: val_loss did not improve from 0.39622\n",
      "2109/2109 [==============================] - 202s 96ms/step - loss: 0.4255 - dice_coef: 0.7677 - val_loss: 0.4802 - val_dice_coef: 0.6448 - lr: 0.0010\n",
      "Epoch 15/300\n",
      "2108/2109 [============================>.] - ETA: 0s - loss: 0.4149 - dice_coef: 0.7690\n",
      "Epoch 00015: val_loss did not improve from 0.39622\n",
      "2109/2109 [==============================] - 202s 96ms/step - loss: 0.4148 - dice_coef: 0.7690 - val_loss: 0.4187 - val_dice_coef: 0.6928 - lr: 0.0010\n",
      "Epoch 16/300\n",
      "2108/2109 [============================>.] - ETA: 0s - loss: 0.4136 - dice_coef: 0.7695\n",
      "Epoch 00016: val_loss improved from 0.39622 to 0.36989, saving model to /media/dro/JHSeagate/FYP/jh_fyp_work/3D_UNet/Final/SavedModels/resampled_5mm.hdf5\n",
      "2109/2109 [==============================] - 203s 96ms/step - loss: 0.4136 - dice_coef: 0.7695 - val_loss: 0.3699 - val_dice_coef: 0.7329 - lr: 0.0010\n",
      "Epoch 17/300\n",
      "2108/2109 [============================>.] - ETA: 0s - loss: 0.4116 - dice_coef: 0.7698\n",
      "Epoch 00017: val_loss did not improve from 0.36989\n",
      "2109/2109 [==============================] - 202s 96ms/step - loss: 0.4116 - dice_coef: 0.7696 - val_loss: 0.4389 - val_dice_coef: 0.6091 - lr: 0.0010\n",
      "Epoch 18/300\n",
      "2108/2109 [============================>.] - ETA: 0s - loss: 0.4012 - dice_coef: 0.7744\n",
      "Epoch 00018: val_loss did not improve from 0.36989\n",
      "2109/2109 [==============================] - 202s 96ms/step - loss: 0.4011 - dice_coef: 0.7745 - val_loss: 0.4626 - val_dice_coef: 0.5721 - lr: 0.0010\n",
      "Epoch 19/300\n",
      "2108/2109 [============================>.] - ETA: 0s - loss: 0.3999 - dice_coef: 0.7807\n",
      "Epoch 00019: val_loss did not improve from 0.36989\n",
      "2109/2109 [==============================] - 202s 96ms/step - loss: 0.3999 - dice_coef: 0.7805 - val_loss: 1.6573 - val_dice_coef: 0.3018 - lr: 0.0010\n",
      "Epoch 20/300\n",
      "2108/2109 [============================>.] - ETA: 0s - loss: 0.3782 - dice_coef: 0.7894\n",
      "Epoch 00020: val_loss improved from 0.36989 to 0.30333, saving model to /media/dro/JHSeagate/FYP/jh_fyp_work/3D_UNet/Final/SavedModels/resampled_5mm.hdf5\n",
      "2109/2109 [==============================] - 202s 96ms/step - loss: 0.3782 - dice_coef: 0.7894 - val_loss: 0.3033 - val_dice_coef: 0.7576 - lr: 0.0010\n",
      "Epoch 21/300\n",
      "2108/2109 [============================>.] - ETA: 0s - loss: 0.3803 - dice_coef: 0.7881\n",
      "Epoch 00021: val_loss did not improve from 0.30333\n",
      "2109/2109 [==============================] - 203s 96ms/step - loss: 0.3803 - dice_coef: 0.7881 - val_loss: 0.3366 - val_dice_coef: 0.7214 - lr: 0.0010\n",
      "Epoch 22/300\n",
      "2108/2109 [============================>.] - ETA: 0s - loss: 0.3790 - dice_coef: 0.7860\n",
      "Epoch 00022: val_loss did not improve from 0.30333\n",
      "2109/2109 [==============================] - 203s 96ms/step - loss: 0.3791 - dice_coef: 0.7859 - val_loss: 0.3658 - val_dice_coef: 0.7335 - lr: 0.0010\n",
      "Epoch 23/300\n",
      "2108/2109 [============================>.] - ETA: 0s - loss: 0.3806 - dice_coef: 0.7852\n",
      "Epoch 00023: val_loss did not improve from 0.30333\n",
      "2109/2109 [==============================] - 202s 96ms/step - loss: 0.3806 - dice_coef: 0.7851 - val_loss: 0.4821 - val_dice_coef: 0.5552 - lr: 0.0010\n",
      "Epoch 24/300\n",
      "2108/2109 [============================>.] - ETA: 0s - loss: 0.3787 - dice_coef: 0.7878\n",
      "Epoch 00024: val_loss did not improve from 0.30333\n",
      "2109/2109 [==============================] - 202s 96ms/step - loss: 0.3786 - dice_coef: 0.7879 - val_loss: 0.4626 - val_dice_coef: 0.6836 - lr: 0.0010\n",
      "Epoch 25/300\n",
      "2108/2109 [============================>.] - ETA: 0s - loss: 0.3796 - dice_coef: 0.7837\n",
      "Epoch 00025: val_loss did not improve from 0.30333\n",
      "2109/2109 [==============================] - 202s 96ms/step - loss: 0.3797 - dice_coef: 0.7834 - val_loss: 0.3239 - val_dice_coef: 0.7479 - lr: 0.0010\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 26/300\n",
      "2108/2109 [============================>.] - ETA: 0s - loss: 0.3714 - dice_coef: 0.7927\n",
      "Epoch 00026: val_loss did not improve from 0.30333\n",
      "2109/2109 [==============================] - 202s 96ms/step - loss: 0.3714 - dice_coef: 0.7927 - val_loss: 0.4677 - val_dice_coef: 0.6806 - lr: 0.0010\n",
      "Epoch 27/300\n",
      "2108/2109 [============================>.] - ETA: 0s - loss: 0.3634 - dice_coef: 0.7977\n",
      "Epoch 00027: val_loss did not improve from 0.30333\n",
      "2109/2109 [==============================] - 202s 96ms/step - loss: 0.3634 - dice_coef: 0.7977 - val_loss: 0.4338 - val_dice_coef: 0.6921 - lr: 0.0010\n",
      "Epoch 28/300\n",
      "2108/2109 [============================>.] - ETA: 0s - loss: 0.3654 - dice_coef: 0.7936\n",
      "Epoch 00028: val_loss did not improve from 0.30333\n",
      "2109/2109 [==============================] - 203s 96ms/step - loss: 0.3653 - dice_coef: 0.7936 - val_loss: 0.9125 - val_dice_coef: 0.4753 - lr: 0.0010\n",
      "Epoch 29/300\n",
      "2108/2109 [============================>.] - ETA: 0s - loss: 0.3364 - dice_coef: 0.8114\n",
      "Epoch 00029: val_loss did not improve from 0.30333\n",
      "2109/2109 [==============================] - 203s 96ms/step - loss: 0.3365 - dice_coef: 0.8115 - val_loss: 0.3602 - val_dice_coef: 0.7409 - lr: 5.0000e-04\n",
      "Epoch 30/300\n",
      "2108/2109 [============================>.] - ETA: 0s - loss: 0.3191 - dice_coef: 0.8175\n",
      "Epoch 00030: val_loss improved from 0.30333 to 0.29319, saving model to /media/dro/JHSeagate/FYP/jh_fyp_work/3D_UNet/Final/SavedModels/resampled_5mm.hdf5\n",
      "2109/2109 [==============================] - 203s 96ms/step - loss: 0.3191 - dice_coef: 0.8174 - val_loss: 0.2932 - val_dice_coef: 0.7854 - lr: 5.0000e-04\n",
      "Epoch 31/300\n",
      "2108/2109 [============================>.] - ETA: 0s - loss: 0.3128 - dice_coef: 0.8210\n",
      "Epoch 00031: val_loss improved from 0.29319 to 0.27999, saving model to /media/dro/JHSeagate/FYP/jh_fyp_work/3D_UNet/Final/SavedModels/resampled_5mm.hdf5\n",
      "2109/2109 [==============================] - 202s 96ms/step - loss: 0.3129 - dice_coef: 0.8210 - val_loss: 0.2800 - val_dice_coef: 0.7967 - lr: 5.0000e-04\n",
      "Epoch 32/300\n",
      "2108/2109 [============================>.] - ETA: 0s - loss: 0.3130 - dice_coef: 0.8242\n",
      "Epoch 00032: val_loss did not improve from 0.27999\n",
      "2109/2109 [==============================] - 202s 96ms/step - loss: 0.3132 - dice_coef: 0.8238 - val_loss: 0.2954 - val_dice_coef: 0.7925 - lr: 5.0000e-04\n",
      "Epoch 33/300\n",
      "2108/2109 [============================>.] - ETA: 0s - loss: 0.3025 - dice_coef: 0.8300\n",
      "Epoch 00033: val_loss did not improve from 0.27999\n",
      "2109/2109 [==============================] - 202s 96ms/step - loss: 0.3025 - dice_coef: 0.8300 - val_loss: 0.2822 - val_dice_coef: 0.7764 - lr: 5.0000e-04\n",
      "Epoch 34/300\n",
      "2108/2109 [============================>.] - ETA: 0s - loss: 0.3004 - dice_coef: 0.8312\n",
      "Epoch 00034: val_loss improved from 0.27999 to 0.27561, saving model to /media/dro/JHSeagate/FYP/jh_fyp_work/3D_UNet/Final/SavedModels/resampled_5mm.hdf5\n",
      "2109/2109 [==============================] - 203s 96ms/step - loss: 0.3004 - dice_coef: 0.8310 - val_loss: 0.2756 - val_dice_coef: 0.7597 - lr: 5.0000e-04\n",
      "Epoch 35/300\n",
      "2108/2109 [============================>.] - ETA: 0s - loss: 0.3082 - dice_coef: 0.8278\n",
      "Epoch 00035: val_loss improved from 0.27561 to 0.27393, saving model to /media/dro/JHSeagate/FYP/jh_fyp_work/3D_UNet/Final/SavedModels/resampled_5mm.hdf5\n",
      "2109/2109 [==============================] - 203s 96ms/step - loss: 0.3082 - dice_coef: 0.8275 - val_loss: 0.2739 - val_dice_coef: 0.7828 - lr: 5.0000e-04\n",
      "Epoch 36/300\n",
      "2108/2109 [============================>.] - ETA: 0s - loss: 0.3053 - dice_coef: 0.8260\n",
      "Epoch 00036: val_loss improved from 0.27393 to 0.25953, saving model to /media/dro/JHSeagate/FYP/jh_fyp_work/3D_UNet/Final/SavedModels/resampled_5mm.hdf5\n",
      "2109/2109 [==============================] - 203s 96ms/step - loss: 0.3053 - dice_coef: 0.8261 - val_loss: 0.2595 - val_dice_coef: 0.7979 - lr: 5.0000e-04\n",
      "Epoch 37/300\n",
      "2108/2109 [============================>.] - ETA: 0s - loss: 0.2993 - dice_coef: 0.8310\n",
      "Epoch 00037: val_loss did not improve from 0.25953\n",
      "2109/2109 [==============================] - 202s 96ms/step - loss: 0.2992 - dice_coef: 0.8310 - val_loss: 0.2674 - val_dice_coef: 0.8013 - lr: 5.0000e-04\n",
      "Epoch 38/300\n",
      "2108/2109 [============================>.] - ETA: 0s - loss: 0.2895 - dice_coef: 0.8327\n",
      "Epoch 00038: val_loss improved from 0.25953 to 0.25332, saving model to /media/dro/JHSeagate/FYP/jh_fyp_work/3D_UNet/Final/SavedModels/resampled_5mm.hdf5\n",
      "2109/2109 [==============================] - 203s 96ms/step - loss: 0.2895 - dice_coef: 0.8327 - val_loss: 0.2533 - val_dice_coef: 0.8147 - lr: 5.0000e-04\n",
      "Epoch 39/300\n",
      "2108/2109 [============================>.] - ETA: 0s - loss: 0.2913 - dice_coef: 0.8357\n",
      "Epoch 00039: val_loss did not improve from 0.25332\n",
      "2109/2109 [==============================] - 202s 96ms/step - loss: 0.2913 - dice_coef: 0.8357 - val_loss: 0.2625 - val_dice_coef: 0.8020 - lr: 5.0000e-04\n",
      "Epoch 40/300\n",
      "2108/2109 [============================>.] - ETA: 0s - loss: 0.2900 - dice_coef: 0.8366\n",
      "Epoch 00040: val_loss did not improve from 0.25332\n",
      "2109/2109 [==============================] - 202s 96ms/step - loss: 0.2902 - dice_coef: 0.8362 - val_loss: 0.2715 - val_dice_coef: 0.7791 - lr: 5.0000e-04\n",
      "Epoch 41/300\n",
      "2108/2109 [============================>.] - ETA: 0s - loss: 0.2892 - dice_coef: 0.8339\n",
      "Epoch 00041: val_loss did not improve from 0.25332\n",
      "2109/2109 [==============================] - 202s 96ms/step - loss: 0.2892 - dice_coef: 0.8339 - val_loss: 0.2980 - val_dice_coef: 0.7937 - lr: 5.0000e-04\n",
      "Epoch 42/300\n",
      "2108/2109 [============================>.] - ETA: 0s - loss: 0.2831 - dice_coef: 0.8396\n",
      "Epoch 00042: val_loss did not improve from 0.25332\n",
      "2109/2109 [==============================] - 202s 96ms/step - loss: 0.2830 - dice_coef: 0.8395 - val_loss: 0.2880 - val_dice_coef: 0.7912 - lr: 5.0000e-04\n",
      "Epoch 43/300\n",
      "2108/2109 [============================>.] - ETA: 0s - loss: 0.2838 - dice_coef: 0.8377\n",
      "Epoch 00043: val_loss improved from 0.25332 to 0.24815, saving model to /media/dro/JHSeagate/FYP/jh_fyp_work/3D_UNet/Final/SavedModels/resampled_5mm.hdf5\n",
      "2109/2109 [==============================] - 203s 96ms/step - loss: 0.2838 - dice_coef: 0.8376 - val_loss: 0.2481 - val_dice_coef: 0.8124 - lr: 5.0000e-04\n",
      "Epoch 44/300\n",
      "2108/2109 [============================>.] - ETA: 0s - loss: 0.2796 - dice_coef: 0.8398\n",
      "Epoch 00044: val_loss did not improve from 0.24815\n",
      "2109/2109 [==============================] - 202s 96ms/step - loss: 0.2797 - dice_coef: 0.8396 - val_loss: 0.2539 - val_dice_coef: 0.8172 - lr: 5.0000e-04\n",
      "Epoch 45/300\n",
      "2108/2109 [============================>.] - ETA: 0s - loss: 0.2873 - dice_coef: 0.8412\n",
      "Epoch 00045: val_loss improved from 0.24815 to 0.24633, saving model to /media/dro/JHSeagate/FYP/jh_fyp_work/3D_UNet/Final/SavedModels/resampled_5mm.hdf5\n",
      "2109/2109 [==============================] - 203s 96ms/step - loss: 0.2872 - dice_coef: 0.8413 - val_loss: 0.2463 - val_dice_coef: 0.8098 - lr: 5.0000e-04\n",
      "Epoch 46/300\n",
      "2108/2109 [============================>.] - ETA: 0s - loss: 0.2789 - dice_coef: 0.8395\n",
      "Epoch 00046: val_loss did not improve from 0.24633\n",
      "2109/2109 [==============================] - 202s 96ms/step - loss: 0.2790 - dice_coef: 0.8393 - val_loss: 0.2917 - val_dice_coef: 0.7998 - lr: 5.0000e-04\n",
      "Epoch 47/300\n",
      "2108/2109 [============================>.] - ETA: 0s - loss: 0.2704 - dice_coef: 0.8474\n",
      "Epoch 00047: val_loss improved from 0.24633 to 0.23881, saving model to /media/dro/JHSeagate/FYP/jh_fyp_work/3D_UNet/Final/SavedModels/resampled_5mm.hdf5\n",
      "2109/2109 [==============================] - 202s 96ms/step - loss: 0.2704 - dice_coef: 0.8475 - val_loss: 0.2388 - val_dice_coef: 0.8100 - lr: 5.0000e-04\n",
      "Epoch 48/300\n",
      "2108/2109 [============================>.] - ETA: 0s - loss: 0.2729 - dice_coef: 0.8434\n",
      "Epoch 00048: val_loss did not improve from 0.23881\n",
      "2109/2109 [==============================] - 202s 96ms/step - loss: 0.2730 - dice_coef: 0.8433 - val_loss: 0.2980 - val_dice_coef: 0.7864 - lr: 5.0000e-04\n",
      "Epoch 49/300\n",
      "2108/2109 [============================>.] - ETA: 0s - loss: 0.2711 - dice_coef: 0.8446\n",
      "Epoch 00049: val_loss did not improve from 0.23881\n",
      "2109/2109 [==============================] - 203s 96ms/step - loss: 0.2711 - dice_coef: 0.8445 - val_loss: 0.2529 - val_dice_coef: 0.8062 - lr: 5.0000e-04\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 50/300\n",
      "2108/2109 [============================>.] - ETA: 0s - loss: 0.2777 - dice_coef: 0.8455\n",
      "Epoch 00050: val_loss did not improve from 0.23881\n",
      "2109/2109 [==============================] - 202s 96ms/step - loss: 0.2777 - dice_coef: 0.8455 - val_loss: 0.3028 - val_dice_coef: 0.7915 - lr: 5.0000e-04\n",
      "Epoch 51/300\n",
      "2108/2109 [============================>.] - ETA: 0s - loss: 0.2667 - dice_coef: 0.8492\n",
      "Epoch 00051: val_loss did not improve from 0.23881\n",
      "2109/2109 [==============================] - 202s 96ms/step - loss: 0.2667 - dice_coef: 0.8492 - val_loss: 0.2995 - val_dice_coef: 0.7292 - lr: 5.0000e-04\n",
      "Epoch 52/300\n",
      "2108/2109 [============================>.] - ETA: 0s - loss: 0.2678 - dice_coef: 0.8485\n",
      "Epoch 00052: val_loss did not improve from 0.23881\n",
      "2109/2109 [==============================] - 202s 96ms/step - loss: 0.2677 - dice_coef: 0.8485 - val_loss: 0.2720 - val_dice_coef: 0.7937 - lr: 5.0000e-04\n",
      "Epoch 53/300\n",
      "2108/2109 [============================>.] - ETA: 0s - loss: 0.2665 - dice_coef: 0.8474\n",
      "Epoch 00053: val_loss improved from 0.23881 to 0.22783, saving model to /media/dro/JHSeagate/FYP/jh_fyp_work/3D_UNet/Final/SavedModels/resampled_5mm.hdf5\n",
      "2109/2109 [==============================] - 203s 96ms/step - loss: 0.2665 - dice_coef: 0.8474 - val_loss: 0.2278 - val_dice_coef: 0.8281 - lr: 5.0000e-04\n",
      "Epoch 54/300\n",
      "2108/2109 [============================>.] - ETA: 0s - loss: 0.2607 - dice_coef: 0.8498\n",
      "Epoch 00054: val_loss did not improve from 0.22783\n",
      "2109/2109 [==============================] - 202s 96ms/step - loss: 0.2607 - dice_coef: 0.8497 - val_loss: 0.2318 - val_dice_coef: 0.8276 - lr: 5.0000e-04\n",
      "Epoch 55/300\n",
      "2108/2109 [============================>.] - ETA: 0s - loss: 0.2626 - dice_coef: 0.8508\n",
      "Epoch 00055: val_loss did not improve from 0.22783\n",
      "2109/2109 [==============================] - 202s 96ms/step - loss: 0.2626 - dice_coef: 0.8508 - val_loss: 0.2703 - val_dice_coef: 0.7911 - lr: 5.0000e-04\n",
      "Epoch 56/300\n",
      "2108/2109 [============================>.] - ETA: 0s - loss: 0.2645 - dice_coef: 0.8506\n",
      "Epoch 00056: val_loss did not improve from 0.22783\n",
      "2109/2109 [==============================] - 202s 96ms/step - loss: 0.2646 - dice_coef: 0.8503 - val_loss: 0.2543 - val_dice_coef: 0.8194 - lr: 5.0000e-04\n",
      "Epoch 57/300\n",
      "2108/2109 [============================>.] - ETA: 0s - loss: 0.2658 - dice_coef: 0.8477\n",
      "Epoch 00057: val_loss did not improve from 0.22783\n",
      "2109/2109 [==============================] - 202s 96ms/step - loss: 0.2659 - dice_coef: 0.8475 - val_loss: 0.2673 - val_dice_coef: 0.8071 - lr: 5.0000e-04\n",
      "Epoch 58/300\n",
      "2108/2109 [============================>.] - ETA: 0s - loss: 0.2538 - dice_coef: 0.8527\n",
      "Epoch 00058: val_loss did not improve from 0.22783\n",
      "2109/2109 [==============================] - 202s 96ms/step - loss: 0.2537 - dice_coef: 0.8527 - val_loss: 0.2294 - val_dice_coef: 0.8331 - lr: 5.0000e-04\n",
      "Epoch 59/300\n",
      "2108/2109 [============================>.] - ETA: 0s - loss: 0.2653 - dice_coef: 0.8474\n",
      "Epoch 00059: val_loss did not improve from 0.22783\n",
      "2109/2109 [==============================] - 202s 96ms/step - loss: 0.2654 - dice_coef: 0.8473 - val_loss: 0.2528 - val_dice_coef: 0.8219 - lr: 5.0000e-04\n",
      "Epoch 60/300\n",
      "2108/2109 [============================>.] - ETA: 0s - loss: 0.2580 - dice_coef: 0.8522\n",
      "Epoch 00060: val_loss did not improve from 0.22783\n",
      "2109/2109 [==============================] - 202s 96ms/step - loss: 0.2581 - dice_coef: 0.8521 - val_loss: 0.2323 - val_dice_coef: 0.7892 - lr: 5.0000e-04\n",
      "Epoch 61/300\n",
      "2108/2109 [============================>.] - ETA: 0s - loss: 0.2522 - dice_coef: 0.8533\n",
      "Epoch 00061: val_loss improved from 0.22783 to 0.22332, saving model to /media/dro/JHSeagate/FYP/jh_fyp_work/3D_UNet/Final/SavedModels/resampled_5mm.hdf5\n",
      "2109/2109 [==============================] - 202s 96ms/step - loss: 0.2522 - dice_coef: 0.8533 - val_loss: 0.2233 - val_dice_coef: 0.8173 - lr: 5.0000e-04\n",
      "Epoch 62/300\n",
      "2108/2109 [============================>.] - ETA: 0s - loss: 0.2485 - dice_coef: 0.8571\n",
      "Epoch 00062: val_loss improved from 0.22332 to 0.22241, saving model to /media/dro/JHSeagate/FYP/jh_fyp_work/3D_UNet/Final/SavedModels/resampled_5mm.hdf5\n",
      "2109/2109 [==============================] - 202s 96ms/step - loss: 0.2487 - dice_coef: 0.8569 - val_loss: 0.2224 - val_dice_coef: 0.8342 - lr: 5.0000e-04\n",
      "Epoch 63/300\n",
      "2108/2109 [============================>.] - ETA: 0s - loss: 0.2561 - dice_coef: 0.8523\n",
      "Epoch 00063: val_loss improved from 0.22241 to 0.21998, saving model to /media/dro/JHSeagate/FYP/jh_fyp_work/3D_UNet/Final/SavedModels/resampled_5mm.hdf5\n",
      "2109/2109 [==============================] - 203s 96ms/step - loss: 0.2561 - dice_coef: 0.8523 - val_loss: 0.2200 - val_dice_coef: 0.8368 - lr: 5.0000e-04\n",
      "Epoch 64/300\n",
      "2108/2109 [============================>.] - ETA: 0s - loss: 0.2551 - dice_coef: 0.8513\n",
      "Epoch 00064: val_loss did not improve from 0.21998\n",
      "2109/2109 [==============================] - 202s 96ms/step - loss: 0.2551 - dice_coef: 0.8513 - val_loss: 0.2881 - val_dice_coef: 0.7964 - lr: 5.0000e-04\n",
      "Epoch 65/300\n",
      "2108/2109 [============================>.] - ETA: 0s - loss: 0.2423 - dice_coef: 0.8602\n",
      "Epoch 00065: val_loss did not improve from 0.21998\n",
      "2109/2109 [==============================] - 202s 96ms/step - loss: 0.2424 - dice_coef: 0.8602 - val_loss: 0.2344 - val_dice_coef: 0.8165 - lr: 5.0000e-04\n",
      "Epoch 66/300\n",
      "2108/2109 [============================>.] - ETA: 0s - loss: 0.2505 - dice_coef: 0.8561\n",
      "Epoch 00066: val_loss did not improve from 0.21998\n",
      "2109/2109 [==============================] - 202s 96ms/step - loss: 0.2506 - dice_coef: 0.8559 - val_loss: 0.2713 - val_dice_coef: 0.8075 - lr: 5.0000e-04\n",
      "Epoch 67/300\n",
      "2108/2109 [============================>.] - ETA: 0s - loss: 0.2423 - dice_coef: 0.8600\n",
      "Epoch 00067: val_loss did not improve from 0.21998\n",
      "2109/2109 [==============================] - 202s 96ms/step - loss: 0.2423 - dice_coef: 0.8600 - val_loss: 0.2237 - val_dice_coef: 0.8151 - lr: 5.0000e-04\n",
      "Epoch 68/300\n",
      "2108/2109 [============================>.] - ETA: 0s - loss: 0.2376 - dice_coef: 0.8612\n",
      "Epoch 00068: val_loss did not improve from 0.21998\n",
      "2109/2109 [==============================] - 202s 96ms/step - loss: 0.2375 - dice_coef: 0.8613 - val_loss: 0.2209 - val_dice_coef: 0.8321 - lr: 5.0000e-04\n",
      "Epoch 69/300\n",
      "2108/2109 [============================>.] - ETA: 0s - loss: 0.2453 - dice_coef: 0.8568\n",
      "Epoch 00069: val_loss did not improve from 0.21998\n",
      "2109/2109 [==============================] - 202s 96ms/step - loss: 0.2453 - dice_coef: 0.8568 - val_loss: 0.2284 - val_dice_coef: 0.8350 - lr: 5.0000e-04\n",
      "Epoch 70/300\n",
      "2108/2109 [============================>.] - ETA: 0s - loss: 0.2444 - dice_coef: 0.8585\n",
      "Epoch 00070: val_loss improved from 0.21998 to 0.20264, saving model to /media/dro/JHSeagate/FYP/jh_fyp_work/3D_UNet/Final/SavedModels/resampled_5mm.hdf5\n",
      "2109/2109 [==============================] - 203s 96ms/step - loss: 0.2444 - dice_coef: 0.8585 - val_loss: 0.2026 - val_dice_coef: 0.8411 - lr: 5.0000e-04\n",
      "Epoch 71/300\n",
      "2108/2109 [============================>.] - ETA: 0s - loss: 0.2361 - dice_coef: 0.8613\n",
      "Epoch 00071: val_loss did not improve from 0.20264\n",
      "2109/2109 [==============================] - 202s 96ms/step - loss: 0.2361 - dice_coef: 0.8613 - val_loss: 0.2179 - val_dice_coef: 0.8376 - lr: 5.0000e-04\n",
      "Epoch 72/300\n",
      "2108/2109 [============================>.] - ETA: 0s - loss: 0.2355 - dice_coef: 0.8637\n",
      "Epoch 00072: val_loss improved from 0.20264 to 0.20230, saving model to /media/dro/JHSeagate/FYP/jh_fyp_work/3D_UNet/Final/SavedModels/resampled_5mm.hdf5\n",
      "2109/2109 [==============================] - 203s 96ms/step - loss: 0.2356 - dice_coef: 0.8635 - val_loss: 0.2023 - val_dice_coef: 0.8413 - lr: 5.0000e-04\n",
      "Epoch 73/300\n",
      "2108/2109 [============================>.] - ETA: 0s - loss: 0.2357 - dice_coef: 0.8595\n",
      "Epoch 00073: val_loss did not improve from 0.20230\n",
      "2109/2109 [==============================] - 202s 96ms/step - loss: 0.2356 - dice_coef: 0.8595 - val_loss: 0.2078 - val_dice_coef: 0.8341 - lr: 5.0000e-04\n",
      "Epoch 74/300\n",
      "2108/2109 [============================>.] - ETA: 0s - loss: 0.2361 - dice_coef: 0.8621\n",
      "Epoch 00074: val_loss did not improve from 0.20230\n",
      "2109/2109 [==============================] - 202s 96ms/step - loss: 0.2361 - dice_coef: 0.8621 - val_loss: 0.2058 - val_dice_coef: 0.8409 - lr: 5.0000e-04\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 75/300\n",
      "2108/2109 [============================>.] - ETA: 0s - loss: 0.2332 - dice_coef: 0.8626\n",
      "Epoch 00075: val_loss did not improve from 0.20230\n",
      "2109/2109 [==============================] - 202s 96ms/step - loss: 0.2331 - dice_coef: 0.8626 - val_loss: 0.2107 - val_dice_coef: 0.8449 - lr: 5.0000e-04\n",
      "Epoch 76/300\n",
      "2108/2109 [============================>.] - ETA: 0s - loss: 0.2336 - dice_coef: 0.8626\n",
      "Epoch 00076: val_loss did not improve from 0.20230\n",
      "2109/2109 [==============================] - 202s 96ms/step - loss: 0.2335 - dice_coef: 0.8626 - val_loss: 0.2125 - val_dice_coef: 0.8485 - lr: 5.0000e-04\n",
      "Epoch 77/300\n",
      "2108/2109 [============================>.] - ETA: 0s - loss: 0.2241 - dice_coef: 0.8705\n",
      "Epoch 00077: val_loss did not improve from 0.20230\n",
      "2109/2109 [==============================] - 202s 96ms/step - loss: 0.2241 - dice_coef: 0.8706 - val_loss: 0.2191 - val_dice_coef: 0.8393 - lr: 5.0000e-04\n",
      "Epoch 78/300\n",
      "2108/2109 [============================>.] - ETA: 0s - loss: 0.2264 - dice_coef: 0.8665\n",
      "Epoch 00078: val_loss did not improve from 0.20230\n",
      "2109/2109 [==============================] - 202s 96ms/step - loss: 0.2265 - dice_coef: 0.8665 - val_loss: 0.2115 - val_dice_coef: 0.8408 - lr: 5.0000e-04\n",
      "Epoch 79/300\n",
      "2108/2109 [============================>.] - ETA: 0s - loss: 0.2262 - dice_coef: 0.8660\n",
      "Epoch 00079: val_loss did not improve from 0.20230\n",
      "2109/2109 [==============================] - 203s 96ms/step - loss: 0.2262 - dice_coef: 0.8660 - val_loss: 0.2212 - val_dice_coef: 0.8165 - lr: 5.0000e-04\n",
      "Epoch 80/300\n",
      "2108/2109 [============================>.] - ETA: 0s - loss: 0.2309 - dice_coef: 0.8654\n",
      "Epoch 00080: val_loss did not improve from 0.20230\n",
      "2109/2109 [==============================] - 202s 96ms/step - loss: 0.2309 - dice_coef: 0.8655 - val_loss: 0.3425 - val_dice_coef: 0.7655 - lr: 5.0000e-04\n",
      "Epoch 81/300\n",
      "2108/2109 [============================>.] - ETA: 0s - loss: 0.2124 - dice_coef: 0.8707\n",
      "Epoch 00081: val_loss improved from 0.20230 to 0.19737, saving model to /media/dro/JHSeagate/FYP/jh_fyp_work/3D_UNet/Final/SavedModels/resampled_5mm.hdf5\n",
      "2109/2109 [==============================] - 202s 96ms/step - loss: 0.2125 - dice_coef: 0.8706 - val_loss: 0.1974 - val_dice_coef: 0.8541 - lr: 2.5000e-04\n",
      "Epoch 82/300\n",
      "2108/2109 [============================>.] - ETA: 0s - loss: 0.2042 - dice_coef: 0.8772\n",
      "Epoch 00082: val_loss improved from 0.19737 to 0.18497, saving model to /media/dro/JHSeagate/FYP/jh_fyp_work/3D_UNet/Final/SavedModels/resampled_5mm.hdf5\n",
      "2109/2109 [==============================] - 202s 96ms/step - loss: 0.2042 - dice_coef: 0.8772 - val_loss: 0.1850 - val_dice_coef: 0.8573 - lr: 2.5000e-04\n",
      "Epoch 83/300\n",
      "2108/2109 [============================>.] - ETA: 0s - loss: 0.2053 - dice_coef: 0.8741\n",
      "Epoch 00083: val_loss did not improve from 0.18497\n",
      "2109/2109 [==============================] - 202s 96ms/step - loss: 0.2053 - dice_coef: 0.8739 - val_loss: 0.1968 - val_dice_coef: 0.8441 - lr: 2.5000e-04\n",
      "Epoch 84/300\n",
      "2108/2109 [============================>.] - ETA: 0s - loss: 0.2027 - dice_coef: 0.8767\n",
      "Epoch 00084: val_loss did not improve from 0.18497\n",
      "2109/2109 [==============================] - 202s 96ms/step - loss: 0.2027 - dice_coef: 0.8767 - val_loss: 0.2012 - val_dice_coef: 0.8545 - lr: 2.5000e-04\n",
      "Epoch 85/300\n",
      "2108/2109 [============================>.] - ETA: 0s - loss: 0.1985 - dice_coef: 0.8800\n",
      "Epoch 00085: val_loss did not improve from 0.18497\n",
      "2109/2109 [==============================] - 202s 96ms/step - loss: 0.1985 - dice_coef: 0.8800 - val_loss: 0.1898 - val_dice_coef: 0.8610 - lr: 2.5000e-04\n",
      "Epoch 86/300\n",
      "2108/2109 [============================>.] - ETA: 0s - loss: 0.1991 - dice_coef: 0.8791\n",
      "Epoch 00086: val_loss did not improve from 0.18497\n",
      "2109/2109 [==============================] - 202s 96ms/step - loss: 0.1991 - dice_coef: 0.8790 - val_loss: 0.1889 - val_dice_coef: 0.8629 - lr: 2.5000e-04\n",
      "Epoch 87/300\n",
      "2108/2109 [============================>.] - ETA: 0s - loss: 0.1965 - dice_coef: 0.8788\n",
      "Epoch 00087: val_loss did not improve from 0.18497\n",
      "2109/2109 [==============================] - 202s 96ms/step - loss: 0.1966 - dice_coef: 0.8788 - val_loss: 0.1972 - val_dice_coef: 0.8594 - lr: 2.5000e-04\n",
      "Epoch 88/300\n",
      "2108/2109 [============================>.] - ETA: 0s - loss: 0.1994 - dice_coef: 0.8766\n",
      "Epoch 00088: val_loss did not improve from 0.18497\n",
      "2109/2109 [==============================] - 202s 96ms/step - loss: 0.1995 - dice_coef: 0.8766 - val_loss: 0.2201 - val_dice_coef: 0.8441 - lr: 2.5000e-04\n",
      "Epoch 89/300\n",
      "2108/2109 [============================>.] - ETA: 0s - loss: 0.1966 - dice_coef: 0.8812\n",
      "Epoch 00089: val_loss improved from 0.18497 to 0.17230, saving model to /media/dro/JHSeagate/FYP/jh_fyp_work/3D_UNet/Final/SavedModels/resampled_5mm.hdf5\n",
      "2109/2109 [==============================] - 202s 96ms/step - loss: 0.1966 - dice_coef: 0.8812 - val_loss: 0.1723 - val_dice_coef: 0.8640 - lr: 2.5000e-04\n",
      "Epoch 90/300\n",
      "2108/2109 [============================>.] - ETA: 0s - loss: 0.1950 - dice_coef: 0.8804\n",
      "Epoch 00090: val_loss did not improve from 0.17230\n",
      "2109/2109 [==============================] - 202s 96ms/step - loss: 0.1950 - dice_coef: 0.8804 - val_loss: 0.1816 - val_dice_coef: 0.8642 - lr: 2.5000e-04\n",
      "Epoch 91/300\n",
      "2108/2109 [============================>.] - ETA: 0s - loss: 0.1933 - dice_coef: 0.8836\n",
      "Epoch 00091: val_loss did not improve from 0.17230\n",
      "2109/2109 [==============================] - 202s 96ms/step - loss: 0.1934 - dice_coef: 0.8836 - val_loss: 0.1828 - val_dice_coef: 0.8669 - lr: 2.5000e-04\n",
      "Epoch 92/300\n",
      "2108/2109 [============================>.] - ETA: 0s - loss: 0.1930 - dice_coef: 0.8808\n",
      "Epoch 00092: val_loss did not improve from 0.17230\n",
      "2109/2109 [==============================] - 203s 96ms/step - loss: 0.1930 - dice_coef: 0.8808 - val_loss: 0.1739 - val_dice_coef: 0.8572 - lr: 2.5000e-04\n",
      "Epoch 93/300\n",
      "2108/2109 [============================>.] - ETA: 0s - loss: 0.1929 - dice_coef: 0.8800\n",
      "Epoch 00093: val_loss improved from 0.17230 to 0.16828, saving model to /media/dro/JHSeagate/FYP/jh_fyp_work/3D_UNet/Final/SavedModels/resampled_5mm.hdf5\n",
      "2109/2109 [==============================] - 203s 96ms/step - loss: 0.1929 - dice_coef: 0.8800 - val_loss: 0.1683 - val_dice_coef: 0.8678 - lr: 2.5000e-04\n",
      "Epoch 94/300\n",
      "2108/2109 [============================>.] - ETA: 0s - loss: 0.1875 - dice_coef: 0.8842\n",
      "Epoch 00094: val_loss improved from 0.16828 to 0.16711, saving model to /media/dro/JHSeagate/FYP/jh_fyp_work/3D_UNet/Final/SavedModels/resampled_5mm.hdf5\n",
      "2109/2109 [==============================] - 203s 96ms/step - loss: 0.1876 - dice_coef: 0.8842 - val_loss: 0.1671 - val_dice_coef: 0.8712 - lr: 2.5000e-04\n",
      "Epoch 95/300\n",
      "2108/2109 [============================>.] - ETA: 0s - loss: 0.1882 - dice_coef: 0.8833\n",
      "Epoch 00095: val_loss did not improve from 0.16711\n",
      "2109/2109 [==============================] - 203s 96ms/step - loss: 0.1882 - dice_coef: 0.8834 - val_loss: 0.1755 - val_dice_coef: 0.8617 - lr: 2.5000e-04\n",
      "Epoch 96/300\n",
      "2108/2109 [============================>.] - ETA: 0s - loss: 0.1881 - dice_coef: 0.8854\n",
      "Epoch 00096: val_loss did not improve from 0.16711\n",
      "2109/2109 [==============================] - 204s 97ms/step - loss: 0.1880 - dice_coef: 0.8854 - val_loss: 0.1881 - val_dice_coef: 0.8648 - lr: 2.5000e-04\n",
      "Epoch 97/300\n",
      "2108/2109 [============================>.] - ETA: 0s - loss: 0.1867 - dice_coef: 0.8856\n",
      "Epoch 00097: val_loss improved from 0.16711 to 0.16475, saving model to /media/dro/JHSeagate/FYP/jh_fyp_work/3D_UNet/Final/SavedModels/resampled_5mm.hdf5\n",
      "2109/2109 [==============================] - 203s 96ms/step - loss: 0.1867 - dice_coef: 0.8856 - val_loss: 0.1647 - val_dice_coef: 0.8674 - lr: 2.5000e-04\n",
      "Epoch 98/300\n",
      "2108/2109 [============================>.] - ETA: 0s - loss: 0.1890 - dice_coef: 0.8828\n",
      "Epoch 00098: val_loss did not improve from 0.16475\n",
      "2109/2109 [==============================] - 203s 96ms/step - loss: 0.1890 - dice_coef: 0.8828 - val_loss: 0.1693 - val_dice_coef: 0.8684 - lr: 2.5000e-04\n",
      "Epoch 99/300\n",
      "2108/2109 [============================>.] - ETA: 0s - loss: 0.1820 - dice_coef: 0.8858\n",
      "Epoch 00099: val_loss did not improve from 0.16475\n",
      "2109/2109 [==============================] - 203s 96ms/step - loss: 0.1820 - dice_coef: 0.8856 - val_loss: 0.1997 - val_dice_coef: 0.8565 - lr: 2.5000e-04\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 100/300\n",
      " 159/2109 [=>............................] - ETA: 2:56 - loss: 0.1802 - dice_coef: 0.8720"
     ]
    }
   ],
   "source": [
    "model_name = 'resampled_5mm (300 Epochs)'\n",
    "# Helper.telegram_bot_sendtext(f'Model {model_name} started training')\n",
    "\n",
    "history = model.fit(train_array,\n",
    "                    train_mask_array,\n",
    "                    batch_size=5,\n",
    "                    epochs=300,\n",
    "                    verbose=1,\n",
    "                    shuffle = True,\n",
    "                    validation_data=(valid_array, valid_mask_array),\n",
    "                    callbacks=my_callbacks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91ca7dd6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
