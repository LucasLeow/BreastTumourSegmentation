{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "97ba6c1b",
   "metadata": {},
   "source": [
    "* Same as Model 10 but with +-2 cuboids for contextual Info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ab2a931a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Libraries -------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
    "import os\n",
    "import h5py\n",
    "import keras\n",
    "import loss\n",
    "import Helper\n",
    "import allMetrics\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import UNetModel_3D\n",
    "import matplotlib.pyplot as plt\n",
    "from keras import backend as K\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau, ModelCheckpoint, CSVLogger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "98c89f70",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ae81da60",
   "metadata": {},
   "outputs": [],
   "source": [
    "num = 5\n",
    "\n",
    "train_fileName = f\"/media/dro/JHSeagate/FYP/jh_fyp_work/3D_UNet/trials/numpyDatasets/numPyArrays/train_Scans/train_DS{num}.hdf5\"\n",
    "train_maskfileName = f\"/media/dro/JHSeagate/FYP/jh_fyp_work/3D_UNet/trials/numpyDatasets/numPyArrays/train_Mask_Scans/train_maskDS{num}.hdf5\"\n",
    "\n",
    "train_DatasetName = f\"trainScans_DataSet{num}\"\n",
    "train_maskDatasetName = f\"trainMaskScans_DataSet{num}\"\n",
    "\n",
    "valid_fileName = f\"/media/dro/JHSeagate/FYP/jh_fyp_work/3D_UNet/trials/numpyDatasets/numPyArrays/valid_Scans/valid_DS{num}.hdf5\"\n",
    "valid_maskfileName = f\"/media/dro/JHSeagate/FYP/jh_fyp_work/3D_UNet/trials/numpyDatasets/numPyArrays/valid_Mask_Scans/valid_maskDS{num}.hdf5\"\n",
    "\n",
    "valid_DatasetName = f\"validScans_DataSet{num}\"\n",
    "valid_maskDatasetName = f\"validMaskScans_DataSet{num}\"\n",
    "\n",
    "test_fileName = f\"/media/dro/JHSeagate/FYP/jh_fyp_work/3D_UNet/trials/numpyDatasets/numPyArrays/test_Scans/test_DS{num}.hdf5\"\n",
    "test_maskfileName = f\"/media/dro/JHSeagate/FYP/jh_fyp_work/3D_UNet/trials/numpyDatasets/numPyArrays/test_Mask_Scans/test_maskDS{num}.hdf5\"\n",
    "\n",
    "test_DatasetName = f\"testScans_DataSet{num}\"\n",
    "test_maskDatasetName = f\"testMaskScans_DataSet{num}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3861280c",
   "metadata": {},
   "outputs": [],
   "source": [
    "with h5py.File(train_fileName, 'r') as hf: # File Dir\n",
    "    train_array = hf[train_DatasetName][:]\n",
    "    \n",
    "with h5py.File(train_maskfileName,'r') as hf:\n",
    "    train_mask_array = hf[train_maskDatasetName][:]\n",
    "    \n",
    "with h5py.File(valid_fileName, 'r') as hf: # File Dir\n",
    "    valid_array = hf[valid_DatasetName][:]\n",
    "    \n",
    "with h5py.File(valid_maskfileName,'r') as hf:\n",
    "    valid_mask_array = hf[valid_maskDatasetName][:]\n",
    "    \n",
    "with h5py.File(test_fileName, 'r') as hf: # File Dir\n",
    "    test_array = hf[test_DatasetName][:]\n",
    "    \n",
    "with h5py.File(test_maskfileName,'r') as hf:\n",
    "    test_mask_array = hf[test_maskDatasetName][:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "740eb6d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2955, 64, 64, 96)\n",
      "(1125, 64, 64, 96)\n",
      "(1480, 64, 64, 96)\n"
     ]
    }
   ],
   "source": [
    "print(train_array.shape)\n",
    "print(valid_mask_array.shape)\n",
    "print(test_array.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "37839a0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_array = np.concatenate((train_array, valid_array,test_array))\n",
    "train_mask_array = np.concatenate((train_mask_array,valid_mask_array, test_mask_array))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a724889c",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_array = np.expand_dims(train_array, axis=4)\n",
    "train_mask_array = np.expand_dims(train_mask_array, axis=4)\n",
    "\n",
    "valid_array = np.expand_dims(valid_array, axis=4)\n",
    "valid_mask_array = np.expand_dims(valid_mask_array, axis=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fc327720",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5560, 64, 64, 96, 1)\n",
      "(1125, 64, 64, 96, 1)\n"
     ]
    }
   ],
   "source": [
    "print(train_array.shape)\n",
    "print(valid_mask_array.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a0d0efeb",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-03-02 16:51:45.058219: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-03-02 16:51:45.530662: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1525] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 38414 MB memory:  -> device: 0, name: A100-SXM4-40GB, pci bus id: 0000:01:00.0, compute capability: 8.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sigmoid\n",
      "Model: \"U-Net\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_1 (InputLayer)           [(None, 64, 64, 96,  0           []                               \n",
      "                                 1)]                                                              \n",
      "                                                                                                  \n",
      " conv3d (Conv3D)                (None, 64, 64, 96,   896         ['input_1[0][0]']                \n",
      "                                32)                                                               \n",
      "                                                                                                  \n",
      " batch_normalization (BatchNorm  (None, 64, 64, 96,   128        ['conv3d[0][0]']                 \n",
      " alization)                     32)                                                               \n",
      "                                                                                                  \n",
      " leaky_re_lu (LeakyReLU)        (None, 64, 64, 96,   0           ['batch_normalization[0][0]']    \n",
      "                                32)                                                               \n",
      "                                                                                                  \n",
      " conv3d_1 (Conv3D)              (None, 64, 64, 96,   27680       ['leaky_re_lu[0][0]']            \n",
      "                                32)                                                               \n",
      "                                                                                                  \n",
      " batch_normalization_1 (BatchNo  (None, 64, 64, 96,   128        ['conv3d_1[0][0]']               \n",
      " rmalization)                   32)                                                               \n",
      "                                                                                                  \n",
      " leaky_re_lu_1 (LeakyReLU)      (None, 64, 64, 96,   0           ['batch_normalization_1[0][0]']  \n",
      "                                32)                                                               \n",
      "                                                                                                  \n",
      " max_pooling3d (MaxPooling3D)   (None, 32, 32, 48,   0           ['leaky_re_lu_1[0][0]']          \n",
      "                                32)                                                               \n",
      "                                                                                                  \n",
      " conv3d_2 (Conv3D)              (None, 32, 32, 48,   55360       ['max_pooling3d[0][0]']          \n",
      "                                64)                                                               \n",
      "                                                                                                  \n",
      " batch_normalization_2 (BatchNo  (None, 32, 32, 48,   256        ['conv3d_2[0][0]']               \n",
      " rmalization)                   64)                                                               \n",
      "                                                                                                  \n",
      " leaky_re_lu_2 (LeakyReLU)      (None, 32, 32, 48,   0           ['batch_normalization_2[0][0]']  \n",
      "                                64)                                                               \n",
      "                                                                                                  \n",
      " conv3d_3 (Conv3D)              (None, 32, 32, 48,   110656      ['leaky_re_lu_2[0][0]']          \n",
      "                                64)                                                               \n",
      "                                                                                                  \n",
      " batch_normalization_3 (BatchNo  (None, 32, 32, 48,   256        ['conv3d_3[0][0]']               \n",
      " rmalization)                   64)                                                               \n",
      "                                                                                                  \n",
      " leaky_re_lu_3 (LeakyReLU)      (None, 32, 32, 48,   0           ['batch_normalization_3[0][0]']  \n",
      "                                64)                                                               \n",
      "                                                                                                  \n",
      " max_pooling3d_1 (MaxPooling3D)  (None, 16, 16, 24,   0          ['leaky_re_lu_3[0][0]']          \n",
      "                                64)                                                               \n",
      "                                                                                                  \n",
      " conv3d_4 (Conv3D)              (None, 16, 16, 24,   221312      ['max_pooling3d_1[0][0]']        \n",
      "                                128)                                                              \n",
      "                                                                                                  \n",
      " batch_normalization_4 (BatchNo  (None, 16, 16, 24,   512        ['conv3d_4[0][0]']               \n",
      " rmalization)                   128)                                                              \n",
      "                                                                                                  \n",
      " leaky_re_lu_4 (LeakyReLU)      (None, 16, 16, 24,   0           ['batch_normalization_4[0][0]']  \n",
      "                                128)                                                              \n",
      "                                                                                                  \n",
      " conv3d_5 (Conv3D)              (None, 16, 16, 24,   442496      ['leaky_re_lu_4[0][0]']          \n",
      "                                128)                                                              \n",
      "                                                                                                  \n",
      " batch_normalization_5 (BatchNo  (None, 16, 16, 24,   512        ['conv3d_5[0][0]']               \n",
      " rmalization)                   128)                                                              \n",
      "                                                                                                  \n",
      " leaky_re_lu_5 (LeakyReLU)      (None, 16, 16, 24,   0           ['batch_normalization_5[0][0]']  \n",
      "                                128)                                                              \n",
      "                                                                                                  \n",
      " max_pooling3d_2 (MaxPooling3D)  (None, 8, 8, 12, 12  0          ['leaky_re_lu_5[0][0]']          \n",
      "                                8)                                                                \n",
      "                                                                                                  \n",
      " conv3d_6 (Conv3D)              (None, 8, 8, 12, 25  884992      ['max_pooling3d_2[0][0]']        \n",
      "                                6)                                                                \n",
      "                                                                                                  \n",
      " batch_normalization_6 (BatchNo  (None, 8, 8, 12, 25  1024       ['conv3d_6[0][0]']               \n",
      " rmalization)                   6)                                                                \n",
      "                                                                                                  \n",
      " leaky_re_lu_6 (LeakyReLU)      (None, 8, 8, 12, 25  0           ['batch_normalization_6[0][0]']  \n",
      "                                6)                                                                \n",
      "                                                                                                  \n",
      " conv3d_7 (Conv3D)              (None, 8, 8, 12, 25  1769728     ['leaky_re_lu_6[0][0]']          \n",
      "                                6)                                                                \n",
      "                                                                                                  \n",
      " batch_normalization_7 (BatchNo  (None, 8, 8, 12, 25  1024       ['conv3d_7[0][0]']               \n",
      " rmalization)                   6)                                                                \n",
      "                                                                                                  \n",
      " leaky_re_lu_7 (LeakyReLU)      (None, 8, 8, 12, 25  0           ['batch_normalization_7[0][0]']  \n",
      "                                6)                                                                \n",
      "                                                                                                  \n",
      " max_pooling3d_3 (MaxPooling3D)  (None, 4, 4, 6, 256  0          ['leaky_re_lu_7[0][0]']          \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " conv3d_8 (Conv3D)              (None, 4, 4, 6, 512  3539456     ['max_pooling3d_3[0][0]']        \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " batch_normalization_8 (BatchNo  (None, 4, 4, 6, 512  2048       ['conv3d_8[0][0]']               \n",
      " rmalization)                   )                                                                 \n",
      "                                                                                                  \n",
      " leaky_re_lu_8 (LeakyReLU)      (None, 4, 4, 6, 512  0           ['batch_normalization_8[0][0]']  \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " conv3d_9 (Conv3D)              (None, 4, 4, 6, 512  7078400     ['leaky_re_lu_8[0][0]']          \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " batch_normalization_9 (BatchNo  (None, 4, 4, 6, 512  2048       ['conv3d_9[0][0]']               \n",
      " rmalization)                   )                                                                 \n",
      "                                                                                                  \n",
      " leaky_re_lu_9 (LeakyReLU)      (None, 4, 4, 6, 512  0           ['batch_normalization_9[0][0]']  \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " max_pooling3d_4 (MaxPooling3D)  (None, 2, 2, 3, 512  0          ['leaky_re_lu_9[0][0]']          \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " conv3d_10 (Conv3D)             (None, 2, 2, 3, 102  14156800    ['max_pooling3d_4[0][0]']        \n",
      "                                4)                                                                \n",
      "                                                                                                  \n",
      " batch_normalization_10 (BatchN  (None, 2, 2, 3, 102  4096       ['conv3d_10[0][0]']              \n",
      " ormalization)                  4)                                                                \n",
      "                                                                                                  \n",
      " leaky_re_lu_10 (LeakyReLU)     (None, 2, 2, 3, 102  0           ['batch_normalization_10[0][0]'] \n",
      "                                4)                                                                \n",
      "                                                                                                  \n",
      " conv3d_11 (Conv3D)             (None, 2, 2, 3, 102  28312576    ['leaky_re_lu_10[0][0]']         \n",
      "                                4)                                                                \n",
      "                                                                                                  \n",
      " batch_normalization_11 (BatchN  (None, 2, 2, 3, 102  4096       ['conv3d_11[0][0]']              \n",
      " ormalization)                  4)                                                                \n",
      "                                                                                                  \n",
      " leaky_re_lu_11 (LeakyReLU)     (None, 2, 2, 3, 102  0           ['batch_normalization_11[0][0]'] \n",
      "                                4)                                                                \n",
      "                                                                                                  \n",
      " conv3d_transpose (Conv3DTransp  (None, 4, 4, 6, 512  4194816    ['leaky_re_lu_11[0][0]']         \n",
      " ose)                           )                                                                 \n",
      "                                                                                                  \n",
      " concatenate (Concatenate)      (None, 4, 4, 6, 102  0           ['conv3d_transpose[0][0]',       \n",
      "                                4)                                'leaky_re_lu_9[0][0]']          \n",
      "                                                                                                  \n",
      " conv3d_12 (Conv3D)             (None, 4, 4, 6, 512  14156288    ['concatenate[0][0]']            \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " batch_normalization_12 (BatchN  (None, 4, 4, 6, 512  2048       ['conv3d_12[0][0]']              \n",
      " ormalization)                  )                                                                 \n",
      "                                                                                                  \n",
      " leaky_re_lu_12 (LeakyReLU)     (None, 4, 4, 6, 512  0           ['batch_normalization_12[0][0]'] \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " conv3d_13 (Conv3D)             (None, 4, 4, 6, 512  7078400     ['leaky_re_lu_12[0][0]']         \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " batch_normalization_13 (BatchN  (None, 4, 4, 6, 512  2048       ['conv3d_13[0][0]']              \n",
      " ormalization)                  )                                                                 \n",
      "                                                                                                  \n",
      " leaky_re_lu_13 (LeakyReLU)     (None, 4, 4, 6, 512  0           ['batch_normalization_13[0][0]'] \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " conv3d_transpose_1 (Conv3DTran  (None, 8, 8, 12, 25  1048832    ['leaky_re_lu_13[0][0]']         \n",
      " spose)                         6)                                                                \n",
      "                                                                                                  \n",
      " concatenate_1 (Concatenate)    (None, 8, 8, 12, 51  0           ['conv3d_transpose_1[0][0]',     \n",
      "                                2)                                'leaky_re_lu_7[0][0]']          \n",
      "                                                                                                  \n",
      " conv3d_14 (Conv3D)             (None, 8, 8, 12, 25  3539200     ['concatenate_1[0][0]']          \n",
      "                                6)                                                                \n",
      "                                                                                                  \n",
      " batch_normalization_14 (BatchN  (None, 8, 8, 12, 25  1024       ['conv3d_14[0][0]']              \n",
      " ormalization)                  6)                                                                \n",
      "                                                                                                  \n",
      " leaky_re_lu_14 (LeakyReLU)     (None, 8, 8, 12, 25  0           ['batch_normalization_14[0][0]'] \n",
      "                                6)                                                                \n",
      "                                                                                                  \n",
      " conv3d_15 (Conv3D)             (None, 8, 8, 12, 25  1769728     ['leaky_re_lu_14[0][0]']         \n",
      "                                6)                                                                \n",
      "                                                                                                  \n",
      " batch_normalization_15 (BatchN  (None, 8, 8, 12, 25  1024       ['conv3d_15[0][0]']              \n",
      " ormalization)                  6)                                                                \n",
      "                                                                                                  \n",
      " leaky_re_lu_15 (LeakyReLU)     (None, 8, 8, 12, 25  0           ['batch_normalization_15[0][0]'] \n",
      "                                6)                                                                \n",
      "                                                                                                  \n",
      " conv3d_transpose_2 (Conv3DTran  (None, 16, 16, 24,   262272     ['leaky_re_lu_15[0][0]']         \n",
      " spose)                         128)                                                              \n",
      "                                                                                                  \n",
      " concatenate_2 (Concatenate)    (None, 16, 16, 24,   0           ['conv3d_transpose_2[0][0]',     \n",
      "                                256)                              'leaky_re_lu_5[0][0]']          \n",
      "                                                                                                  \n",
      " conv3d_16 (Conv3D)             (None, 16, 16, 24,   884864      ['concatenate_2[0][0]']          \n",
      "                                128)                                                              \n",
      "                                                                                                  \n",
      " batch_normalization_16 (BatchN  (None, 16, 16, 24,   512        ['conv3d_16[0][0]']              \n",
      " ormalization)                  128)                                                              \n",
      "                                                                                                  \n",
      " leaky_re_lu_16 (LeakyReLU)     (None, 16, 16, 24,   0           ['batch_normalization_16[0][0]'] \n",
      "                                128)                                                              \n",
      "                                                                                                  \n",
      " conv3d_17 (Conv3D)             (None, 16, 16, 24,   442496      ['leaky_re_lu_16[0][0]']         \n",
      "                                128)                                                              \n",
      "                                                                                                  \n",
      " batch_normalization_17 (BatchN  (None, 16, 16, 24,   512        ['conv3d_17[0][0]']              \n",
      " ormalization)                  128)                                                              \n",
      "                                                                                                  \n",
      " leaky_re_lu_17 (LeakyReLU)     (None, 16, 16, 24,   0           ['batch_normalization_17[0][0]'] \n",
      "                                128)                                                              \n",
      "                                                                                                  \n",
      " conv3d_transpose_3 (Conv3DTran  (None, 32, 32, 48,   65600      ['leaky_re_lu_17[0][0]']         \n",
      " spose)                         64)                                                               \n",
      "                                                                                                  \n",
      " concatenate_3 (Concatenate)    (None, 32, 32, 48,   0           ['conv3d_transpose_3[0][0]',     \n",
      "                                128)                              'leaky_re_lu_3[0][0]']          \n",
      "                                                                                                  \n",
      " conv3d_18 (Conv3D)             (None, 32, 32, 48,   221248      ['concatenate_3[0][0]']          \n",
      "                                64)                                                               \n",
      "                                                                                                  \n",
      " batch_normalization_18 (BatchN  (None, 32, 32, 48,   256        ['conv3d_18[0][0]']              \n",
      " ormalization)                  64)                                                               \n",
      "                                                                                                  \n",
      " leaky_re_lu_18 (LeakyReLU)     (None, 32, 32, 48,   0           ['batch_normalization_18[0][0]'] \n",
      "                                64)                                                               \n",
      "                                                                                                  \n",
      " conv3d_19 (Conv3D)             (None, 32, 32, 48,   110656      ['leaky_re_lu_18[0][0]']         \n",
      "                                64)                                                               \n",
      "                                                                                                  \n",
      " batch_normalization_19 (BatchN  (None, 32, 32, 48,   256        ['conv3d_19[0][0]']              \n",
      " ormalization)                  64)                                                               \n",
      "                                                                                                  \n",
      " leaky_re_lu_19 (LeakyReLU)     (None, 32, 32, 48,   0           ['batch_normalization_19[0][0]'] \n",
      "                                64)                                                               \n",
      "                                                                                                  \n",
      " conv3d_transpose_4 (Conv3DTran  (None, 64, 64, 96,   16416      ['leaky_re_lu_19[0][0]']         \n",
      " spose)                         32)                                                               \n",
      "                                                                                                  \n",
      " concatenate_4 (Concatenate)    (None, 64, 64, 96,   0           ['conv3d_transpose_4[0][0]',     \n",
      "                                64)                               'leaky_re_lu_1[0][0]']          \n",
      "                                                                                                  \n",
      " conv3d_20 (Conv3D)             (None, 64, 64, 96,   55328       ['concatenate_4[0][0]']          \n",
      "                                32)                                                               \n",
      "                                                                                                  \n",
      " batch_normalization_20 (BatchN  (None, 64, 64, 96,   128        ['conv3d_20[0][0]']              \n",
      " ormalization)                  32)                                                               \n",
      "                                                                                                  \n",
      " leaky_re_lu_20 (LeakyReLU)     (None, 64, 64, 96,   0           ['batch_normalization_20[0][0]'] \n",
      "                                32)                                                               \n",
      "                                                                                                  \n",
      " conv3d_21 (Conv3D)             (None, 64, 64, 96,   27680       ['leaky_re_lu_20[0][0]']         \n",
      "                                32)                                                               \n",
      "                                                                                                  \n",
      " batch_normalization_21 (BatchN  (None, 64, 64, 96,   128        ['conv3d_21[0][0]']              \n",
      " ormalization)                  32)                                                               \n",
      "                                                                                                  \n",
      " leaky_re_lu_21 (LeakyReLU)     (None, 64, 64, 96,   0           ['batch_normalization_21[0][0]'] \n",
      "                                32)                                                               \n",
      "                                                                                                  \n",
      " conv3d_22 (Conv3D)             (None, 64, 64, 96,   865         ['leaky_re_lu_21[0][0]']         \n",
      "                                1)                                                                \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 90,499,105\n",
      "Trainable params: 90,487,073\n",
      "Non-trainable params: 12,032\n",
      "__________________________________________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "LR = 0.001\n",
    "opt = tf.keras.optimizers.Nadam(LR)\n",
    "\n",
    "input_shape = (64,64,96,1)\n",
    "num_class = 1\n",
    "\n",
    "metrics = [allMetrics.dice_coef]\n",
    "\n",
    "model = UNetModel_3D.build_unet(input_shape, n_classes = num_class)\n",
    "model.compile(optimizer=opt, loss=loss.tversky_crossentropy, metrics=metrics)\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a224af39",
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_path = '/media/dro/JHSeagate/FYP/jh_fyp_work/3D_UNet/Final/CSVLogs/Model11_run2.csv'\n",
    "model_checkpoint_path = '/media/dro/JHSeagate/FYP/jh_fyp_work/3D_UNet/Final/SavedModels/Model11_run2.hdf5'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7977f06e",
   "metadata": {},
   "outputs": [],
   "source": [
    "my_callbacks = [\n",
    "    ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=8, mode = 'auto'),\n",
    "    EarlyStopping(monitor='val_loss', patience=30, restore_best_weights=True, mode = 'auto'),\n",
    "    CSVLogger(csv_path, separator=',', append=True),\n",
    "    ModelCheckpoint(filepath=model_checkpoint_path,\n",
    "                    monitor='val_loss',\n",
    "                    mode='auto',\n",
    "                    verbose=1,\n",
    "                    save_best_only= True)\n",
    "]\n",
    "#     CustomCallBack()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "12ac2431",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/300\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-03-02 16:52:45.570197: I tensorflow/stream_executor/cuda/cuda_dnn.cc:366] Loaded cuDNN version 8301\n",
      "2023-03-02 16:52:46.420432: W tensorflow/stream_executor/gpu/asm_compiler.cc:111] *** WARNING *** You are using ptxas 11.0.221, which is older than 11.1. ptxas before 11.1 is known to miscompile XLA code, leading to incorrect results or invalid-address errors.\n",
      "\n",
      "You may not need to update to CUDA 11.1; cherry-picking the ptxas binary is often sufficient.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1112/1112 [==============================] - ETA: 0s - loss: 0.8031 - dice_coef: 0.4659\n",
      "Epoch 00001: val_loss improved from inf to 0.71392, saving model to /media/dro/JHSeagate/FYP/jh_fyp_work/3D_UNet/Final/SavedModels/Model11_run2.hdf5\n",
      "1112/1112 [==============================] - 194s 165ms/step - loss: 0.8031 - dice_coef: 0.4659 - val_loss: 0.7139 - val_dice_coef: 0.5086 - lr: 0.0010\n",
      "Epoch 2/300\n",
      "1112/1112 [==============================] - ETA: 0s - loss: 0.6702 - dice_coef: 0.6451\n",
      "Epoch 00002: val_loss did not improve from 0.71392\n",
      "1112/1112 [==============================] - 178s 160ms/step - loss: 0.6702 - dice_coef: 0.6451 - val_loss: 1.6377 - val_dice_coef: 0.2394 - lr: 0.0010\n",
      "Epoch 3/300\n",
      "1112/1112 [==============================] - ETA: 0s - loss: 0.6345 - dice_coef: 0.6934\n",
      "Epoch 00003: val_loss did not improve from 0.71392\n",
      "1112/1112 [==============================] - 178s 160ms/step - loss: 0.6345 - dice_coef: 0.6934 - val_loss: 0.7526 - val_dice_coef: 0.5162 - lr: 0.0010\n",
      "Epoch 4/300\n",
      "1112/1112 [==============================] - ETA: 0s - loss: 0.6162 - dice_coef: 0.7112\n",
      "Epoch 00004: val_loss did not improve from 0.71392\n",
      "1112/1112 [==============================] - 177s 159ms/step - loss: 0.6162 - dice_coef: 0.7112 - val_loss: 1.1432 - val_dice_coef: 0.0335 - lr: 0.0010\n",
      "Epoch 5/300\n",
      "1112/1112 [==============================] - ETA: 0s - loss: 0.5999 - dice_coef: 0.7301\n",
      "Epoch 00005: val_loss did not improve from 0.71392\n",
      "1112/1112 [==============================] - 177s 159ms/step - loss: 0.5999 - dice_coef: 0.7301 - val_loss: 1.3747 - val_dice_coef: 0.0021 - lr: 0.0010\n",
      "Epoch 6/300\n",
      "1112/1112 [==============================] - ETA: 0s - loss: 0.5881 - dice_coef: 0.7434\n",
      "Epoch 00006: val_loss did not improve from 0.71392\n",
      "1112/1112 [==============================] - 177s 159ms/step - loss: 0.5881 - dice_coef: 0.7434 - val_loss: 0.9937 - val_dice_coef: 0.1816 - lr: 0.0010\n",
      "Epoch 7/300\n",
      "1112/1112 [==============================] - ETA: 0s - loss: 0.5772 - dice_coef: 0.7652\n",
      "Epoch 00007: val_loss did not improve from 0.71392\n",
      "1112/1112 [==============================] - 177s 159ms/step - loss: 0.5772 - dice_coef: 0.7652 - val_loss: 0.8323 - val_dice_coef: 0.3813 - lr: 0.0010\n",
      "Epoch 8/300\n",
      "1112/1112 [==============================] - ETA: 0s - loss: 0.5615 - dice_coef: 0.7737\n",
      "Epoch 00008: val_loss improved from 0.71392 to 0.63373, saving model to /media/dro/JHSeagate/FYP/jh_fyp_work/3D_UNet/Final/SavedModels/Model11_run2.hdf5\n",
      "1112/1112 [==============================] - 178s 160ms/step - loss: 0.5615 - dice_coef: 0.7737 - val_loss: 0.6337 - val_dice_coef: 0.5904 - lr: 0.0010\n",
      "Epoch 9/300\n",
      "1112/1112 [==============================] - ETA: 0s - loss: 0.5599 - dice_coef: 0.7801\n",
      "Epoch 00009: val_loss did not improve from 0.63373\n",
      "1112/1112 [==============================] - 177s 159ms/step - loss: 0.5599 - dice_coef: 0.7801 - val_loss: 0.9397 - val_dice_coef: 0.1261 - lr: 0.0010\n",
      "Epoch 10/300\n",
      "1112/1112 [==============================] - ETA: 0s - loss: 0.4552 - dice_coef: 0.7896\n",
      "Epoch 00010: val_loss did not improve from 0.63373\n",
      "1112/1112 [==============================] - 176s 158ms/step - loss: 0.4552 - dice_coef: 0.7896 - val_loss: 0.9394 - val_dice_coef: 0.3944 - lr: 0.0010\n",
      "Epoch 11/300\n",
      "1112/1112 [==============================] - ETA: 0s - loss: 0.4510 - dice_coef: 0.7855\n",
      "Epoch 00011: val_loss did not improve from 0.63373\n",
      "1112/1112 [==============================] - 177s 159ms/step - loss: 0.4510 - dice_coef: 0.7855 - val_loss: 0.7623 - val_dice_coef: 0.4747 - lr: 0.0010\n",
      "Epoch 12/300\n",
      "1112/1112 [==============================] - ETA: 0s - loss: 0.4523 - dice_coef: 0.7614\n",
      "Epoch 00012: val_loss did not improve from 0.63373\n",
      "1112/1112 [==============================] - 176s 159ms/step - loss: 0.4523 - dice_coef: 0.7614 - val_loss: 1.4298 - val_dice_coef: 0.2750 - lr: 0.0010\n",
      "Epoch 13/300\n",
      "1112/1112 [==============================] - ETA: 0s - loss: 0.3806 - dice_coef: 0.7623\n",
      "Epoch 00013: val_loss did not improve from 0.63373\n",
      "1112/1112 [==============================] - 176s 159ms/step - loss: 0.3806 - dice_coef: 0.7623 - val_loss: 1.6539 - val_dice_coef: 0.2416 - lr: 0.0010\n",
      "Epoch 14/300\n",
      "1112/1112 [==============================] - ETA: 0s - loss: 0.3958 - dice_coef: 0.7539\n",
      "Epoch 00014: val_loss improved from 0.63373 to 0.43021, saving model to /media/dro/JHSeagate/FYP/jh_fyp_work/3D_UNet/Final/SavedModels/Model11_run2.hdf5\n",
      "1112/1112 [==============================] - 178s 160ms/step - loss: 0.3958 - dice_coef: 0.7539 - val_loss: 0.4302 - val_dice_coef: 0.6218 - lr: 0.0010\n",
      "Epoch 15/300\n",
      "1112/1112 [==============================] - ETA: 0s - loss: 0.3646 - dice_coef: 0.7722\n",
      "Epoch 00015: val_loss did not improve from 0.43021\n",
      "1112/1112 [==============================] - 176s 158ms/step - loss: 0.3646 - dice_coef: 0.7722 - val_loss: 0.9800 - val_dice_coef: 0.0481 - lr: 0.0010\n",
      "Epoch 16/300\n",
      "1112/1112 [==============================] - ETA: 0s - loss: 0.3632 - dice_coef: 0.7645\n",
      "Epoch 00016: val_loss did not improve from 0.43021\n",
      "1112/1112 [==============================] - 177s 159ms/step - loss: 0.3632 - dice_coef: 0.7645 - val_loss: 1.3972 - val_dice_coef: 0.2989 - lr: 0.0010\n",
      "Epoch 17/300\n",
      "1112/1112 [==============================] - ETA: 0s - loss: 0.3559 - dice_coef: 0.7739\n",
      "Epoch 00017: val_loss did not improve from 0.43021\n",
      "1112/1112 [==============================] - 176s 158ms/step - loss: 0.3559 - dice_coef: 0.7739 - val_loss: 0.9024 - val_dice_coef: 0.4070 - lr: 0.0010\n",
      "Epoch 18/300\n",
      "1112/1112 [==============================] - ETA: 0s - loss: 0.3446 - dice_coef: 0.7793\n",
      "Epoch 00018: val_loss did not improve from 0.43021\n",
      "1112/1112 [==============================] - 176s 158ms/step - loss: 0.3446 - dice_coef: 0.7793 - val_loss: 0.6908 - val_dice_coef: 0.2062 - lr: 0.0010\n",
      "Epoch 19/300\n",
      "1112/1112 [==============================] - ETA: 0s - loss: 0.3241 - dice_coef: 0.7940\n",
      "Epoch 00019: val_loss did not improve from 0.43021\n",
      "1112/1112 [==============================] - 176s 158ms/step - loss: 0.3241 - dice_coef: 0.7940 - val_loss: 0.4307 - val_dice_coef: 0.6570 - lr: 0.0010\n",
      "Epoch 20/300\n",
      "1112/1112 [==============================] - ETA: 0s - loss: 0.3174 - dice_coef: 0.8045\n",
      "Epoch 00020: val_loss did not improve from 0.43021\n",
      "1112/1112 [==============================] - 177s 159ms/step - loss: 0.3174 - dice_coef: 0.8045 - val_loss: 0.5725 - val_dice_coef: 0.4830 - lr: 0.0010\n",
      "Epoch 21/300\n",
      "1112/1112 [==============================] - ETA: 0s - loss: 0.3112 - dice_coef: 0.8064\n",
      "Epoch 00021: val_loss improved from 0.43021 to 0.41691, saving model to /media/dro/JHSeagate/FYP/jh_fyp_work/3D_UNet/Final/SavedModels/Model11_run2.hdf5\n",
      "1112/1112 [==============================] - 177s 159ms/step - loss: 0.3112 - dice_coef: 0.8064 - val_loss: 0.4169 - val_dice_coef: 0.6147 - lr: 0.0010\n",
      "Epoch 22/300\n",
      "1112/1112 [==============================] - ETA: 0s - loss: 0.3269 - dice_coef: 0.7949\n",
      "Epoch 00022: val_loss did not improve from 0.41691\n",
      "1112/1112 [==============================] - 176s 158ms/step - loss: 0.3269 - dice_coef: 0.7949 - val_loss: 0.4689 - val_dice_coef: 0.5723 - lr: 0.0010\n",
      "Epoch 23/300\n",
      "1112/1112 [==============================] - ETA: 0s - loss: 0.3104 - dice_coef: 0.8103\n",
      "Epoch 00023: val_loss improved from 0.41691 to 0.33585, saving model to /media/dro/JHSeagate/FYP/jh_fyp_work/3D_UNet/Final/SavedModels/Model11_run2.hdf5\n",
      "1112/1112 [==============================] - 177s 160ms/step - loss: 0.3104 - dice_coef: 0.8103 - val_loss: 0.3359 - val_dice_coef: 0.7254 - lr: 0.0010\n",
      "Epoch 24/300\n",
      "1112/1112 [==============================] - ETA: 0s - loss: 0.3181 - dice_coef: 0.8054\n",
      "Epoch 00024: val_loss did not improve from 0.33585\n",
      "1112/1112 [==============================] - 176s 158ms/step - loss: 0.3181 - dice_coef: 0.8054 - val_loss: 0.3763 - val_dice_coef: 0.6402 - lr: 0.0010\n",
      "Epoch 25/300\n",
      "1112/1112 [==============================] - ETA: 0s - loss: 0.3044 - dice_coef: 0.8148\n",
      "Epoch 00025: val_loss did not improve from 0.33585\n",
      "1112/1112 [==============================] - 176s 158ms/step - loss: 0.3044 - dice_coef: 0.8148 - val_loss: 0.3547 - val_dice_coef: 0.6583 - lr: 0.0010\n",
      "Epoch 26/300\n",
      "1112/1112 [==============================] - ETA: 0s - loss: 0.3230 - dice_coef: 0.8063\n",
      "Epoch 00026: val_loss did not improve from 0.33585\n",
      "1112/1112 [==============================] - 175s 158ms/step - loss: 0.3230 - dice_coef: 0.8063 - val_loss: 0.4837 - val_dice_coef: 0.4964 - lr: 0.0010\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 27/300\n",
      "1112/1112 [==============================] - ETA: 0s - loss: 0.3109 - dice_coef: 0.8102\n",
      "Epoch 00027: val_loss did not improve from 0.33585\n",
      "1112/1112 [==============================] - 175s 158ms/step - loss: 0.3109 - dice_coef: 0.8102 - val_loss: 0.4395 - val_dice_coef: 0.6395 - lr: 0.0010\n",
      "Epoch 28/300\n",
      "1112/1112 [==============================] - ETA: 0s - loss: 0.3045 - dice_coef: 0.8124\n",
      "Epoch 00028: val_loss did not improve from 0.33585\n",
      "1112/1112 [==============================] - 175s 158ms/step - loss: 0.3045 - dice_coef: 0.8124 - val_loss: 0.6276 - val_dice_coef: 0.2750 - lr: 0.0010\n",
      "Epoch 29/300\n",
      "1112/1112 [==============================] - ETA: 0s - loss: 0.2957 - dice_coef: 0.8204\n",
      "Epoch 00029: val_loss improved from 0.33585 to 0.32009, saving model to /media/dro/JHSeagate/FYP/jh_fyp_work/3D_UNet/Final/SavedModels/Model11_run2.hdf5\n",
      "1112/1112 [==============================] - 177s 159ms/step - loss: 0.2957 - dice_coef: 0.8204 - val_loss: 0.3201 - val_dice_coef: 0.7216 - lr: 0.0010\n",
      "Epoch 30/300\n",
      "1112/1112 [==============================] - ETA: 0s - loss: 0.3053 - dice_coef: 0.8111\n",
      "Epoch 00030: val_loss did not improve from 0.32009\n",
      "1112/1112 [==============================] - 175s 158ms/step - loss: 0.3053 - dice_coef: 0.8111 - val_loss: 1.8646 - val_dice_coef: 0.2617 - lr: 0.0010\n",
      "Epoch 31/300\n",
      "1112/1112 [==============================] - ETA: 0s - loss: 0.3173 - dice_coef: 0.8043\n",
      "Epoch 00031: val_loss did not improve from 0.32009\n",
      "1112/1112 [==============================] - 175s 157ms/step - loss: 0.3173 - dice_coef: 0.8043 - val_loss: 0.4984 - val_dice_coef: 0.6075 - lr: 0.0010\n",
      "Epoch 32/300\n",
      "1112/1112 [==============================] - ETA: 0s - loss: 0.2850 - dice_coef: 0.8282\n",
      "Epoch 00032: val_loss improved from 0.32009 to 0.29809, saving model to /media/dro/JHSeagate/FYP/jh_fyp_work/3D_UNet/Final/SavedModels/Model11_run2.hdf5\n",
      "1112/1112 [==============================] - 177s 159ms/step - loss: 0.2850 - dice_coef: 0.8282 - val_loss: 0.2981 - val_dice_coef: 0.7325 - lr: 0.0010\n",
      "Epoch 33/300\n",
      "1112/1112 [==============================] - ETA: 0s - loss: 0.2816 - dice_coef: 0.8333\n",
      "Epoch 00033: val_loss did not improve from 0.29809\n",
      "1112/1112 [==============================] - 175s 158ms/step - loss: 0.2816 - dice_coef: 0.8333 - val_loss: 0.3158 - val_dice_coef: 0.6977 - lr: 0.0010\n",
      "Epoch 34/300\n",
      "1112/1112 [==============================] - ETA: 0s - loss: 0.2892 - dice_coef: 0.8223\n",
      "Epoch 00034: val_loss did not improve from 0.29809\n",
      "1112/1112 [==============================] - 175s 158ms/step - loss: 0.2892 - dice_coef: 0.8223 - val_loss: 0.5113 - val_dice_coef: 0.5934 - lr: 0.0010\n",
      "Epoch 35/300\n",
      "1112/1112 [==============================] - ETA: 0s - loss: 0.2760 - dice_coef: 0.8284\n",
      "Epoch 00035: val_loss did not improve from 0.29809\n",
      "1112/1112 [==============================] - 175s 157ms/step - loss: 0.2760 - dice_coef: 0.8284 - val_loss: 0.5678 - val_dice_coef: 0.5661 - lr: 0.0010\n",
      "Epoch 36/300\n",
      "1112/1112 [==============================] - ETA: 0s - loss: 0.2767 - dice_coef: 0.8357\n",
      "Epoch 00036: val_loss did not improve from 0.29809\n",
      "1112/1112 [==============================] - 175s 157ms/step - loss: 0.2767 - dice_coef: 0.8357 - val_loss: 1.1498 - val_dice_coef: 0.3590 - lr: 0.0010\n",
      "Epoch 37/300\n",
      "1112/1112 [==============================] - ETA: 0s - loss: 0.2845 - dice_coef: 0.8292\n",
      "Epoch 00037: val_loss did not improve from 0.29809\n",
      "1112/1112 [==============================] - 175s 157ms/step - loss: 0.2845 - dice_coef: 0.8292 - val_loss: 0.3234 - val_dice_coef: 0.7375 - lr: 0.0010\n",
      "Epoch 38/300\n",
      "1112/1112 [==============================] - ETA: 0s - loss: 0.2725 - dice_coef: 0.8379\n",
      "Epoch 00038: val_loss did not improve from 0.29809\n",
      "1112/1112 [==============================] - 175s 157ms/step - loss: 0.2725 - dice_coef: 0.8379 - val_loss: 0.8589 - val_dice_coef: 0.0858 - lr: 0.0010\n",
      "Epoch 39/300\n",
      "1112/1112 [==============================] - ETA: 0s - loss: 0.2990 - dice_coef: 0.8199\n",
      "Epoch 00039: val_loss did not improve from 0.29809\n",
      "1112/1112 [==============================] - 175s 157ms/step - loss: 0.2990 - dice_coef: 0.8199 - val_loss: 0.3191 - val_dice_coef: 0.7359 - lr: 0.0010\n",
      "Epoch 40/300\n",
      "1112/1112 [==============================] - ETA: 0s - loss: 0.2581 - dice_coef: 0.8484\n",
      "Epoch 00040: val_loss did not improve from 0.29809\n",
      "1112/1112 [==============================] - 175s 157ms/step - loss: 0.2581 - dice_coef: 0.8484 - val_loss: 0.3558 - val_dice_coef: 0.7012 - lr: 0.0010\n",
      "Epoch 41/300\n",
      "1112/1112 [==============================] - ETA: 0s - loss: 0.2542 - dice_coef: 0.8433\n",
      "Epoch 00041: val_loss improved from 0.29809 to 0.29084, saving model to /media/dro/JHSeagate/FYP/jh_fyp_work/3D_UNet/Final/SavedModels/Model11_run2.hdf5\n",
      "1112/1112 [==============================] - 176s 159ms/step - loss: 0.2542 - dice_coef: 0.8433 - val_loss: 0.2908 - val_dice_coef: 0.7698 - lr: 5.0000e-04\n",
      "Epoch 42/300\n",
      "1112/1112 [==============================] - ETA: 0s - loss: 0.2458 - dice_coef: 0.8488\n",
      "Epoch 00042: val_loss improved from 0.29084 to 0.26721, saving model to /media/dro/JHSeagate/FYP/jh_fyp_work/3D_UNet/Final/SavedModels/Model11_run2.hdf5\n",
      "1112/1112 [==============================] - 177s 159ms/step - loss: 0.2458 - dice_coef: 0.8488 - val_loss: 0.2672 - val_dice_coef: 0.7751 - lr: 5.0000e-04\n",
      "Epoch 43/300\n",
      "1112/1112 [==============================] - ETA: 0s - loss: 0.2307 - dice_coef: 0.8603\n",
      "Epoch 00043: val_loss did not improve from 0.26721\n",
      "1112/1112 [==============================] - 175s 157ms/step - loss: 0.2307 - dice_coef: 0.8603 - val_loss: 0.2766 - val_dice_coef: 0.7742 - lr: 5.0000e-04\n",
      "Epoch 44/300\n",
      "1112/1112 [==============================] - ETA: 0s - loss: 0.2225 - dice_coef: 0.8659\n",
      "Epoch 00044: val_loss did not improve from 0.26721\n",
      "1112/1112 [==============================] - 175s 157ms/step - loss: 0.2225 - dice_coef: 0.8659 - val_loss: 0.2743 - val_dice_coef: 0.7796 - lr: 5.0000e-04\n",
      "Epoch 45/300\n",
      "1112/1112 [==============================] - ETA: 0s - loss: 0.2262 - dice_coef: 0.8622\n",
      "Epoch 00045: val_loss did not improve from 0.26721\n",
      "1112/1112 [==============================] - 175s 158ms/step - loss: 0.2262 - dice_coef: 0.8622 - val_loss: 0.2973 - val_dice_coef: 0.6861 - lr: 5.0000e-04\n",
      "Epoch 46/300\n",
      "1112/1112 [==============================] - ETA: 0s - loss: 0.2261 - dice_coef: 0.8643\n",
      "Epoch 00046: val_loss improved from 0.26721 to 0.25559, saving model to /media/dro/JHSeagate/FYP/jh_fyp_work/3D_UNet/Final/SavedModels/Model11_run2.hdf5\n",
      "1112/1112 [==============================] - 176s 159ms/step - loss: 0.2261 - dice_coef: 0.8643 - val_loss: 0.2556 - val_dice_coef: 0.7716 - lr: 5.0000e-04\n",
      "Epoch 47/300\n",
      "1112/1112 [==============================] - ETA: 0s - loss: 0.2234 - dice_coef: 0.8670\n",
      "Epoch 00047: val_loss improved from 0.25559 to 0.25376, saving model to /media/dro/JHSeagate/FYP/jh_fyp_work/3D_UNet/Final/SavedModels/Model11_run2.hdf5\n",
      "1112/1112 [==============================] - 177s 159ms/step - loss: 0.2234 - dice_coef: 0.8670 - val_loss: 0.2538 - val_dice_coef: 0.7885 - lr: 5.0000e-04\n",
      "Epoch 48/300\n",
      "1112/1112 [==============================] - ETA: 0s - loss: 0.2401 - dice_coef: 0.8580\n",
      "Epoch 00048: val_loss did not improve from 0.25376\n",
      "1112/1112 [==============================] - 175s 157ms/step - loss: 0.2401 - dice_coef: 0.8580 - val_loss: 0.3190 - val_dice_coef: 0.7162 - lr: 5.0000e-04\n",
      "Epoch 49/300\n",
      "1112/1112 [==============================] - ETA: 0s - loss: 0.2288 - dice_coef: 0.8652\n",
      "Epoch 00049: val_loss improved from 0.25376 to 0.24884, saving model to /media/dro/JHSeagate/FYP/jh_fyp_work/3D_UNet/Final/SavedModels/Model11_run2.hdf5\n",
      "1112/1112 [==============================] - 177s 159ms/step - loss: 0.2288 - dice_coef: 0.8652 - val_loss: 0.2488 - val_dice_coef: 0.7890 - lr: 5.0000e-04\n",
      "Epoch 50/300\n",
      "1112/1112 [==============================] - ETA: 0s - loss: 0.2155 - dice_coef: 0.8751\n",
      "Epoch 00050: val_loss did not improve from 0.24884\n",
      "1112/1112 [==============================] - 175s 157ms/step - loss: 0.2155 - dice_coef: 0.8751 - val_loss: 0.3213 - val_dice_coef: 0.6324 - lr: 5.0000e-04\n",
      "Epoch 51/300\n",
      "1112/1112 [==============================] - ETA: 0s - loss: 0.2172 - dice_coef: 0.8712\n",
      "Epoch 00051: val_loss did not improve from 0.24884\n",
      "1112/1112 [==============================] - 174s 157ms/step - loss: 0.2172 - dice_coef: 0.8712 - val_loss: 0.2668 - val_dice_coef: 0.7538 - lr: 5.0000e-04\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 52/300\n",
      "1112/1112 [==============================] - ETA: 0s - loss: 0.2120 - dice_coef: 0.8741\n",
      "Epoch 00052: val_loss did not improve from 0.24884\n",
      "1112/1112 [==============================] - 174s 157ms/step - loss: 0.2120 - dice_coef: 0.8741 - val_loss: 0.2569 - val_dice_coef: 0.7403 - lr: 5.0000e-04\n",
      "Epoch 53/300\n",
      "1112/1112 [==============================] - ETA: 0s - loss: 0.2130 - dice_coef: 0.8747\n",
      "Epoch 00053: val_loss did not improve from 0.24884\n",
      "1112/1112 [==============================] - 175s 157ms/step - loss: 0.2130 - dice_coef: 0.8747 - val_loss: 0.2764 - val_dice_coef: 0.7887 - lr: 5.0000e-04\n",
      "Epoch 54/300\n",
      "1112/1112 [==============================] - ETA: 0s - loss: 0.2067 - dice_coef: 0.8778\n",
      "Epoch 00054: val_loss did not improve from 0.24884\n",
      "1112/1112 [==============================] - 174s 157ms/step - loss: 0.2067 - dice_coef: 0.8778 - val_loss: 0.4248 - val_dice_coef: 0.4894 - lr: 5.0000e-04\n",
      "Epoch 55/300\n",
      "1112/1112 [==============================] - ETA: 0s - loss: 0.2081 - dice_coef: 0.8734\n",
      "Epoch 00055: val_loss improved from 0.24884 to 0.24264, saving model to /media/dro/JHSeagate/FYP/jh_fyp_work/3D_UNet/Final/SavedModels/Model11_run2.hdf5\n",
      "1112/1112 [==============================] - 176s 159ms/step - loss: 0.2081 - dice_coef: 0.8734 - val_loss: 0.2426 - val_dice_coef: 0.8061 - lr: 5.0000e-04\n",
      "Epoch 56/300\n",
      "1112/1112 [==============================] - ETA: 0s - loss: 0.1996 - dice_coef: 0.8785\n",
      "Epoch 00056: val_loss did not improve from 0.24264\n",
      "1112/1112 [==============================] - 175s 157ms/step - loss: 0.1996 - dice_coef: 0.8785 - val_loss: 0.2618 - val_dice_coef: 0.7869 - lr: 5.0000e-04\n",
      "Epoch 57/300\n",
      "1112/1112 [==============================] - ETA: 0s - loss: 0.2140 - dice_coef: 0.8712\n",
      "Epoch 00057: val_loss did not improve from 0.24264\n",
      "1112/1112 [==============================] - 175s 157ms/step - loss: 0.2140 - dice_coef: 0.8712 - val_loss: 0.2693 - val_dice_coef: 0.7556 - lr: 5.0000e-04\n",
      "Epoch 58/300\n",
      "1112/1112 [==============================] - ETA: 0s - loss: 0.2126 - dice_coef: 0.8700\n",
      "Epoch 00058: val_loss did not improve from 0.24264\n",
      "1112/1112 [==============================] - 175s 157ms/step - loss: 0.2126 - dice_coef: 0.8700 - val_loss: 0.3395 - val_dice_coef: 0.6713 - lr: 5.0000e-04\n",
      "Epoch 59/300\n",
      "1112/1112 [==============================] - ETA: 0s - loss: 0.2077 - dice_coef: 0.8740\n",
      "Epoch 00059: val_loss did not improve from 0.24264\n",
      "1112/1112 [==============================] - 174s 157ms/step - loss: 0.2077 - dice_coef: 0.8740 - val_loss: 0.2746 - val_dice_coef: 0.7894 - lr: 5.0000e-04\n",
      "Epoch 60/300\n",
      "1112/1112 [==============================] - ETA: 0s - loss: 0.1974 - dice_coef: 0.8785\n",
      "Epoch 00060: val_loss did not improve from 0.24264\n",
      "1112/1112 [==============================] - 175s 157ms/step - loss: 0.1974 - dice_coef: 0.8785 - val_loss: 0.3115 - val_dice_coef: 0.7588 - lr: 5.0000e-04\n",
      "Epoch 61/300\n",
      "1112/1112 [==============================] - ETA: 0s - loss: 0.2027 - dice_coef: 0.8731\n",
      "Epoch 00061: val_loss improved from 0.24264 to 0.23208, saving model to /media/dro/JHSeagate/FYP/jh_fyp_work/3D_UNet/Final/SavedModels/Model11_run2.hdf5\n",
      "1112/1112 [==============================] - 176s 158ms/step - loss: 0.2027 - dice_coef: 0.8731 - val_loss: 0.2321 - val_dice_coef: 0.7878 - lr: 5.0000e-04\n",
      "Epoch 62/300\n",
      "1112/1112 [==============================] - ETA: 0s - loss: 0.1898 - dice_coef: 0.8837\n",
      "Epoch 00062: val_loss did not improve from 0.23208\n",
      "1112/1112 [==============================] - 174s 157ms/step - loss: 0.1898 - dice_coef: 0.8837 - val_loss: 0.2507 - val_dice_coef: 0.8058 - lr: 5.0000e-04\n",
      "Epoch 63/300\n",
      "1112/1112 [==============================] - ETA: 0s - loss: 0.1934 - dice_coef: 0.8763\n",
      "Epoch 00063: val_loss did not improve from 0.23208\n",
      "1112/1112 [==============================] - 175s 157ms/step - loss: 0.1934 - dice_coef: 0.8763 - val_loss: 0.2820 - val_dice_coef: 0.6924 - lr: 5.0000e-04\n",
      "Epoch 64/300\n",
      "1112/1112 [==============================] - ETA: 0s - loss: 0.1825 - dice_coef: 0.8843\n",
      "Epoch 00064: val_loss improved from 0.23208 to 0.22178, saving model to /media/dro/JHSeagate/FYP/jh_fyp_work/3D_UNet/Final/SavedModels/Model11_run2.hdf5\n",
      "1112/1112 [==============================] - 176s 159ms/step - loss: 0.1825 - dice_coef: 0.8843 - val_loss: 0.2218 - val_dice_coef: 0.8264 - lr: 5.0000e-04\n",
      "Epoch 65/300\n",
      "1112/1112 [==============================] - ETA: 0s - loss: 0.1901 - dice_coef: 0.8820\n",
      "Epoch 00065: val_loss did not improve from 0.22178\n",
      "1112/1112 [==============================] - 175s 157ms/step - loss: 0.1901 - dice_coef: 0.8820 - val_loss: 0.2493 - val_dice_coef: 0.7423 - lr: 5.0000e-04\n",
      "Epoch 66/300\n",
      "1112/1112 [==============================] - ETA: 0s - loss: 0.1899 - dice_coef: 0.8845\n",
      "Epoch 00066: val_loss did not improve from 0.22178\n",
      "1112/1112 [==============================] - 175s 157ms/step - loss: 0.1899 - dice_coef: 0.8845 - val_loss: 0.2404 - val_dice_coef: 0.7991 - lr: 5.0000e-04\n",
      "Epoch 67/300\n",
      "1112/1112 [==============================] - ETA: 0s - loss: 0.1891 - dice_coef: 0.8833\n",
      "Epoch 00067: val_loss did not improve from 0.22178\n",
      "1112/1112 [==============================] - 175s 157ms/step - loss: 0.1891 - dice_coef: 0.8833 - val_loss: 0.2946 - val_dice_coef: 0.7759 - lr: 5.0000e-04\n",
      "Epoch 68/300\n",
      "1112/1112 [==============================] - ETA: 0s - loss: 0.1889 - dice_coef: 0.8804\n",
      "Epoch 00068: val_loss did not improve from 0.22178\n",
      "1112/1112 [==============================] - 175s 157ms/step - loss: 0.1889 - dice_coef: 0.8804 - val_loss: 0.2256 - val_dice_coef: 0.8203 - lr: 5.0000e-04\n",
      "Epoch 69/300\n",
      "1112/1112 [==============================] - ETA: 0s - loss: 0.1818 - dice_coef: 0.8881\n",
      "Epoch 00069: val_loss did not improve from 0.22178\n",
      "1112/1112 [==============================] - 175s 157ms/step - loss: 0.1818 - dice_coef: 0.8881 - val_loss: 0.2264 - val_dice_coef: 0.8110 - lr: 5.0000e-04\n",
      "Epoch 70/300\n",
      "1112/1112 [==============================] - ETA: 0s - loss: 0.1681 - dice_coef: 0.8929\n",
      "Epoch 00070: val_loss did not improve from 0.22178\n",
      "1112/1112 [==============================] - 175s 157ms/step - loss: 0.1681 - dice_coef: 0.8929 - val_loss: 0.3252 - val_dice_coef: 0.6157 - lr: 5.0000e-04\n",
      "Epoch 71/300\n",
      "1112/1112 [==============================] - ETA: 0s - loss: 0.1833 - dice_coef: 0.8833\n",
      "Epoch 00071: val_loss did not improve from 0.22178\n",
      "1112/1112 [==============================] - 175s 157ms/step - loss: 0.1833 - dice_coef: 0.8833 - val_loss: 0.3024 - val_dice_coef: 0.7675 - lr: 5.0000e-04\n",
      "Epoch 72/300\n",
      "1112/1112 [==============================] - ETA: 0s - loss: 0.1751 - dice_coef: 0.8911\n",
      "Epoch 00072: val_loss did not improve from 0.22178\n",
      "1112/1112 [==============================] - 175s 157ms/step - loss: 0.1751 - dice_coef: 0.8911 - val_loss: 0.2635 - val_dice_coef: 0.8040 - lr: 5.0000e-04\n",
      "Epoch 73/300\n",
      "1112/1112 [==============================] - ETA: 0s - loss: 0.1592 - dice_coef: 0.8971\n",
      "Epoch 00073: val_loss did not improve from 0.22178\n",
      "1112/1112 [==============================] - 175s 157ms/step - loss: 0.1592 - dice_coef: 0.8971 - val_loss: 0.2799 - val_dice_coef: 0.7920 - lr: 2.5000e-04\n",
      "Epoch 74/300\n",
      "1112/1112 [==============================] - ETA: 0s - loss: 0.1531 - dice_coef: 0.8958\n",
      "Epoch 00074: val_loss did not improve from 0.22178\n",
      "1112/1112 [==============================] - 175s 157ms/step - loss: 0.1531 - dice_coef: 0.8958 - val_loss: 0.2559 - val_dice_coef: 0.8046 - lr: 2.5000e-04\n",
      "Epoch 75/300\n",
      "1112/1112 [==============================] - ETA: 0s - loss: 0.1468 - dice_coef: 0.8973\n",
      "Epoch 00075: val_loss improved from 0.22178 to 0.20444, saving model to /media/dro/JHSeagate/FYP/jh_fyp_work/3D_UNet/Final/SavedModels/Model11_run2.hdf5\n",
      "1112/1112 [==============================] - 177s 159ms/step - loss: 0.1468 - dice_coef: 0.8973 - val_loss: 0.2044 - val_dice_coef: 0.8285 - lr: 2.5000e-04\n",
      "Epoch 76/300\n",
      "1112/1112 [==============================] - ETA: 0s - loss: 0.1443 - dice_coef: 0.8990\n",
      "Epoch 00076: val_loss did not improve from 0.20444\n",
      "1112/1112 [==============================] - 175s 157ms/step - loss: 0.1443 - dice_coef: 0.8990 - val_loss: 0.2232 - val_dice_coef: 0.8282 - lr: 2.5000e-04\n",
      "Epoch 77/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1112/1112 [==============================] - ETA: 0s - loss: 0.1455 - dice_coef: 0.8985\n",
      "Epoch 00077: val_loss did not improve from 0.20444\n",
      "1112/1112 [==============================] - 174s 157ms/step - loss: 0.1455 - dice_coef: 0.8985 - val_loss: 0.3015 - val_dice_coef: 0.7781 - lr: 2.5000e-04\n",
      "Epoch 78/300\n",
      " 269/1112 [======>.......................] - ETA: 2:05 - loss: 0.1475 - dice_coef: 0.8867"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_113533/1552413383.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;31m# Helper.telegram_bot_sendtext(f'Model {model_name} started training')\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m history = model.fit(train_array,\n\u001b[0m\u001b[1;32m      5\u001b[0m                     \u001b[0mtrain_mask_array\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m                     \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/keras/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     62\u001b[0m     \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 64\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     65\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=broad-except\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1214\u001b[0m                 _r=1):\n\u001b[1;32m   1215\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1216\u001b[0;31m               \u001b[0mtmp_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1217\u001b[0m               \u001b[0;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1218\u001b[0m                 \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/tensorflow/python/util/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m     \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 150\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    151\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    908\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    909\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0mOptionalXlaContext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jit_compile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 910\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    911\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    912\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    940\u001b[0m       \u001b[0;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    941\u001b[0m       \u001b[0;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 942\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=not-callable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    943\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    944\u001b[0m       \u001b[0;31m# Release the lock early so that multiple threads can perform the call\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   3128\u001b[0m       (graph_function,\n\u001b[1;32m   3129\u001b[0m        filtered_flat_args) = self._maybe_define_function(args, kwargs)\n\u001b[0;32m-> 3130\u001b[0;31m     return graph_function._call_flat(\n\u001b[0m\u001b[1;32m   3131\u001b[0m         filtered_flat_args, captured_inputs=graph_function.captured_inputs)  # pylint: disable=protected-access\n\u001b[1;32m   3132\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1957\u001b[0m         and executing_eagerly):\n\u001b[1;32m   1958\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1959\u001b[0;31m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0m\u001b[1;32m   1960\u001b[0m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[1;32m   1961\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    596\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0m_InterpolateFunctionError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    597\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcancellation_manager\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 598\u001b[0;31m           outputs = execute.execute(\n\u001b[0m\u001b[1;32m    599\u001b[0m               \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msignature\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    600\u001b[0m               \u001b[0mnum_outputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_outputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     56\u001b[0m   \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 58\u001b[0;31m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0m\u001b[1;32m     59\u001b[0m                                         inputs, attrs, num_outputs)\n\u001b[1;32m     60\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model_name = 'Model11 (300 Epochs)'\n",
    "# Helper.telegram_bot_sendtext(f'Model {model_name} started training')\n",
    "\n",
    "history = model.fit(train_array,\n",
    "                    train_mask_array,\n",
    "                    batch_size=5,\n",
    "                    epochs=300,\n",
    "                    verbose=1,\n",
    "                    shuffle = True,\n",
    "                    validation_data=(valid_array, valid_mask_array),\n",
    "                    callbacks=my_callbacks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4137a2a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
