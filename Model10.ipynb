{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "96361681",
   "metadata": {},
   "source": [
    "### Model 10 Information\n",
    "* To test out training of cubes where mask label exists\n",
    "* No random number generator used\n",
    "* Training data : 64 x 64 x 96\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a46fc8ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Libraries -------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
    "import os\n",
    "import h5py\n",
    "import keras\n",
    "import loss\n",
    "import Helper\n",
    "import allMetrics\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import UNetModel_3D_simplified\n",
    "import matplotlib.pyplot as plt\n",
    "from keras import backend as K\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau, ModelCheckpoint, CSVLogger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa7f42a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# physical_devices = tf.config.list_physical_devices('GPU')\n",
    "# print(physical_devices)\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6ec342b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_fileName = \"/media/dro/JHSeagate/FYP/jh_fyp_work/3D_UNet/trials/numpyDatasets/numPyArrays/train_Scans/train_DS4.hdf5\"\n",
    "train_maskfileName = \"/media/dro/JHSeagate/FYP/jh_fyp_work/3D_UNet/trials/numpyDatasets/numPyArrays/train_Mask_Scans/train_maskDS4.hdf5\"\n",
    "\n",
    "train_DatasetName = \"trainScans_DataSet4\"\n",
    "train_maskDatasetName = \"trainMaskScans_DataSet4\"\n",
    "\n",
    "valid_fileName = \"/media/dro/JHSeagate/FYP/jh_fyp_work/3D_UNet/trials/numpyDatasets/numPyArrays/valid_Scans/valid_DS4.hdf5\"\n",
    "valid_maskfileName = \"/media/dro/JHSeagate/FYP/jh_fyp_work/3D_UNet/trials/numpyDatasets/numPyArrays/valid_Mask_Scans/valid_maskDS4.hdf5\"\n",
    "\n",
    "valid_DatasetName = \"validScans_DataSet4\"\n",
    "valid_maskDatasetName = \"validMaskScans_DataSet4\"\n",
    "\n",
    "test_fileName = \"/media/dro/JHSeagate/FYP/jh_fyp_work/3D_UNet/trials/numpyDatasets/numPyArrays/test_Scans/test_DS4.hdf5\"\n",
    "test_maskfileName = \"/media/dro/JHSeagate/FYP/jh_fyp_work/3D_UNet/trials/numpyDatasets/numPyArrays/test_Mask_Scans/test_maskDS4.hdf5\"\n",
    "\n",
    "test_DatasetName = \"testScans_DataSet4\"\n",
    "test_maskDatasetName = \"testMaskScans_DataSet4\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5dad4890",
   "metadata": {},
   "outputs": [],
   "source": [
    "with h5py.File(train_fileName, 'r') as hf: # File Dir\n",
    "    train_array = hf[train_DatasetName][:]\n",
    "    \n",
    "with h5py.File(train_maskfileName,'r') as hf:\n",
    "    train_mask_array = hf[train_maskDatasetName][:]\n",
    "    \n",
    "with h5py.File(valid_fileName, 'r') as hf: # File Dir\n",
    "    valid_array = hf[valid_DatasetName][:]\n",
    "    \n",
    "with h5py.File(valid_maskfileName,'r') as hf:\n",
    "    valid_mask_array = hf[valid_maskDatasetName][:]\n",
    "    \n",
    "with h5py.File(test_fileName, 'r') as hf: # File Dir\n",
    "    test_array = hf[test_DatasetName][:]\n",
    "    \n",
    "with h5py.File(test_maskfileName,'r') as hf:\n",
    "    test_mask_array = hf[test_maskDatasetName][:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3dc57191",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(591, 64, 64, 96)\n",
      "(591, 64, 64, 96)\n",
      "--\n",
      "(225, 64, 64, 96)\n",
      "(225, 64, 64, 96)\n",
      "--\n",
      "(296, 64, 64, 96)\n",
      "(296, 64, 64, 96)\n"
     ]
    }
   ],
   "source": [
    "print(train_array.shape)\n",
    "print(train_mask_array.shape)\n",
    "print('--')\n",
    "print(valid_array.shape)\n",
    "print(valid_mask_array.shape)\n",
    "print('--')\n",
    "print(test_array.shape)\n",
    "print(test_mask_array.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5818cc22",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_scan = np.concatenate((train_array,valid_array,test_array))\n",
    "train_mask_scan = np.concatenate((train_mask_array,valid_mask_array,test_mask_array))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "dec5a0c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_array = np.expand_dims(train_scan, axis=4)\n",
    "train_mask_array = np.expand_dims(train_mask_scan, axis=4)\n",
    "\n",
    "valid_array = np.expand_dims(valid_array, axis=4)\n",
    "valid_mask_array = np.expand_dims(valid_mask_array, axis=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a3d9816a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1112, 64, 64, 96, 1)\n",
      "(1112, 64, 64, 96, 1)\n",
      "--\n",
      "(225, 64, 64, 96, 1)\n",
      "(225, 64, 64, 96, 1)\n"
     ]
    }
   ],
   "source": [
    "print(train_array.shape)\n",
    "print(train_mask_array.shape)\n",
    "print('--')\n",
    "print(valid_array.shape)\n",
    "print(valid_mask_array.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17cb0261",
   "metadata": {},
   "source": [
    "### Init Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0aa0b148",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-03-02 16:54:19.046642: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-03-02 16:54:19.530200: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1525] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 38414 MB memory:  -> device: 0, name: A100-SXM4-40GB, pci bus id: 0000:47:00.0, compute capability: 8.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sigmoid\n",
      "Model: \"U-Net\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_1 (InputLayer)           [(None, 64, 64, 96,  0           []                               \n",
      "                                 1)]                                                              \n",
      "                                                                                                  \n",
      " conv3d (Conv3D)                (None, 64, 64, 96,   896         ['input_1[0][0]']                \n",
      "                                32)                                                               \n",
      "                                                                                                  \n",
      " batch_normalization (BatchNorm  (None, 64, 64, 96,   128        ['conv3d[0][0]']                 \n",
      " alization)                     32)                                                               \n",
      "                                                                                                  \n",
      " leaky_re_lu (LeakyReLU)        (None, 64, 64, 96,   0           ['batch_normalization[0][0]']    \n",
      "                                32)                                                               \n",
      "                                                                                                  \n",
      " conv3d_1 (Conv3D)              (None, 64, 64, 96,   27680       ['leaky_re_lu[0][0]']            \n",
      "                                32)                                                               \n",
      "                                                                                                  \n",
      " batch_normalization_1 (BatchNo  (None, 64, 64, 96,   128        ['conv3d_1[0][0]']               \n",
      " rmalization)                   32)                                                               \n",
      "                                                                                                  \n",
      " leaky_re_lu_1 (LeakyReLU)      (None, 64, 64, 96,   0           ['batch_normalization_1[0][0]']  \n",
      "                                32)                                                               \n",
      "                                                                                                  \n",
      " max_pooling3d (MaxPooling3D)   (None, 32, 32, 48,   0           ['leaky_re_lu_1[0][0]']          \n",
      "                                32)                                                               \n",
      "                                                                                                  \n",
      " conv3d_2 (Conv3D)              (None, 32, 32, 48,   55360       ['max_pooling3d[0][0]']          \n",
      "                                64)                                                               \n",
      "                                                                                                  \n",
      " batch_normalization_2 (BatchNo  (None, 32, 32, 48,   256        ['conv3d_2[0][0]']               \n",
      " rmalization)                   64)                                                               \n",
      "                                                                                                  \n",
      " leaky_re_lu_2 (LeakyReLU)      (None, 32, 32, 48,   0           ['batch_normalization_2[0][0]']  \n",
      "                                64)                                                               \n",
      "                                                                                                  \n",
      " conv3d_3 (Conv3D)              (None, 32, 32, 48,   110656      ['leaky_re_lu_2[0][0]']          \n",
      "                                64)                                                               \n",
      "                                                                                                  \n",
      " batch_normalization_3 (BatchNo  (None, 32, 32, 48,   256        ['conv3d_3[0][0]']               \n",
      " rmalization)                   64)                                                               \n",
      "                                                                                                  \n",
      " leaky_re_lu_3 (LeakyReLU)      (None, 32, 32, 48,   0           ['batch_normalization_3[0][0]']  \n",
      "                                64)                                                               \n",
      "                                                                                                  \n",
      " max_pooling3d_1 (MaxPooling3D)  (None, 16, 16, 24,   0          ['leaky_re_lu_3[0][0]']          \n",
      "                                64)                                                               \n",
      "                                                                                                  \n",
      " conv3d_4 (Conv3D)              (None, 16, 16, 24,   221312      ['max_pooling3d_1[0][0]']        \n",
      "                                128)                                                              \n",
      "                                                                                                  \n",
      " batch_normalization_4 (BatchNo  (None, 16, 16, 24,   512        ['conv3d_4[0][0]']               \n",
      " rmalization)                   128)                                                              \n",
      "                                                                                                  \n",
      " leaky_re_lu_4 (LeakyReLU)      (None, 16, 16, 24,   0           ['batch_normalization_4[0][0]']  \n",
      "                                128)                                                              \n",
      "                                                                                                  \n",
      " conv3d_5 (Conv3D)              (None, 16, 16, 24,   442496      ['leaky_re_lu_4[0][0]']          \n",
      "                                128)                                                              \n",
      "                                                                                                  \n",
      " batch_normalization_5 (BatchNo  (None, 16, 16, 24,   512        ['conv3d_5[0][0]']               \n",
      " rmalization)                   128)                                                              \n",
      "                                                                                                  \n",
      " leaky_re_lu_5 (LeakyReLU)      (None, 16, 16, 24,   0           ['batch_normalization_5[0][0]']  \n",
      "                                128)                                                              \n",
      "                                                                                                  \n",
      " max_pooling3d_2 (MaxPooling3D)  (None, 8, 8, 12, 12  0          ['leaky_re_lu_5[0][0]']          \n",
      "                                8)                                                                \n",
      "                                                                                                  \n",
      " conv3d_6 (Conv3D)              (None, 8, 8, 12, 25  884992      ['max_pooling3d_2[0][0]']        \n",
      "                                6)                                                                \n",
      "                                                                                                  \n",
      " batch_normalization_6 (BatchNo  (None, 8, 8, 12, 25  1024       ['conv3d_6[0][0]']               \n",
      " rmalization)                   6)                                                                \n",
      "                                                                                                  \n",
      " leaky_re_lu_6 (LeakyReLU)      (None, 8, 8, 12, 25  0           ['batch_normalization_6[0][0]']  \n",
      "                                6)                                                                \n",
      "                                                                                                  \n",
      " conv3d_7 (Conv3D)              (None, 8, 8, 12, 25  1769728     ['leaky_re_lu_6[0][0]']          \n",
      "                                6)                                                                \n",
      "                                                                                                  \n",
      " batch_normalization_7 (BatchNo  (None, 8, 8, 12, 25  1024       ['conv3d_7[0][0]']               \n",
      " rmalization)                   6)                                                                \n",
      "                                                                                                  \n",
      " leaky_re_lu_7 (LeakyReLU)      (None, 8, 8, 12, 25  0           ['batch_normalization_7[0][0]']  \n",
      "                                6)                                                                \n",
      "                                                                                                  \n",
      " max_pooling3d_3 (MaxPooling3D)  (None, 4, 4, 6, 256  0          ['leaky_re_lu_7[0][0]']          \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " conv3d_8 (Conv3D)              (None, 4, 4, 6, 512  3539456     ['max_pooling3d_3[0][0]']        \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " batch_normalization_8 (BatchNo  (None, 4, 4, 6, 512  2048       ['conv3d_8[0][0]']               \n",
      " rmalization)                   )                                                                 \n",
      "                                                                                                  \n",
      " leaky_re_lu_8 (LeakyReLU)      (None, 4, 4, 6, 512  0           ['batch_normalization_8[0][0]']  \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " conv3d_9 (Conv3D)              (None, 4, 4, 6, 512  7078400     ['leaky_re_lu_8[0][0]']          \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " batch_normalization_9 (BatchNo  (None, 4, 4, 6, 512  2048       ['conv3d_9[0][0]']               \n",
      " rmalization)                   )                                                                 \n",
      "                                                                                                  \n",
      " leaky_re_lu_9 (LeakyReLU)      (None, 4, 4, 6, 512  0           ['batch_normalization_9[0][0]']  \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " conv3d_transpose (Conv3DTransp  (None, 8, 8, 12, 25  1048832    ['leaky_re_lu_9[0][0]']          \n",
      " ose)                           6)                                                                \n",
      "                                                                                                  \n",
      " concatenate (Concatenate)      (None, 8, 8, 12, 51  0           ['conv3d_transpose[0][0]',       \n",
      "                                2)                                'leaky_re_lu_7[0][0]']          \n",
      "                                                                                                  \n",
      " conv3d_10 (Conv3D)             (None, 8, 8, 12, 25  3539200     ['concatenate[0][0]']            \n",
      "                                6)                                                                \n",
      "                                                                                                  \n",
      " batch_normalization_10 (BatchN  (None, 8, 8, 12, 25  1024       ['conv3d_10[0][0]']              \n",
      " ormalization)                  6)                                                                \n",
      "                                                                                                  \n",
      " leaky_re_lu_10 (LeakyReLU)     (None, 8, 8, 12, 25  0           ['batch_normalization_10[0][0]'] \n",
      "                                6)                                                                \n",
      "                                                                                                  \n",
      " conv3d_11 (Conv3D)             (None, 8, 8, 12, 25  1769728     ['leaky_re_lu_10[0][0]']         \n",
      "                                6)                                                                \n",
      "                                                                                                  \n",
      " batch_normalization_11 (BatchN  (None, 8, 8, 12, 25  1024       ['conv3d_11[0][0]']              \n",
      " ormalization)                  6)                                                                \n",
      "                                                                                                  \n",
      " leaky_re_lu_11 (LeakyReLU)     (None, 8, 8, 12, 25  0           ['batch_normalization_11[0][0]'] \n",
      "                                6)                                                                \n",
      "                                                                                                  \n",
      " conv3d_transpose_1 (Conv3DTran  (None, 16, 16, 24,   262272     ['leaky_re_lu_11[0][0]']         \n",
      " spose)                         128)                                                              \n",
      "                                                                                                  \n",
      " concatenate_1 (Concatenate)    (None, 16, 16, 24,   0           ['conv3d_transpose_1[0][0]',     \n",
      "                                256)                              'leaky_re_lu_5[0][0]']          \n",
      "                                                                                                  \n",
      " conv3d_12 (Conv3D)             (None, 16, 16, 24,   884864      ['concatenate_1[0][0]']          \n",
      "                                128)                                                              \n",
      "                                                                                                  \n",
      " batch_normalization_12 (BatchN  (None, 16, 16, 24,   512        ['conv3d_12[0][0]']              \n",
      " ormalization)                  128)                                                              \n",
      "                                                                                                  \n",
      " leaky_re_lu_12 (LeakyReLU)     (None, 16, 16, 24,   0           ['batch_normalization_12[0][0]'] \n",
      "                                128)                                                              \n",
      "                                                                                                  \n",
      " conv3d_13 (Conv3D)             (None, 16, 16, 24,   442496      ['leaky_re_lu_12[0][0]']         \n",
      "                                128)                                                              \n",
      "                                                                                                  \n",
      " batch_normalization_13 (BatchN  (None, 16, 16, 24,   512        ['conv3d_13[0][0]']              \n",
      " ormalization)                  128)                                                              \n",
      "                                                                                                  \n",
      " leaky_re_lu_13 (LeakyReLU)     (None, 16, 16, 24,   0           ['batch_normalization_13[0][0]'] \n",
      "                                128)                                                              \n",
      "                                                                                                  \n",
      " conv3d_transpose_2 (Conv3DTran  (None, 32, 32, 48,   65600      ['leaky_re_lu_13[0][0]']         \n",
      " spose)                         64)                                                               \n",
      "                                                                                                  \n",
      " concatenate_2 (Concatenate)    (None, 32, 32, 48,   0           ['conv3d_transpose_2[0][0]',     \n",
      "                                128)                              'leaky_re_lu_3[0][0]']          \n",
      "                                                                                                  \n",
      " conv3d_14 (Conv3D)             (None, 32, 32, 48,   221248      ['concatenate_2[0][0]']          \n",
      "                                64)                                                               \n",
      "                                                                                                  \n",
      " batch_normalization_14 (BatchN  (None, 32, 32, 48,   256        ['conv3d_14[0][0]']              \n",
      " ormalization)                  64)                                                               \n",
      "                                                                                                  \n",
      " leaky_re_lu_14 (LeakyReLU)     (None, 32, 32, 48,   0           ['batch_normalization_14[0][0]'] \n",
      "                                64)                                                               \n",
      "                                                                                                  \n",
      " conv3d_15 (Conv3D)             (None, 32, 32, 48,   110656      ['leaky_re_lu_14[0][0]']         \n",
      "                                64)                                                               \n",
      "                                                                                                  \n",
      " batch_normalization_15 (BatchN  (None, 32, 32, 48,   256        ['conv3d_15[0][0]']              \n",
      " ormalization)                  64)                                                               \n",
      "                                                                                                  \n",
      " leaky_re_lu_15 (LeakyReLU)     (None, 32, 32, 48,   0           ['batch_normalization_15[0][0]'] \n",
      "                                64)                                                               \n",
      "                                                                                                  \n",
      " conv3d_transpose_3 (Conv3DTran  (None, 64, 64, 96,   16416      ['leaky_re_lu_15[0][0]']         \n",
      " spose)                         32)                                                               \n",
      "                                                                                                  \n",
      " concatenate_3 (Concatenate)    (None, 64, 64, 96,   0           ['conv3d_transpose_3[0][0]',     \n",
      "                                64)                               'leaky_re_lu_1[0][0]']          \n",
      "                                                                                                  \n",
      " conv3d_16 (Conv3D)             (None, 64, 64, 96,   55328       ['concatenate_3[0][0]']          \n",
      "                                32)                                                               \n",
      "                                                                                                  \n",
      " batch_normalization_16 (BatchN  (None, 64, 64, 96,   128        ['conv3d_16[0][0]']              \n",
      " ormalization)                  32)                                                               \n",
      "                                                                                                  \n",
      " leaky_re_lu_16 (LeakyReLU)     (None, 64, 64, 96,   0           ['batch_normalization_16[0][0]'] \n",
      "                                32)                                                               \n",
      "                                                                                                  \n",
      " conv3d_17 (Conv3D)             (None, 64, 64, 96,   27680       ['leaky_re_lu_16[0][0]']         \n",
      "                                32)                                                               \n",
      "                                                                                                  \n",
      " batch_normalization_17 (BatchN  (None, 64, 64, 96,   128        ['conv3d_17[0][0]']              \n",
      " ormalization)                  32)                                                               \n",
      "                                                                                                  \n",
      " leaky_re_lu_17 (LeakyReLU)     (None, 64, 64, 96,   0           ['batch_normalization_17[0][0]'] \n",
      "                                32)                                                               \n",
      "                                                                                                  \n",
      " conv3d_18 (Conv3D)             (None, 64, 64, 96,   865         ['leaky_re_lu_17[0][0]']         \n",
      "                                1)                                                                \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 22,587,937\n",
      "Trainable params: 22,582,049\n",
      "Non-trainable params: 5,888\n",
      "__________________________________________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "LR = 0.001\n",
    "opt = tf.keras.optimizers.Nadam(LR)\n",
    "\n",
    "input_shape = (64,64,96,1)\n",
    "num_class = 1\n",
    "\n",
    "metrics = [allMetrics.dice_coef]\n",
    "\n",
    "model = UNetModel_3D_simplified.build_unet(input_shape, n_classes = num_class)\n",
    "model.compile(optimizer=opt, loss=loss.tversky_crossentropy, metrics=metrics)\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ca1cd772",
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_path = '/media/dro/JHSeagate/FYP/jh_fyp_work/3D_UNet/Final/CSVLogs/Model10_run4.csv'\n",
    "model_checkpoint_path = '/media/dro/JHSeagate/FYP/jh_fyp_work/3D_UNet/Final/SavedModels/Model10_run4.hdf5'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1ffc4418",
   "metadata": {},
   "outputs": [],
   "source": [
    "my_callbacks = [\n",
    "    ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=8, mode = 'auto'),\n",
    "    EarlyStopping(monitor='val_loss', patience=50, restore_best_weights=True, mode = 'auto'),\n",
    "    CSVLogger(csv_path, separator=',', append=True),\n",
    "    ModelCheckpoint(filepath=model_checkpoint_path,\n",
    "                    monitor='val_loss',\n",
    "                    mode='auto',\n",
    "                    verbose=1,\n",
    "                    save_best_only= True)\n",
    "]\n",
    "#     CustomCallBack()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "58fdccd0",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/300\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-03-02 16:55:22.046228: I tensorflow/stream_executor/cuda/cuda_dnn.cc:366] Loaded cuDNN version 8301\n",
      "2023-03-02 16:55:22.908514: W tensorflow/stream_executor/gpu/asm_compiler.cc:111] *** WARNING *** You are using ptxas 11.0.221, which is older than 11.1. ptxas before 11.1 is known to miscompile XLA code, leading to incorrect results or invalid-address errors.\n",
      "\n",
      "You may not need to update to CUDA 11.1; cherry-picking the ptxas binary is often sufficient.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "139/139 [==============================] - ETA: 0s - loss: 0.9850 - dice_coef: 0.2429\n",
      "Epoch 00001: val_loss improved from inf to 9.25300, saving model to /media/dro/JHSeagate/FYP/jh_fyp_work/3D_UNet/Final/SavedModels/Model10_run4.hdf5\n",
      "139/139 [==============================] - 42s 231ms/step - loss: 0.9850 - dice_coef: 0.2429 - val_loss: 9.2530 - val_dice_coef: 7.3362e-05 - lr: 0.0010\n",
      "Epoch 2/300\n",
      "139/139 [==============================] - ETA: 0s - loss: 0.7756 - dice_coef: 0.4657\n",
      "Epoch 00002: val_loss improved from 9.25300 to 1.68251, saving model to /media/dro/JHSeagate/FYP/jh_fyp_work/3D_UNet/Final/SavedModels/Model10_run4.hdf5\n",
      "139/139 [==============================] - 31s 223ms/step - loss: 0.7756 - dice_coef: 0.4657 - val_loss: 1.6825 - val_dice_coef: 7.3046e-05 - lr: 0.0010\n",
      "Epoch 3/300\n",
      "139/139 [==============================] - ETA: 0s - loss: 0.6421 - dice_coef: 0.6028\n",
      "Epoch 00003: val_loss improved from 1.68251 to 1.15902, saving model to /media/dro/JHSeagate/FYP/jh_fyp_work/3D_UNet/Final/SavedModels/Model10_run4.hdf5\n",
      "139/139 [==============================] - 30s 219ms/step - loss: 0.6421 - dice_coef: 0.6028 - val_loss: 1.1590 - val_dice_coef: 0.0424 - lr: 0.0010\n",
      "Epoch 4/300\n",
      "139/139 [==============================] - ETA: 0s - loss: 0.6185 - dice_coef: 0.6331\n",
      "Epoch 00004: val_loss did not improve from 1.15902\n",
      "139/139 [==============================] - 30s 216ms/step - loss: 0.6185 - dice_coef: 0.6331 - val_loss: 1.3643 - val_dice_coef: 5.4597e-04 - lr: 0.0010\n",
      "Epoch 5/300\n",
      "139/139 [==============================] - ETA: 0s - loss: 0.5524 - dice_coef: 0.6964\n",
      "Epoch 00005: val_loss did not improve from 1.15902\n",
      "139/139 [==============================] - 30s 215ms/step - loss: 0.5524 - dice_coef: 0.6964 - val_loss: 1.3988 - val_dice_coef: 5.0043e-04 - lr: 0.0010\n",
      "Epoch 6/300\n",
      "139/139 [==============================] - ETA: 0s - loss: 0.5468 - dice_coef: 0.6958\n",
      "Epoch 00006: val_loss did not improve from 1.15902\n",
      "139/139 [==============================] - 29s 212ms/step - loss: 0.5468 - dice_coef: 0.6958 - val_loss: 5.1160 - val_dice_coef: 0.0848 - lr: 0.0010\n",
      "Epoch 7/300\n",
      "139/139 [==============================] - ETA: 0s - loss: 0.5304 - dice_coef: 0.7082\n",
      "Epoch 00007: val_loss did not improve from 1.15902\n",
      "139/139 [==============================] - 30s 214ms/step - loss: 0.5304 - dice_coef: 0.7082 - val_loss: 3.0971 - val_dice_coef: 0.2059 - lr: 0.0010\n",
      "Epoch 8/300\n",
      "139/139 [==============================] - ETA: 0s - loss: 0.5179 - dice_coef: 0.7193\n",
      "Epoch 00008: val_loss did not improve from 1.15902\n",
      "139/139 [==============================] - 30s 214ms/step - loss: 0.5179 - dice_coef: 0.7193 - val_loss: 5.0555 - val_dice_coef: 0.1683 - lr: 0.0010\n",
      "Epoch 9/300\n",
      "139/139 [==============================] - ETA: 0s - loss: 0.5250 - dice_coef: 0.7096\n",
      "Epoch 00009: val_loss did not improve from 1.15902\n",
      "139/139 [==============================] - 30s 213ms/step - loss: 0.5250 - dice_coef: 0.7096 - val_loss: 3.4327 - val_dice_coef: 0.1167 - lr: 0.0010\n",
      "Epoch 10/300\n",
      "139/139 [==============================] - ETA: 0s - loss: 0.4952 - dice_coef: 0.7436\n",
      "Epoch 00010: val_loss did not improve from 1.15902\n",
      "139/139 [==============================] - 30s 213ms/step - loss: 0.4952 - dice_coef: 0.7436 - val_loss: 2.6987 - val_dice_coef: 0.2248 - lr: 0.0010\n",
      "Epoch 11/300\n",
      "139/139 [==============================] - ETA: 0s - loss: 0.4722 - dice_coef: 0.7581\n",
      "Epoch 00011: val_loss improved from 1.15902 to 1.04179, saving model to /media/dro/JHSeagate/FYP/jh_fyp_work/3D_UNet/Final/SavedModels/Model10_run4.hdf5\n",
      "139/139 [==============================] - 30s 217ms/step - loss: 0.4722 - dice_coef: 0.7581 - val_loss: 1.0418 - val_dice_coef: 0.4015 - lr: 0.0010\n",
      "Epoch 12/300\n",
      "139/139 [==============================] - ETA: 0s - loss: 0.4708 - dice_coef: 0.7696\n",
      "Epoch 00012: val_loss did not improve from 1.04179\n",
      "139/139 [==============================] - 30s 214ms/step - loss: 0.4708 - dice_coef: 0.7696 - val_loss: 1.4534 - val_dice_coef: 0.2885 - lr: 0.0010\n",
      "Epoch 13/300\n",
      "139/139 [==============================] - ETA: 0s - loss: 0.4626 - dice_coef: 0.7659\n",
      "Epoch 00013: val_loss improved from 1.04179 to 0.56442, saving model to /media/dro/JHSeagate/FYP/jh_fyp_work/3D_UNet/Final/SavedModels/Model10_run4.hdf5\n",
      "139/139 [==============================] - 30s 216ms/step - loss: 0.4626 - dice_coef: 0.7659 - val_loss: 0.5644 - val_dice_coef: 0.6577 - lr: 0.0010\n",
      "Epoch 14/300\n",
      "139/139 [==============================] - ETA: 0s - loss: 0.4701 - dice_coef: 0.7595\n",
      "Epoch 00014: val_loss did not improve from 0.56442\n",
      "139/139 [==============================] - 29s 210ms/step - loss: 0.4701 - dice_coef: 0.7595 - val_loss: 0.7132 - val_dice_coef: 0.5675 - lr: 0.0010\n",
      "Epoch 15/300\n",
      "139/139 [==============================] - ETA: 0s - loss: 0.4437 - dice_coef: 0.7849\n",
      "Epoch 00015: val_loss did not improve from 0.56442\n",
      "139/139 [==============================] - 29s 212ms/step - loss: 0.4437 - dice_coef: 0.7849 - val_loss: 1.3157 - val_dice_coef: 0.0022 - lr: 0.0010\n",
      "Epoch 16/300\n",
      "139/139 [==============================] - ETA: 0s - loss: 0.4531 - dice_coef: 0.7764\n",
      "Epoch 00016: val_loss did not improve from 0.56442\n",
      "139/139 [==============================] - 29s 212ms/step - loss: 0.4531 - dice_coef: 0.7764 - val_loss: 1.2656 - val_dice_coef: 0.0035 - lr: 0.0010\n",
      "Epoch 17/300\n",
      "139/139 [==============================] - ETA: 0s - loss: 0.4347 - dice_coef: 0.7858\n",
      "Epoch 00017: val_loss did not improve from 0.56442\n",
      "139/139 [==============================] - 29s 212ms/step - loss: 0.4347 - dice_coef: 0.7858 - val_loss: 0.7349 - val_dice_coef: 0.5069 - lr: 0.0010\n",
      "Epoch 18/300\n",
      "139/139 [==============================] - ETA: 0s - loss: 0.4389 - dice_coef: 0.7854\n",
      "Epoch 00018: val_loss did not improve from 0.56442\n",
      "139/139 [==============================] - 29s 211ms/step - loss: 0.4389 - dice_coef: 0.7854 - val_loss: 1.8576 - val_dice_coef: 0.2981 - lr: 0.0010\n",
      "Epoch 19/300\n",
      "139/139 [==============================] - ETA: 0s - loss: 0.4147 - dice_coef: 0.8044\n",
      "Epoch 00019: val_loss did not improve from 0.56442\n",
      "139/139 [==============================] - 29s 212ms/step - loss: 0.4147 - dice_coef: 0.8044 - val_loss: 0.8158 - val_dice_coef: 0.3031 - lr: 0.0010\n",
      "Epoch 20/300\n",
      "139/139 [==============================] - ETA: 0s - loss: 0.4076 - dice_coef: 0.8067\n",
      "Epoch 00020: val_loss did not improve from 0.56442\n",
      "139/139 [==============================] - 29s 212ms/step - loss: 0.4076 - dice_coef: 0.8067 - val_loss: 1.7747 - val_dice_coef: 0.2621 - lr: 0.0010\n",
      "Epoch 21/300\n",
      "139/139 [==============================] - ETA: 0s - loss: 0.4072 - dice_coef: 0.8063\n",
      "Epoch 00021: val_loss improved from 0.56442 to 0.48980, saving model to /media/dro/JHSeagate/FYP/jh_fyp_work/3D_UNet/Final/SavedModels/Model10_run4.hdf5\n",
      "139/139 [==============================] - 30s 215ms/step - loss: 0.4072 - dice_coef: 0.8063 - val_loss: 0.4898 - val_dice_coef: 0.7194 - lr: 0.0010\n",
      "Epoch 22/300\n",
      "139/139 [==============================] - ETA: 0s - loss: 0.4057 - dice_coef: 0.8065\n",
      "Epoch 00022: val_loss did not improve from 0.48980\n",
      "139/139 [==============================] - 29s 212ms/step - loss: 0.4057 - dice_coef: 0.8065 - val_loss: 0.8188 - val_dice_coef: 0.4933 - lr: 0.0010\n",
      "Epoch 23/300\n",
      "139/139 [==============================] - ETA: 0s - loss: 0.3983 - dice_coef: 0.8107\n",
      "Epoch 00023: val_loss did not improve from 0.48980\n",
      "139/139 [==============================] - 29s 211ms/step - loss: 0.3983 - dice_coef: 0.8107 - val_loss: 0.7887 - val_dice_coef: 0.5437 - lr: 0.0010\n",
      "Epoch 24/300\n",
      "139/139 [==============================] - ETA: 0s - loss: 0.3904 - dice_coef: 0.8115\n",
      "Epoch 00024: val_loss did not improve from 0.48980\n",
      "139/139 [==============================] - 29s 212ms/step - loss: 0.3904 - dice_coef: 0.8115 - val_loss: 0.8017 - val_dice_coef: 0.2960 - lr: 0.0010\n",
      "Epoch 25/300\n",
      "139/139 [==============================] - ETA: 0s - loss: 0.3919 - dice_coef: 0.8162\n",
      "Epoch 00025: val_loss did not improve from 0.48980\n",
      "139/139 [==============================] - 29s 212ms/step - loss: 0.3919 - dice_coef: 0.8162 - val_loss: 0.7365 - val_dice_coef: 0.3349 - lr: 0.0010\n",
      "Epoch 26/300\n",
      "139/139 [==============================] - ETA: 0s - loss: 0.3757 - dice_coef: 0.8260\n",
      "Epoch 00026: val_loss did not improve from 0.48980\n",
      "139/139 [==============================] - 30s 213ms/step - loss: 0.3757 - dice_coef: 0.8260 - val_loss: 1.1664 - val_dice_coef: 0.0429 - lr: 0.0010\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 27/300\n",
      "139/139 [==============================] - ETA: 0s - loss: 0.3719 - dice_coef: 0.8452\n",
      "Epoch 00027: val_loss did not improve from 0.48980\n",
      "139/139 [==============================] - 30s 213ms/step - loss: 0.3719 - dice_coef: 0.8452 - val_loss: 0.5429 - val_dice_coef: 0.6726 - lr: 0.0010\n",
      "Epoch 28/300\n",
      "139/139 [==============================] - ETA: 0s - loss: 0.3688 - dice_coef: 0.8278\n",
      "Epoch 00028: val_loss did not improve from 0.48980\n",
      "139/139 [==============================] - 29s 211ms/step - loss: 0.3688 - dice_coef: 0.8278 - val_loss: 0.8507 - val_dice_coef: 0.5086 - lr: 0.0010\n",
      "Epoch 29/300\n",
      "139/139 [==============================] - ETA: 0s - loss: 0.3725 - dice_coef: 0.8286\n",
      "Epoch 00029: val_loss did not improve from 0.48980\n",
      "139/139 [==============================] - 29s 212ms/step - loss: 0.3725 - dice_coef: 0.8286 - val_loss: 0.9828 - val_dice_coef: 0.4267 - lr: 0.0010\n",
      "Epoch 30/300\n",
      "139/139 [==============================] - ETA: 0s - loss: 0.3475 - dice_coef: 0.8358\n",
      "Epoch 00030: val_loss did not improve from 0.48980\n",
      "139/139 [==============================] - 29s 211ms/step - loss: 0.3475 - dice_coef: 0.8358 - val_loss: 0.6224 - val_dice_coef: 0.6441 - lr: 5.0000e-04\n",
      "Epoch 31/300\n",
      "139/139 [==============================] - ETA: 0s - loss: 0.3285 - dice_coef: 0.8587\n",
      "Epoch 00031: val_loss did not improve from 0.48980\n",
      "139/139 [==============================] - 29s 212ms/step - loss: 0.3285 - dice_coef: 0.8587 - val_loss: 0.6359 - val_dice_coef: 0.4450 - lr: 5.0000e-04\n",
      "Epoch 32/300\n",
      "139/139 [==============================] - ETA: 0s - loss: 0.3333 - dice_coef: 0.8506\n",
      "Epoch 00032: val_loss did not improve from 0.48980\n",
      "139/139 [==============================] - 29s 211ms/step - loss: 0.3333 - dice_coef: 0.8506 - val_loss: 0.9253 - val_dice_coef: 0.4322 - lr: 5.0000e-04\n",
      "Epoch 33/300\n",
      "139/139 [==============================] - ETA: 0s - loss: 0.3312 - dice_coef: 0.8568\n",
      "Epoch 00033: val_loss improved from 0.48980 to 0.47966, saving model to /media/dro/JHSeagate/FYP/jh_fyp_work/3D_UNet/Final/SavedModels/Model10_run4.hdf5\n",
      "139/139 [==============================] - 30s 216ms/step - loss: 0.3312 - dice_coef: 0.8568 - val_loss: 0.4797 - val_dice_coef: 0.6728 - lr: 5.0000e-04\n",
      "Epoch 34/300\n",
      "139/139 [==============================] - ETA: 0s - loss: 0.3142 - dice_coef: 0.8680\n",
      "Epoch 00034: val_loss did not improve from 0.47966\n",
      "139/139 [==============================] - 30s 213ms/step - loss: 0.3142 - dice_coef: 0.8680 - val_loss: 0.7214 - val_dice_coef: 0.3509 - lr: 5.0000e-04\n",
      "Epoch 35/300\n",
      "139/139 [==============================] - ETA: 0s - loss: 0.3082 - dice_coef: 0.8736\n",
      "Epoch 00035: val_loss improved from 0.47966 to 0.44215, saving model to /media/dro/JHSeagate/FYP/jh_fyp_work/3D_UNet/Final/SavedModels/Model10_run4.hdf5\n",
      "139/139 [==============================] - 30s 217ms/step - loss: 0.3082 - dice_coef: 0.8736 - val_loss: 0.4421 - val_dice_coef: 0.7653 - lr: 5.0000e-04\n",
      "Epoch 36/300\n",
      "139/139 [==============================] - ETA: 0s - loss: 0.3200 - dice_coef: 0.8596\n",
      "Epoch 00036: val_loss did not improve from 0.44215\n",
      "139/139 [==============================] - 29s 211ms/step - loss: 0.3200 - dice_coef: 0.8596 - val_loss: 0.7430 - val_dice_coef: 0.3354 - lr: 5.0000e-04\n",
      "Epoch 37/300\n",
      "139/139 [==============================] - ETA: 0s - loss: 0.3062 - dice_coef: 0.8726\n",
      "Epoch 00037: val_loss did not improve from 0.44215\n",
      "139/139 [==============================] - 29s 211ms/step - loss: 0.3062 - dice_coef: 0.8726 - val_loss: 0.4630 - val_dice_coef: 0.7462 - lr: 5.0000e-04\n",
      "Epoch 38/300\n",
      "139/139 [==============================] - ETA: 0s - loss: 0.3066 - dice_coef: 0.8741\n",
      "Epoch 00038: val_loss did not improve from 0.44215\n",
      "139/139 [==============================] - 29s 211ms/step - loss: 0.3066 - dice_coef: 0.8741 - val_loss: 0.9104 - val_dice_coef: 0.4677 - lr: 5.0000e-04\n",
      "Epoch 39/300\n",
      "139/139 [==============================] - ETA: 0s - loss: 0.3055 - dice_coef: 0.8678\n",
      "Epoch 00039: val_loss did not improve from 0.44215\n",
      "139/139 [==============================] - 30s 213ms/step - loss: 0.3055 - dice_coef: 0.8678 - val_loss: 0.6052 - val_dice_coef: 0.6496 - lr: 5.0000e-04\n",
      "Epoch 40/300\n",
      "139/139 [==============================] - ETA: 0s - loss: 0.3020 - dice_coef: 0.8677\n",
      "Epoch 00040: val_loss did not improve from 0.44215\n",
      "139/139 [==============================] - 29s 211ms/step - loss: 0.3020 - dice_coef: 0.8677 - val_loss: 1.1658 - val_dice_coef: 0.0321 - lr: 5.0000e-04\n",
      "Epoch 41/300\n",
      "139/139 [==============================] - ETA: 0s - loss: 0.2903 - dice_coef: 0.8784\n",
      "Epoch 00041: val_loss improved from 0.44215 to 0.37125, saving model to /media/dro/JHSeagate/FYP/jh_fyp_work/3D_UNet/Final/SavedModels/Model10_run4.hdf5\n",
      "139/139 [==============================] - 30s 215ms/step - loss: 0.2903 - dice_coef: 0.8784 - val_loss: 0.3712 - val_dice_coef: 0.7986 - lr: 5.0000e-04\n",
      "Epoch 42/300\n",
      "139/139 [==============================] - ETA: 0s - loss: 0.2903 - dice_coef: 0.8783\n",
      "Epoch 00042: val_loss did not improve from 0.37125\n",
      "139/139 [==============================] - 29s 211ms/step - loss: 0.2903 - dice_coef: 0.8783 - val_loss: 0.4147 - val_dice_coef: 0.7870 - lr: 5.0000e-04\n",
      "Epoch 43/300\n",
      "139/139 [==============================] - ETA: 0s - loss: 0.2897 - dice_coef: 0.8790\n",
      "Epoch 00043: val_loss improved from 0.37125 to 0.35497, saving model to /media/dro/JHSeagate/FYP/jh_fyp_work/3D_UNet/Final/SavedModels/Model10_run4.hdf5\n",
      "139/139 [==============================] - 30s 214ms/step - loss: 0.2897 - dice_coef: 0.8790 - val_loss: 0.3550 - val_dice_coef: 0.8204 - lr: 5.0000e-04\n",
      "Epoch 44/300\n",
      "139/139 [==============================] - ETA: 0s - loss: 0.2865 - dice_coef: 0.8824\n",
      "Epoch 00044: val_loss did not improve from 0.35497\n",
      "139/139 [==============================] - 29s 212ms/step - loss: 0.2865 - dice_coef: 0.8824 - val_loss: 0.4810 - val_dice_coef: 0.7430 - lr: 5.0000e-04\n",
      "Epoch 45/300\n",
      "139/139 [==============================] - ETA: 0s - loss: 0.2847 - dice_coef: 0.8723\n",
      "Epoch 00045: val_loss did not improve from 0.35497\n",
      "139/139 [==============================] - 29s 212ms/step - loss: 0.2847 - dice_coef: 0.8723 - val_loss: 0.7108 - val_dice_coef: 0.5986 - lr: 5.0000e-04\n",
      "Epoch 46/300\n",
      "139/139 [==============================] - ETA: 0s - loss: 0.2827 - dice_coef: 0.8773\n",
      "Epoch 00046: val_loss did not improve from 0.35497\n",
      "139/139 [==============================] - 29s 211ms/step - loss: 0.2827 - dice_coef: 0.8773 - val_loss: 0.6475 - val_dice_coef: 0.6043 - lr: 5.0000e-04\n",
      "Epoch 47/300\n",
      "139/139 [==============================] - ETA: 0s - loss: 0.2768 - dice_coef: 0.8843\n",
      "Epoch 00047: val_loss did not improve from 0.35497\n",
      "139/139 [==============================] - 29s 212ms/step - loss: 0.2768 - dice_coef: 0.8843 - val_loss: 0.4239 - val_dice_coef: 0.7374 - lr: 5.0000e-04\n",
      "Epoch 48/300\n",
      "139/139 [==============================] - ETA: 0s - loss: 0.2777 - dice_coef: 0.8817\n",
      "Epoch 00048: val_loss improved from 0.35497 to 0.33218, saving model to /media/dro/JHSeagate/FYP/jh_fyp_work/3D_UNet/Final/SavedModels/Model10_run4.hdf5\n",
      "139/139 [==============================] - 30s 215ms/step - loss: 0.2777 - dice_coef: 0.8817 - val_loss: 0.3322 - val_dice_coef: 0.8158 - lr: 5.0000e-04\n",
      "Epoch 49/300\n",
      "139/139 [==============================] - ETA: 0s - loss: 0.2697 - dice_coef: 0.8876\n",
      "Epoch 00049: val_loss did not improve from 0.33218\n",
      "139/139 [==============================] - 29s 211ms/step - loss: 0.2697 - dice_coef: 0.8876 - val_loss: 0.6660 - val_dice_coef: 0.3918 - lr: 5.0000e-04\n",
      "Epoch 50/300\n",
      "139/139 [==============================] - ETA: 0s - loss: 0.2704 - dice_coef: 0.8835\n",
      "Epoch 00050: val_loss did not improve from 0.33218\n",
      "139/139 [==============================] - 29s 212ms/step - loss: 0.2704 - dice_coef: 0.8835 - val_loss: 1.1061 - val_dice_coef: 0.0571 - lr: 5.0000e-04\n",
      "Epoch 51/300\n",
      "139/139 [==============================] - ETA: 0s - loss: 0.2648 - dice_coef: 0.8925\n",
      "Epoch 00051: val_loss did not improve from 0.33218\n",
      "139/139 [==============================] - 29s 212ms/step - loss: 0.2648 - dice_coef: 0.8925 - val_loss: 1.3479 - val_dice_coef: 0.0043 - lr: 5.0000e-04\n",
      "Epoch 52/300\n",
      "139/139 [==============================] - ETA: 0s - loss: 0.2632 - dice_coef: 0.8883\n",
      "Epoch 00052: val_loss did not improve from 0.33218\n",
      "139/139 [==============================] - 30s 214ms/step - loss: 0.2632 - dice_coef: 0.8883 - val_loss: 0.5509 - val_dice_coef: 0.7086 - lr: 5.0000e-04\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 53/300\n",
      "139/139 [==============================] - ETA: 0s - loss: 0.2501 - dice_coef: 0.8987\n",
      "Epoch 00053: val_loss did not improve from 0.33218\n",
      "139/139 [==============================] - 29s 211ms/step - loss: 0.2501 - dice_coef: 0.8987 - val_loss: 0.3463 - val_dice_coef: 0.8336 - lr: 5.0000e-04\n",
      "Epoch 54/300\n",
      "139/139 [==============================] - ETA: 0s - loss: 0.2558 - dice_coef: 0.8922\n",
      "Epoch 00054: val_loss did not improve from 0.33218\n",
      "139/139 [==============================] - 29s 212ms/step - loss: 0.2558 - dice_coef: 0.8922 - val_loss: 0.5059 - val_dice_coef: 0.5701 - lr: 5.0000e-04\n",
      "Epoch 55/300\n",
      "139/139 [==============================] - ETA: 0s - loss: 0.2534 - dice_coef: 0.8943\n",
      "Epoch 00055: val_loss did not improve from 0.33218\n",
      "139/139 [==============================] - 29s 211ms/step - loss: 0.2534 - dice_coef: 0.8943 - val_loss: 0.4002 - val_dice_coef: 0.8149 - lr: 5.0000e-04\n",
      "Epoch 56/300\n",
      "139/139 [==============================] - ETA: 0s - loss: 0.2533 - dice_coef: 0.8900\n",
      "Epoch 00056: val_loss improved from 0.33218 to 0.30087, saving model to /media/dro/JHSeagate/FYP/jh_fyp_work/3D_UNet/Final/SavedModels/Model10_run4.hdf5\n",
      "139/139 [==============================] - 30s 215ms/step - loss: 0.2533 - dice_coef: 0.8900 - val_loss: 0.3009 - val_dice_coef: 0.8477 - lr: 5.0000e-04\n",
      "Epoch 57/300\n",
      "139/139 [==============================] - ETA: 0s - loss: 0.2459 - dice_coef: 0.8971\n",
      "Epoch 00057: val_loss did not improve from 0.30087\n",
      "139/139 [==============================] - 29s 211ms/step - loss: 0.2459 - dice_coef: 0.8971 - val_loss: 0.3418 - val_dice_coef: 0.8363 - lr: 5.0000e-04\n",
      "Epoch 58/300\n",
      "139/139 [==============================] - ETA: 0s - loss: 0.2512 - dice_coef: 0.8959\n",
      "Epoch 00058: val_loss did not improve from 0.30087\n",
      "139/139 [==============================] - 29s 212ms/step - loss: 0.2512 - dice_coef: 0.8959 - val_loss: 0.5689 - val_dice_coef: 0.5024 - lr: 5.0000e-04\n",
      "Epoch 59/300\n",
      "139/139 [==============================] - ETA: 0s - loss: 0.2538 - dice_coef: 0.8878\n",
      "Epoch 00059: val_loss did not improve from 0.30087\n",
      "139/139 [==============================] - 29s 211ms/step - loss: 0.2538 - dice_coef: 0.8878 - val_loss: 0.4162 - val_dice_coef: 0.7824 - lr: 5.0000e-04\n",
      "Epoch 60/300\n",
      "139/139 [==============================] - ETA: 0s - loss: 0.2470 - dice_coef: 0.8932\n",
      "Epoch 00060: val_loss did not improve from 0.30087\n",
      "139/139 [==============================] - 29s 212ms/step - loss: 0.2470 - dice_coef: 0.8932 - val_loss: 0.5778 - val_dice_coef: 0.6819 - lr: 5.0000e-04\n",
      "Epoch 61/300\n",
      "139/139 [==============================] - ETA: 0s - loss: 0.2439 - dice_coef: 0.8968\n",
      "Epoch 00061: val_loss did not improve from 0.30087\n",
      "139/139 [==============================] - 29s 212ms/step - loss: 0.2439 - dice_coef: 0.8968 - val_loss: 0.3633 - val_dice_coef: 0.8234 - lr: 5.0000e-04\n",
      "Epoch 62/300\n",
      "139/139 [==============================] - ETA: 0s - loss: 0.2410 - dice_coef: 0.8975\n",
      "Epoch 00062: val_loss did not improve from 0.30087\n",
      "139/139 [==============================] - 29s 211ms/step - loss: 0.2410 - dice_coef: 0.8975 - val_loss: 0.3284 - val_dice_coef: 0.8326 - lr: 5.0000e-04\n",
      "Epoch 63/300\n",
      "139/139 [==============================] - ETA: 0s - loss: 0.2405 - dice_coef: 0.8974\n",
      "Epoch 00063: val_loss did not improve from 0.30087\n",
      "139/139 [==============================] - 29s 211ms/step - loss: 0.2405 - dice_coef: 0.8974 - val_loss: 0.4076 - val_dice_coef: 0.7970 - lr: 5.0000e-04\n",
      "Epoch 64/300\n",
      "139/139 [==============================] - ETA: 0s - loss: 0.2325 - dice_coef: 0.9023\n",
      "Epoch 00064: val_loss did not improve from 0.30087\n",
      "139/139 [==============================] - 29s 211ms/step - loss: 0.2325 - dice_coef: 0.9023 - val_loss: 0.3161 - val_dice_coef: 0.8145 - lr: 5.0000e-04\n",
      "Epoch 65/300\n",
      "139/139 [==============================] - ETA: 0s - loss: 0.2189 - dice_coef: 0.9066\n",
      "Epoch 00065: val_loss improved from 0.30087 to 0.27905, saving model to /media/dro/JHSeagate/FYP/jh_fyp_work/3D_UNet/Final/SavedModels/Model10_run4.hdf5\n",
      "139/139 [==============================] - 30s 215ms/step - loss: 0.2189 - dice_coef: 0.9066 - val_loss: 0.2790 - val_dice_coef: 0.8521 - lr: 2.5000e-04\n",
      "Epoch 66/300\n",
      "139/139 [==============================] - ETA: 0s - loss: 0.2165 - dice_coef: 0.9094\n",
      "Epoch 00066: val_loss did not improve from 0.27905\n",
      "139/139 [==============================] - 30s 213ms/step - loss: 0.2165 - dice_coef: 0.9094 - val_loss: 0.3026 - val_dice_coef: 0.8546 - lr: 2.5000e-04\n",
      "Epoch 67/300\n",
      "139/139 [==============================] - ETA: 0s - loss: 0.2144 - dice_coef: 0.9079\n",
      "Epoch 00067: val_loss did not improve from 0.27905\n",
      "139/139 [==============================] - 29s 212ms/step - loss: 0.2144 - dice_coef: 0.9079 - val_loss: 0.3064 - val_dice_coef: 0.8504 - lr: 2.5000e-04\n",
      "Epoch 68/300\n",
      "139/139 [==============================] - ETA: 0s - loss: 0.2154 - dice_coef: 0.9067\n",
      "Epoch 00068: val_loss improved from 0.27905 to 0.26734, saving model to /media/dro/JHSeagate/FYP/jh_fyp_work/3D_UNet/Final/SavedModels/Model10_run4.hdf5\n",
      "139/139 [==============================] - 30s 216ms/step - loss: 0.2154 - dice_coef: 0.9067 - val_loss: 0.2673 - val_dice_coef: 0.8684 - lr: 2.5000e-04\n",
      "Epoch 69/300\n",
      "139/139 [==============================] - ETA: 0s - loss: 0.2162 - dice_coef: 0.9039\n",
      "Epoch 00069: val_loss did not improve from 0.26734\n",
      "139/139 [==============================] - 29s 211ms/step - loss: 0.2162 - dice_coef: 0.9039 - val_loss: 0.3829 - val_dice_coef: 0.8165 - lr: 2.5000e-04\n",
      "Epoch 70/300\n",
      "139/139 [==============================] - ETA: 0s - loss: 0.2086 - dice_coef: 0.9099\n",
      "Epoch 00070: val_loss did not improve from 0.26734\n",
      "139/139 [==============================] - 29s 212ms/step - loss: 0.2086 - dice_coef: 0.9099 - val_loss: 0.3999 - val_dice_coef: 0.8003 - lr: 2.5000e-04\n",
      "Epoch 71/300\n",
      "139/139 [==============================] - ETA: 0s - loss: 0.2070 - dice_coef: 0.9116\n",
      "Epoch 00071: val_loss did not improve from 0.26734\n",
      "139/139 [==============================] - 29s 212ms/step - loss: 0.2070 - dice_coef: 0.9116 - val_loss: 0.4113 - val_dice_coef: 0.7042 - lr: 2.5000e-04\n",
      "Epoch 72/300\n",
      "139/139 [==============================] - ETA: 0s - loss: 0.2127 - dice_coef: 0.9056\n",
      "Epoch 00072: val_loss did not improve from 0.26734\n",
      "139/139 [==============================] - 29s 212ms/step - loss: 0.2127 - dice_coef: 0.9056 - val_loss: 0.3415 - val_dice_coef: 0.8338 - lr: 2.5000e-04\n",
      "Epoch 73/300\n",
      "139/139 [==============================] - ETA: 0s - loss: 0.2075 - dice_coef: 0.9108\n",
      "Epoch 00073: val_loss did not improve from 0.26734\n",
      "139/139 [==============================] - 29s 211ms/step - loss: 0.2075 - dice_coef: 0.9108 - val_loss: 0.3643 - val_dice_coef: 0.8131 - lr: 2.5000e-04\n",
      "Epoch 74/300\n",
      "139/139 [==============================] - ETA: 0s - loss: 0.2075 - dice_coef: 0.9128\n",
      "Epoch 00074: val_loss did not improve from 0.26734\n",
      "139/139 [==============================] - 29s 211ms/step - loss: 0.2075 - dice_coef: 0.9128 - val_loss: 0.3577 - val_dice_coef: 0.8228 - lr: 2.5000e-04\n",
      "Epoch 75/300\n",
      "139/139 [==============================] - ETA: 0s - loss: 0.2021 - dice_coef: 0.9102\n",
      "Epoch 00075: val_loss did not improve from 0.26734\n",
      "139/139 [==============================] - 29s 211ms/step - loss: 0.2021 - dice_coef: 0.9102 - val_loss: 0.3508 - val_dice_coef: 0.7284 - lr: 2.5000e-04\n",
      "Epoch 76/300\n",
      "139/139 [==============================] - ETA: 0s - loss: 0.2018 - dice_coef: 0.9110\n",
      "Epoch 00076: val_loss did not improve from 0.26734\n",
      "139/139 [==============================] - 29s 211ms/step - loss: 0.2018 - dice_coef: 0.9110 - val_loss: 0.2918 - val_dice_coef: 0.8135 - lr: 2.5000e-04\n",
      "Epoch 77/300\n",
      "139/139 [==============================] - ETA: 0s - loss: 0.1957 - dice_coef: 0.9118\n",
      "Epoch 00077: val_loss did not improve from 0.26734\n",
      "139/139 [==============================] - 29s 212ms/step - loss: 0.1957 - dice_coef: 0.9118 - val_loss: 0.3412 - val_dice_coef: 0.8380 - lr: 1.2500e-04\n",
      "Epoch 78/300\n",
      "139/139 [==============================] - ETA: 0s - loss: 0.1887 - dice_coef: 0.9191\n",
      "Epoch 00078: val_loss improved from 0.26734 to 0.23990, saving model to /media/dro/JHSeagate/FYP/jh_fyp_work/3D_UNet/Final/SavedModels/Model10_run4.hdf5\n",
      "139/139 [==============================] - 30s 215ms/step - loss: 0.1887 - dice_coef: 0.9191 - val_loss: 0.2399 - val_dice_coef: 0.8788 - lr: 1.2500e-04\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 79/300\n",
      "139/139 [==============================] - ETA: 0s - loss: 0.1896 - dice_coef: 0.9185\n",
      "Epoch 00079: val_loss improved from 0.23990 to 0.23488, saving model to /media/dro/JHSeagate/FYP/jh_fyp_work/3D_UNet/Final/SavedModels/Model10_run4.hdf5\n",
      "139/139 [==============================] - 30s 215ms/step - loss: 0.1896 - dice_coef: 0.9185 - val_loss: 0.2349 - val_dice_coef: 0.8782 - lr: 1.2500e-04\n",
      "Epoch 80/300\n",
      "139/139 [==============================] - ETA: 0s - loss: 0.1869 - dice_coef: 0.9181\n",
      "Epoch 00080: val_loss did not improve from 0.23488\n",
      "139/139 [==============================] - 29s 212ms/step - loss: 0.1869 - dice_coef: 0.9181 - val_loss: 0.2428 - val_dice_coef: 0.8809 - lr: 1.2500e-04\n",
      "Epoch 81/300\n",
      "139/139 [==============================] - ETA: 0s - loss: 0.1867 - dice_coef: 0.9159\n",
      "Epoch 00081: val_loss did not improve from 0.23488\n",
      "139/139 [==============================] - 29s 211ms/step - loss: 0.1867 - dice_coef: 0.9159 - val_loss: 0.2507 - val_dice_coef: 0.8812 - lr: 1.2500e-04\n",
      "Epoch 82/300\n",
      "139/139 [==============================] - ETA: 0s - loss: 0.1900 - dice_coef: 0.9155\n",
      "Epoch 00082: val_loss did not improve from 0.23488\n",
      "139/139 [==============================] - 29s 212ms/step - loss: 0.1900 - dice_coef: 0.9155 - val_loss: 0.3139 - val_dice_coef: 0.8538 - lr: 1.2500e-04\n",
      "Epoch 83/300\n",
      "139/139 [==============================] - ETA: 0s - loss: 0.1885 - dice_coef: 0.9156\n",
      "Epoch 00083: val_loss did not improve from 0.23488\n",
      "139/139 [==============================] - 29s 212ms/step - loss: 0.1885 - dice_coef: 0.9156 - val_loss: 0.2740 - val_dice_coef: 0.8705 - lr: 1.2500e-04\n",
      "Epoch 84/300\n",
      "139/139 [==============================] - ETA: 0s - loss: 0.1880 - dice_coef: 0.9126\n",
      "Epoch 00084: val_loss did not improve from 0.23488\n",
      "139/139 [==============================] - 29s 211ms/step - loss: 0.1880 - dice_coef: 0.9126 - val_loss: 0.2594 - val_dice_coef: 0.8803 - lr: 1.2500e-04\n",
      "Epoch 85/300\n",
      "139/139 [==============================] - ETA: 0s - loss: 0.1861 - dice_coef: 0.9165\n",
      "Epoch 00085: val_loss did not improve from 0.23488\n",
      "139/139 [==============================] - 29s 211ms/step - loss: 0.1861 - dice_coef: 0.9165 - val_loss: 0.2479 - val_dice_coef: 0.8824 - lr: 1.2500e-04\n",
      "Epoch 86/300\n",
      "139/139 [==============================] - ETA: 0s - loss: 0.1800 - dice_coef: 0.9191\n",
      "Epoch 00086: val_loss did not improve from 0.23488\n",
      "139/139 [==============================] - 29s 212ms/step - loss: 0.1800 - dice_coef: 0.9191 - val_loss: 0.2459 - val_dice_coef: 0.8814 - lr: 1.2500e-04\n",
      "Epoch 87/300\n",
      "139/139 [==============================] - ETA: 0s - loss: 0.1836 - dice_coef: 0.9206\n",
      "Epoch 00087: val_loss did not improve from 0.23488\n",
      "139/139 [==============================] - 29s 212ms/step - loss: 0.1836 - dice_coef: 0.9206 - val_loss: 0.2410 - val_dice_coef: 0.8816 - lr: 1.2500e-04\n",
      "Epoch 88/300\n",
      "139/139 [==============================] - ETA: 0s - loss: 0.1801 - dice_coef: 0.9209\n",
      "Epoch 00088: val_loss improved from 0.23488 to 0.23079, saving model to /media/dro/JHSeagate/FYP/jh_fyp_work/3D_UNet/Final/SavedModels/Model10_run4.hdf5\n",
      "139/139 [==============================] - 30s 216ms/step - loss: 0.1801 - dice_coef: 0.9209 - val_loss: 0.2308 - val_dice_coef: 0.8883 - lr: 6.2500e-05\n",
      "Epoch 89/300\n",
      "139/139 [==============================] - ETA: 0s - loss: 0.1755 - dice_coef: 0.9209\n",
      "Epoch 00089: val_loss did not improve from 0.23079\n",
      "139/139 [==============================] - 29s 212ms/step - loss: 0.1755 - dice_coef: 0.9209 - val_loss: 0.2478 - val_dice_coef: 0.8840 - lr: 6.2500e-05\n",
      "Epoch 90/300\n",
      "139/139 [==============================] - ETA: 0s - loss: 0.1761 - dice_coef: 0.9219\n",
      "Epoch 00090: val_loss improved from 0.23079 to 0.22052, saving model to /media/dro/JHSeagate/FYP/jh_fyp_work/3D_UNet/Final/SavedModels/Model10_run4.hdf5\n",
      "139/139 [==============================] - 30s 215ms/step - loss: 0.1761 - dice_coef: 0.9219 - val_loss: 0.2205 - val_dice_coef: 0.8841 - lr: 6.2500e-05\n",
      "Epoch 91/300\n",
      "139/139 [==============================] - ETA: 0s - loss: 0.1733 - dice_coef: 0.9227\n",
      "Epoch 00091: val_loss did not improve from 0.22052\n",
      "139/139 [==============================] - 29s 211ms/step - loss: 0.1733 - dice_coef: 0.9227 - val_loss: 0.2416 - val_dice_coef: 0.8841 - lr: 6.2500e-05\n",
      "Epoch 92/300\n",
      "139/139 [==============================] - ETA: 0s - loss: 0.1744 - dice_coef: 0.9208\n",
      "Epoch 00092: val_loss did not improve from 0.22052\n",
      "139/139 [==============================] - 29s 211ms/step - loss: 0.1744 - dice_coef: 0.9208 - val_loss: 0.2335 - val_dice_coef: 0.8892 - lr: 6.2500e-05\n",
      "Epoch 93/300\n",
      "139/139 [==============================] - ETA: 0s - loss: 0.1744 - dice_coef: 0.9219\n",
      "Epoch 00093: val_loss did not improve from 0.22052\n",
      "139/139 [==============================] - 29s 212ms/step - loss: 0.1744 - dice_coef: 0.9219 - val_loss: 0.2269 - val_dice_coef: 0.8734 - lr: 6.2500e-05\n",
      "Epoch 94/300\n",
      "139/139 [==============================] - ETA: 0s - loss: 0.1714 - dice_coef: 0.9212\n",
      "Epoch 00094: val_loss did not improve from 0.22052\n",
      "139/139 [==============================] - 30s 212ms/step - loss: 0.1714 - dice_coef: 0.9212 - val_loss: 0.2785 - val_dice_coef: 0.8716 - lr: 6.2500e-05\n",
      "Epoch 95/300\n",
      "139/139 [==============================] - ETA: 0s - loss: 0.1709 - dice_coef: 0.9231\n",
      "Epoch 00095: val_loss did not improve from 0.22052\n",
      "139/139 [==============================] - 29s 211ms/step - loss: 0.1709 - dice_coef: 0.9231 - val_loss: 0.2314 - val_dice_coef: 0.8890 - lr: 6.2500e-05\n",
      "Epoch 96/300\n",
      "139/139 [==============================] - ETA: 0s - loss: 0.1715 - dice_coef: 0.9206\n",
      "Epoch 00096: val_loss did not improve from 0.22052\n",
      "139/139 [==============================] - 29s 211ms/step - loss: 0.1715 - dice_coef: 0.9206 - val_loss: 0.2464 - val_dice_coef: 0.8846 - lr: 6.2500e-05\n",
      "Epoch 97/300\n",
      "139/139 [==============================] - ETA: 0s - loss: 0.1721 - dice_coef: 0.9206\n",
      "Epoch 00097: val_loss did not improve from 0.22052\n",
      "139/139 [==============================] - 30s 212ms/step - loss: 0.1721 - dice_coef: 0.9206 - val_loss: 0.2331 - val_dice_coef: 0.8898 - lr: 6.2500e-05\n",
      "Epoch 98/300\n",
      "139/139 [==============================] - ETA: 0s - loss: 0.1737 - dice_coef: 0.9232\n",
      "Epoch 00098: val_loss did not improve from 0.22052\n",
      "139/139 [==============================] - 29s 212ms/step - loss: 0.1737 - dice_coef: 0.9232 - val_loss: 0.2254 - val_dice_coef: 0.8814 - lr: 6.2500e-05\n",
      "Epoch 99/300\n",
      "139/139 [==============================] - ETA: 0s - loss: 0.1729 - dice_coef: 0.9218\n",
      "Epoch 00099: val_loss improved from 0.22052 to 0.21879, saving model to /media/dro/JHSeagate/FYP/jh_fyp_work/3D_UNet/Final/SavedModels/Model10_run4.hdf5\n",
      "139/139 [==============================] - 30s 215ms/step - loss: 0.1729 - dice_coef: 0.9218 - val_loss: 0.2188 - val_dice_coef: 0.8884 - lr: 3.1250e-05\n",
      "Epoch 100/300\n",
      "139/139 [==============================] - ETA: 0s - loss: 0.1675 - dice_coef: 0.9200\n",
      "Epoch 00100: val_loss did not improve from 0.21879\n",
      "139/139 [==============================] - 29s 211ms/step - loss: 0.1675 - dice_coef: 0.9200 - val_loss: 0.2236 - val_dice_coef: 0.8901 - lr: 3.1250e-05\n",
      "Epoch 101/300\n",
      "139/139 [==============================] - ETA: 0s - loss: 0.1674 - dice_coef: 0.9225\n",
      "Epoch 00101: val_loss did not improve from 0.21879\n",
      "139/139 [==============================] - 29s 212ms/step - loss: 0.1674 - dice_coef: 0.9225 - val_loss: 0.2324 - val_dice_coef: 0.8885 - lr: 3.1250e-05\n",
      "Epoch 102/300\n",
      "139/139 [==============================] - ETA: 0s - loss: 0.1661 - dice_coef: 0.9224\n",
      "Epoch 00102: val_loss did not improve from 0.21879\n",
      "139/139 [==============================] - 29s 211ms/step - loss: 0.1661 - dice_coef: 0.9224 - val_loss: 0.2306 - val_dice_coef: 0.8886 - lr: 3.1250e-05\n",
      "Epoch 103/300\n",
      "139/139 [==============================] - ETA: 0s - loss: 0.1682 - dice_coef: 0.9228\n",
      "Epoch 00103: val_loss improved from 0.21879 to 0.21078, saving model to /media/dro/JHSeagate/FYP/jh_fyp_work/3D_UNet/Final/SavedModels/Model10_run4.hdf5\n",
      "139/139 [==============================] - 30s 214ms/step - loss: 0.1682 - dice_coef: 0.9228 - val_loss: 0.2108 - val_dice_coef: 0.8827 - lr: 3.1250e-05\n",
      "Epoch 104/300\n",
      "139/139 [==============================] - ETA: 0s - loss: 0.1661 - dice_coef: 0.9215\n",
      "Epoch 00104: val_loss did not improve from 0.21078\n",
      "139/139 [==============================] - 29s 212ms/step - loss: 0.1661 - dice_coef: 0.9215 - val_loss: 0.2110 - val_dice_coef: 0.8817 - lr: 3.1250e-05\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 105/300\n",
      "139/139 [==============================] - ETA: 0s - loss: 0.1637 - dice_coef: 0.9244\n",
      "Epoch 00105: val_loss did not improve from 0.21078\n",
      "139/139 [==============================] - 29s 211ms/step - loss: 0.1637 - dice_coef: 0.9244 - val_loss: 0.2112 - val_dice_coef: 0.8892 - lr: 3.1250e-05\n",
      "Epoch 106/300\n",
      "139/139 [==============================] - ETA: 0s - loss: 0.1663 - dice_coef: 0.9235\n",
      "Epoch 00106: val_loss did not improve from 0.21078\n",
      "139/139 [==============================] - 29s 211ms/step - loss: 0.1663 - dice_coef: 0.9235 - val_loss: 0.2237 - val_dice_coef: 0.8898 - lr: 3.1250e-05\n",
      "Epoch 107/300\n",
      "139/139 [==============================] - ETA: 0s - loss: 0.1657 - dice_coef: 0.9220\n",
      "Epoch 00107: val_loss did not improve from 0.21078\n",
      "139/139 [==============================] - 29s 212ms/step - loss: 0.1657 - dice_coef: 0.9220 - val_loss: 0.2161 - val_dice_coef: 0.8913 - lr: 3.1250e-05\n",
      "Epoch 108/300\n",
      "139/139 [==============================] - ETA: 0s - loss: 0.1643 - dice_coef: 0.9247\n",
      "Epoch 00108: val_loss improved from 0.21078 to 0.21043, saving model to /media/dro/JHSeagate/FYP/jh_fyp_work/3D_UNet/Final/SavedModels/Model10_run4.hdf5\n",
      "139/139 [==============================] - 30s 215ms/step - loss: 0.1643 - dice_coef: 0.9247 - val_loss: 0.2104 - val_dice_coef: 0.8898 - lr: 3.1250e-05\n",
      "Epoch 109/300\n",
      "139/139 [==============================] - ETA: 0s - loss: 0.1659 - dice_coef: 0.9230\n",
      "Epoch 00109: val_loss improved from 0.21043 to 0.20863, saving model to /media/dro/JHSeagate/FYP/jh_fyp_work/3D_UNet/Final/SavedModels/Model10_run4.hdf5\n",
      "139/139 [==============================] - 30s 215ms/step - loss: 0.1659 - dice_coef: 0.9230 - val_loss: 0.2086 - val_dice_coef: 0.8772 - lr: 3.1250e-05\n",
      "Epoch 110/300\n",
      "139/139 [==============================] - ETA: 0s - loss: 0.1639 - dice_coef: 0.9248\n",
      "Epoch 00110: val_loss did not improve from 0.20863\n",
      "139/139 [==============================] - 29s 211ms/step - loss: 0.1639 - dice_coef: 0.9248 - val_loss: 0.2147 - val_dice_coef: 0.8922 - lr: 3.1250e-05\n",
      "Epoch 111/300\n",
      "139/139 [==============================] - ETA: 0s - loss: 0.1643 - dice_coef: 0.9230\n",
      "Epoch 00111: val_loss did not improve from 0.20863\n",
      "139/139 [==============================] - 29s 211ms/step - loss: 0.1643 - dice_coef: 0.9230 - val_loss: 0.2129 - val_dice_coef: 0.8810 - lr: 3.1250e-05\n",
      "Epoch 112/300\n",
      "139/139 [==============================] - ETA: 0s - loss: 0.1670 - dice_coef: 0.9233\n",
      "Epoch 00112: val_loss did not improve from 0.20863\n",
      "139/139 [==============================] - 29s 211ms/step - loss: 0.1670 - dice_coef: 0.9233 - val_loss: 0.2385 - val_dice_coef: 0.8862 - lr: 3.1250e-05\n",
      "Epoch 113/300\n",
      "139/139 [==============================] - ETA: 0s - loss: 0.1633 - dice_coef: 0.9244\n",
      "Epoch 00113: val_loss did not improve from 0.20863\n",
      "139/139 [==============================] - 29s 212ms/step - loss: 0.1633 - dice_coef: 0.9244 - val_loss: 0.2131 - val_dice_coef: 0.8714 - lr: 3.1250e-05\n",
      "Epoch 114/300\n",
      "139/139 [==============================] - ETA: 0s - loss: 0.1622 - dice_coef: 0.9226\n",
      "Epoch 00114: val_loss did not improve from 0.20863\n",
      "139/139 [==============================] - 29s 210ms/step - loss: 0.1622 - dice_coef: 0.9226 - val_loss: 0.2189 - val_dice_coef: 0.8926 - lr: 3.1250e-05\n",
      "Epoch 115/300\n",
      "139/139 [==============================] - ETA: 0s - loss: 0.1640 - dice_coef: 0.9250\n",
      "Epoch 00115: val_loss did not improve from 0.20863\n",
      "139/139 [==============================] - 29s 210ms/step - loss: 0.1640 - dice_coef: 0.9250 - val_loss: 0.2126 - val_dice_coef: 0.8912 - lr: 3.1250e-05\n",
      "Epoch 116/300\n",
      "139/139 [==============================] - ETA: 0s - loss: 0.1624 - dice_coef: 0.9225\n",
      "Epoch 00116: val_loss did not improve from 0.20863\n",
      "139/139 [==============================] - 29s 211ms/step - loss: 0.1624 - dice_coef: 0.9225 - val_loss: 0.2167 - val_dice_coef: 0.8915 - lr: 3.1250e-05\n",
      "Epoch 117/300\n",
      "139/139 [==============================] - ETA: 0s - loss: 0.1635 - dice_coef: 0.9267\n",
      "Epoch 00117: val_loss did not improve from 0.20863\n",
      "139/139 [==============================] - 29s 210ms/step - loss: 0.1635 - dice_coef: 0.9267 - val_loss: 0.2118 - val_dice_coef: 0.8930 - lr: 3.1250e-05\n",
      "Epoch 118/300\n",
      "139/139 [==============================] - ETA: 0s - loss: 0.1626 - dice_coef: 0.9234\n",
      "Epoch 00118: val_loss improved from 0.20863 to 0.20664, saving model to /media/dro/JHSeagate/FYP/jh_fyp_work/3D_UNet/Final/SavedModels/Model10_run4.hdf5\n",
      "139/139 [==============================] - 30s 214ms/step - loss: 0.1626 - dice_coef: 0.9234 - val_loss: 0.2066 - val_dice_coef: 0.8823 - lr: 1.5625e-05\n",
      "Epoch 119/300\n",
      "139/139 [==============================] - ETA: 0s - loss: 0.1604 - dice_coef: 0.9240\n",
      "Epoch 00119: val_loss did not improve from 0.20664\n",
      "139/139 [==============================] - 29s 211ms/step - loss: 0.1604 - dice_coef: 0.9240 - val_loss: 0.2147 - val_dice_coef: 0.8937 - lr: 1.5625e-05\n",
      "Epoch 120/300\n",
      "139/139 [==============================] - ETA: 0s - loss: 0.1623 - dice_coef: 0.9230\n",
      "Epoch 00120: val_loss improved from 0.20664 to 0.20512, saving model to /media/dro/JHSeagate/FYP/jh_fyp_work/3D_UNet/Final/SavedModels/Model10_run4.hdf5\n",
      "139/139 [==============================] - 30s 215ms/step - loss: 0.1623 - dice_coef: 0.9230 - val_loss: 0.2051 - val_dice_coef: 0.8881 - lr: 1.5625e-05\n",
      "Epoch 121/300\n",
      "139/139 [==============================] - ETA: 0s - loss: 0.1614 - dice_coef: 0.9232\n",
      "Epoch 00121: val_loss did not improve from 0.20512\n",
      "139/139 [==============================] - 29s 211ms/step - loss: 0.1614 - dice_coef: 0.9232 - val_loss: 0.2244 - val_dice_coef: 0.8918 - lr: 1.5625e-05\n",
      "Epoch 122/300\n",
      "139/139 [==============================] - ETA: 0s - loss: 0.1604 - dice_coef: 0.9242\n",
      "Epoch 00122: val_loss did not improve from 0.20512\n",
      "139/139 [==============================] - 29s 211ms/step - loss: 0.1604 - dice_coef: 0.9242 - val_loss: 0.2200 - val_dice_coef: 0.8932 - lr: 1.5625e-05\n",
      "Epoch 123/300\n",
      "139/139 [==============================] - ETA: 0s - loss: 0.1609 - dice_coef: 0.9243\n",
      "Epoch 00123: val_loss did not improve from 0.20512\n",
      "139/139 [==============================] - 29s 210ms/step - loss: 0.1609 - dice_coef: 0.9243 - val_loss: 0.2079 - val_dice_coef: 0.8940 - lr: 1.5625e-05\n",
      "Epoch 124/300\n",
      "139/139 [==============================] - ETA: 0s - loss: 0.1606 - dice_coef: 0.9264\n",
      "Epoch 00124: val_loss improved from 0.20512 to 0.20436, saving model to /media/dro/JHSeagate/FYP/jh_fyp_work/3D_UNet/Final/SavedModels/Model10_run4.hdf5\n",
      "139/139 [==============================] - 30s 214ms/step - loss: 0.1606 - dice_coef: 0.9264 - val_loss: 0.2044 - val_dice_coef: 0.8928 - lr: 1.5625e-05\n",
      "Epoch 125/300\n",
      "139/139 [==============================] - ETA: 0s - loss: 0.1590 - dice_coef: 0.9224\n",
      "Epoch 00125: val_loss did not improve from 0.20436\n",
      "139/139 [==============================] - 29s 211ms/step - loss: 0.1590 - dice_coef: 0.9224 - val_loss: 0.2051 - val_dice_coef: 0.8932 - lr: 1.5625e-05\n",
      "Epoch 126/300\n",
      "139/139 [==============================] - ETA: 0s - loss: 0.1596 - dice_coef: 0.9253\n",
      "Epoch 00126: val_loss did not improve from 0.20436\n",
      "139/139 [==============================] - 29s 210ms/step - loss: 0.1596 - dice_coef: 0.9253 - val_loss: 0.2182 - val_dice_coef: 0.8940 - lr: 1.5625e-05\n",
      "Epoch 127/300\n",
      "139/139 [==============================] - ETA: 0s - loss: 0.1599 - dice_coef: 0.9260\n",
      "Epoch 00127: val_loss improved from 0.20436 to 0.20417, saving model to /media/dro/JHSeagate/FYP/jh_fyp_work/3D_UNet/Final/SavedModels/Model10_run4.hdf5\n",
      "139/139 [==============================] - 30s 215ms/step - loss: 0.1599 - dice_coef: 0.9260 - val_loss: 0.2042 - val_dice_coef: 0.8907 - lr: 1.5625e-05\n",
      "Epoch 128/300\n",
      "139/139 [==============================] - ETA: 0s - loss: 0.1583 - dice_coef: 0.9261\n",
      "Epoch 00128: val_loss did not improve from 0.20417\n",
      "139/139 [==============================] - 29s 210ms/step - loss: 0.1583 - dice_coef: 0.9261 - val_loss: 0.2122 - val_dice_coef: 0.8938 - lr: 1.5625e-05\n",
      "Epoch 129/300\n",
      "139/139 [==============================] - ETA: 0s - loss: 0.1582 - dice_coef: 0.9241\n",
      "Epoch 00129: val_loss improved from 0.20417 to 0.20224, saving model to /media/dro/JHSeagate/FYP/jh_fyp_work/3D_UNet/Final/SavedModels/Model10_run4.hdf5\n",
      "139/139 [==============================] - 30s 214ms/step - loss: 0.1582 - dice_coef: 0.9241 - val_loss: 0.2022 - val_dice_coef: 0.8924 - lr: 1.5625e-05\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 130/300\n",
      "139/139 [==============================] - ETA: 0s - loss: 0.1584 - dice_coef: 0.9254\n",
      "Epoch 00130: val_loss did not improve from 0.20224\n",
      "139/139 [==============================] - 29s 210ms/step - loss: 0.1584 - dice_coef: 0.9254 - val_loss: 0.2069 - val_dice_coef: 0.8945 - lr: 1.5625e-05\n",
      "Epoch 131/300\n",
      "139/139 [==============================] - ETA: 0s - loss: 0.1585 - dice_coef: 0.9254\n",
      "Epoch 00131: val_loss did not improve from 0.20224\n",
      "139/139 [==============================] - 29s 210ms/step - loss: 0.1585 - dice_coef: 0.9254 - val_loss: 0.2100 - val_dice_coef: 0.8951 - lr: 1.5625e-05\n",
      "Epoch 132/300\n",
      "139/139 [==============================] - ETA: 0s - loss: 0.1579 - dice_coef: 0.9240\n",
      "Epoch 00132: val_loss did not improve from 0.20224\n",
      "139/139 [==============================] - 29s 210ms/step - loss: 0.1579 - dice_coef: 0.9240 - val_loss: 0.2083 - val_dice_coef: 0.8955 - lr: 1.5625e-05\n",
      "Epoch 133/300\n",
      "139/139 [==============================] - ETA: 0s - loss: 0.1592 - dice_coef: 0.9243\n",
      "Epoch 00133: val_loss did not improve from 0.20224\n",
      "139/139 [==============================] - 29s 210ms/step - loss: 0.1592 - dice_coef: 0.9243 - val_loss: 0.2117 - val_dice_coef: 0.8951 - lr: 1.5625e-05\n",
      "Epoch 134/300\n",
      "139/139 [==============================] - ETA: 0s - loss: 0.1583 - dice_coef: 0.9251\n",
      "Epoch 00134: val_loss did not improve from 0.20224\n",
      "139/139 [==============================] - 29s 210ms/step - loss: 0.1583 - dice_coef: 0.9251 - val_loss: 0.2027 - val_dice_coef: 0.8923 - lr: 1.5625e-05\n",
      "Epoch 135/300\n",
      "139/139 [==============================] - ETA: 0s - loss: 0.1591 - dice_coef: 0.9219\n",
      "Epoch 00135: val_loss did not improve from 0.20224\n",
      "139/139 [==============================] - 29s 210ms/step - loss: 0.1591 - dice_coef: 0.9219 - val_loss: 0.2125 - val_dice_coef: 0.8950 - lr: 1.5625e-05\n",
      "Epoch 136/300\n",
      "139/139 [==============================] - ETA: 0s - loss: 0.1590 - dice_coef: 0.9256\n",
      "Epoch 00136: val_loss did not improve from 0.20224\n",
      "139/139 [==============================] - 29s 210ms/step - loss: 0.1590 - dice_coef: 0.9256 - val_loss: 0.2072 - val_dice_coef: 0.8948 - lr: 1.5625e-05\n",
      "Epoch 137/300\n",
      "139/139 [==============================] - ETA: 0s - loss: 0.1579 - dice_coef: 0.9244\n",
      "Epoch 00137: val_loss did not improve from 0.20224\n",
      "139/139 [==============================] - 29s 210ms/step - loss: 0.1579 - dice_coef: 0.9244 - val_loss: 0.2105 - val_dice_coef: 0.8946 - lr: 1.5625e-05\n",
      "Epoch 138/300\n",
      "139/139 [==============================] - ETA: 0s - loss: 0.1587 - dice_coef: 0.9245\n",
      "Epoch 00138: val_loss improved from 0.20224 to 0.20221, saving model to /media/dro/JHSeagate/FYP/jh_fyp_work/3D_UNet/Final/SavedModels/Model10_run4.hdf5\n",
      "139/139 [==============================] - 30s 214ms/step - loss: 0.1587 - dice_coef: 0.9245 - val_loss: 0.2022 - val_dice_coef: 0.8944 - lr: 7.8125e-06\n",
      "Epoch 139/300\n",
      "139/139 [==============================] - ETA: 0s - loss: 0.1580 - dice_coef: 0.9225\n",
      "Epoch 00139: val_loss did not improve from 0.20221\n",
      "139/139 [==============================] - 29s 210ms/step - loss: 0.1580 - dice_coef: 0.9225 - val_loss: 0.2038 - val_dice_coef: 0.8959 - lr: 7.8125e-06\n",
      "Epoch 140/300\n",
      "139/139 [==============================] - ETA: 0s - loss: 0.1570 - dice_coef: 0.9258\n",
      "Epoch 00140: val_loss improved from 0.20221 to 0.20018, saving model to /media/dro/JHSeagate/FYP/jh_fyp_work/3D_UNet/Final/SavedModels/Model10_run4.hdf5\n",
      "139/139 [==============================] - 30s 214ms/step - loss: 0.1570 - dice_coef: 0.9258 - val_loss: 0.2002 - val_dice_coef: 0.8942 - lr: 7.8125e-06\n",
      "Epoch 141/300\n",
      "139/139 [==============================] - ETA: 0s - loss: 0.1579 - dice_coef: 0.9233\n",
      "Epoch 00141: val_loss did not improve from 0.20018\n",
      "139/139 [==============================] - 29s 211ms/step - loss: 0.1579 - dice_coef: 0.9233 - val_loss: 0.2002 - val_dice_coef: 0.8943 - lr: 7.8125e-06\n",
      "Epoch 142/300\n",
      "139/139 [==============================] - ETA: 0s - loss: 0.1576 - dice_coef: 0.9252\n",
      "Epoch 00142: val_loss did not improve from 0.20018\n",
      "139/139 [==============================] - 29s 210ms/step - loss: 0.1576 - dice_coef: 0.9252 - val_loss: 0.2002 - val_dice_coef: 0.8948 - lr: 7.8125e-06\n",
      "Epoch 143/300\n",
      "139/139 [==============================] - ETA: 0s - loss: 0.1589 - dice_coef: 0.9210\n",
      "Epoch 00143: val_loss did not improve from 0.20018\n",
      "139/139 [==============================] - 29s 210ms/step - loss: 0.1589 - dice_coef: 0.9210 - val_loss: 0.2046 - val_dice_coef: 0.8955 - lr: 7.8125e-06\n",
      "Epoch 144/300\n",
      "139/139 [==============================] - ETA: 0s - loss: 0.1572 - dice_coef: 0.9234\n",
      "Epoch 00144: val_loss did not improve from 0.20018\n",
      "139/139 [==============================] - 29s 210ms/step - loss: 0.1572 - dice_coef: 0.9234 - val_loss: 0.2144 - val_dice_coef: 0.8947 - lr: 7.8125e-06\n",
      "Epoch 145/300\n",
      "139/139 [==============================] - ETA: 0s - loss: 0.1579 - dice_coef: 0.9252\n",
      "Epoch 00145: val_loss did not improve from 0.20018\n",
      "139/139 [==============================] - 29s 210ms/step - loss: 0.1579 - dice_coef: 0.9252 - val_loss: 0.2036 - val_dice_coef: 0.8951 - lr: 7.8125e-06\n",
      "Epoch 146/300\n",
      "139/139 [==============================] - ETA: 0s - loss: 0.1577 - dice_coef: 0.9251\n",
      "Epoch 00146: val_loss did not improve from 0.20018\n",
      "139/139 [==============================] - 29s 210ms/step - loss: 0.1577 - dice_coef: 0.9251 - val_loss: 0.2031 - val_dice_coef: 0.8948 - lr: 7.8125e-06\n",
      "Epoch 147/300\n",
      "139/139 [==============================] - ETA: 0s - loss: 0.1544 - dice_coef: 0.9269\n",
      "Epoch 00147: val_loss did not improve from 0.20018\n",
      "139/139 [==============================] - 29s 211ms/step - loss: 0.1544 - dice_coef: 0.9269 - val_loss: 0.2005 - val_dice_coef: 0.8937 - lr: 7.8125e-06\n",
      "Epoch 148/300\n",
      "139/139 [==============================] - ETA: 0s - loss: 0.1566 - dice_coef: 0.9231\n",
      "Epoch 00148: val_loss did not improve from 0.20018\n",
      "139/139 [==============================] - 29s 210ms/step - loss: 0.1566 - dice_coef: 0.9231 - val_loss: 0.2059 - val_dice_coef: 0.8958 - lr: 7.8125e-06\n",
      "Epoch 149/300\n",
      "139/139 [==============================] - ETA: 0s - loss: 0.1577 - dice_coef: 0.9249\n",
      "Epoch 00149: val_loss did not improve from 0.20018\n",
      "139/139 [==============================] - 29s 210ms/step - loss: 0.1577 - dice_coef: 0.9249 - val_loss: 0.2030 - val_dice_coef: 0.8956 - lr: 3.9063e-06\n",
      "Epoch 150/300\n",
      "139/139 [==============================] - ETA: 0s - loss: 0.1568 - dice_coef: 0.9260\n",
      "Epoch 00150: val_loss did not improve from 0.20018\n",
      "139/139 [==============================] - 29s 211ms/step - loss: 0.1568 - dice_coef: 0.9260 - val_loss: 0.2033 - val_dice_coef: 0.8958 - lr: 3.9063e-06\n",
      "Epoch 151/300\n",
      "139/139 [==============================] - ETA: 0s - loss: 0.1566 - dice_coef: 0.9250\n",
      "Epoch 00151: val_loss did not improve from 0.20018\n",
      "139/139 [==============================] - 29s 212ms/step - loss: 0.1566 - dice_coef: 0.9250 - val_loss: 0.2024 - val_dice_coef: 0.8957 - lr: 3.9063e-06\n",
      "Epoch 152/300\n",
      "139/139 [==============================] - ETA: 0s - loss: 0.1540 - dice_coef: 0.9275\n",
      "Epoch 00152: val_loss improved from 0.20018 to 0.19843, saving model to /media/dro/JHSeagate/FYP/jh_fyp_work/3D_UNet/Final/SavedModels/Model10_run4.hdf5\n",
      "139/139 [==============================] - 30s 215ms/step - loss: 0.1540 - dice_coef: 0.9275 - val_loss: 0.1984 - val_dice_coef: 0.8940 - lr: 3.9063e-06\n",
      "Epoch 153/300\n",
      "139/139 [==============================] - ETA: 0s - loss: 0.1586 - dice_coef: 0.9258\n",
      "Epoch 00153: val_loss did not improve from 0.19843\n",
      "139/139 [==============================] - 29s 210ms/step - loss: 0.1586 - dice_coef: 0.9258 - val_loss: 0.2019 - val_dice_coef: 0.8954 - lr: 3.9063e-06\n",
      "Epoch 154/300\n",
      "139/139 [==============================] - ETA: 0s - loss: 0.1557 - dice_coef: 0.9229\n",
      "Epoch 00154: val_loss did not improve from 0.19843\n",
      "139/139 [==============================] - 29s 211ms/step - loss: 0.1557 - dice_coef: 0.9229 - val_loss: 0.2023 - val_dice_coef: 0.8952 - lr: 3.9063e-06\n",
      "Epoch 155/300\n",
      "139/139 [==============================] - ETA: 0s - loss: 0.1551 - dice_coef: 0.9255\n",
      "Epoch 00155: val_loss did not improve from 0.19843\n",
      "139/139 [==============================] - 29s 210ms/step - loss: 0.1551 - dice_coef: 0.9255 - val_loss: 0.2024 - val_dice_coef: 0.8954 - lr: 3.9063e-06\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 156/300\n",
      "139/139 [==============================] - ETA: 0s - loss: 0.1543 - dice_coef: 0.9266\n",
      "Epoch 00156: val_loss did not improve from 0.19843\n",
      "139/139 [==============================] - 29s 210ms/step - loss: 0.1543 - dice_coef: 0.9266 - val_loss: 0.2020 - val_dice_coef: 0.8956 - lr: 3.9063e-06\n",
      "Epoch 157/300\n",
      "139/139 [==============================] - ETA: 0s - loss: 0.1545 - dice_coef: 0.9269\n",
      "Epoch 00157: val_loss did not improve from 0.19843\n",
      "139/139 [==============================] - 29s 210ms/step - loss: 0.1545 - dice_coef: 0.9269 - val_loss: 0.2019 - val_dice_coef: 0.8952 - lr: 3.9063e-06\n",
      "Epoch 158/300\n",
      "139/139 [==============================] - ETA: 0s - loss: 0.1546 - dice_coef: 0.9265\n",
      "Epoch 00158: val_loss did not improve from 0.19843\n",
      "139/139 [==============================] - 29s 210ms/step - loss: 0.1546 - dice_coef: 0.9265 - val_loss: 0.1991 - val_dice_coef: 0.8953 - lr: 3.9063e-06\n",
      "Epoch 159/300\n",
      "139/139 [==============================] - ETA: 0s - loss: 0.1558 - dice_coef: 0.9243\n",
      "Epoch 00159: val_loss did not improve from 0.19843\n",
      "139/139 [==============================] - 29s 209ms/step - loss: 0.1558 - dice_coef: 0.9243 - val_loss: 0.2005 - val_dice_coef: 0.8956 - lr: 3.9063e-06\n",
      "Epoch 160/300\n",
      "139/139 [==============================] - ETA: 0s - loss: 0.1549 - dice_coef: 0.9262\n",
      "Epoch 00160: val_loss did not improve from 0.19843\n",
      "139/139 [==============================] - 29s 210ms/step - loss: 0.1549 - dice_coef: 0.9262 - val_loss: 0.1997 - val_dice_coef: 0.8946 - lr: 3.9063e-06\n",
      "Epoch 161/300\n",
      "139/139 [==============================] - ETA: 0s - loss: 0.1552 - dice_coef: 0.9231\n",
      "Epoch 00161: val_loss did not improve from 0.19843\n",
      "139/139 [==============================] - 29s 210ms/step - loss: 0.1552 - dice_coef: 0.9231 - val_loss: 0.1993 - val_dice_coef: 0.8952 - lr: 1.9531e-06\n",
      "Epoch 162/300\n",
      "139/139 [==============================] - ETA: 0s - loss: 0.1559 - dice_coef: 0.9257\n",
      "Epoch 00162: val_loss did not improve from 0.19843\n",
      "139/139 [==============================] - 29s 209ms/step - loss: 0.1559 - dice_coef: 0.9257 - val_loss: 0.1993 - val_dice_coef: 0.8954 - lr: 1.9531e-06\n",
      "Epoch 163/300\n",
      "139/139 [==============================] - ETA: 0s - loss: 0.1567 - dice_coef: 0.9236\n",
      "Epoch 00163: val_loss did not improve from 0.19843\n",
      "139/139 [==============================] - 29s 210ms/step - loss: 0.1567 - dice_coef: 0.9236 - val_loss: 0.2013 - val_dice_coef: 0.8955 - lr: 1.9531e-06\n",
      "Epoch 164/300\n",
      "139/139 [==============================] - ETA: 0s - loss: 0.1574 - dice_coef: 0.9252\n",
      "Epoch 00164: val_loss did not improve from 0.19843\n",
      "139/139 [==============================] - 29s 211ms/step - loss: 0.1574 - dice_coef: 0.9252 - val_loss: 0.2012 - val_dice_coef: 0.8954 - lr: 1.9531e-06\n",
      "Epoch 165/300\n",
      "139/139 [==============================] - ETA: 0s - loss: 0.1587 - dice_coef: 0.9233\n",
      "Epoch 00165: val_loss did not improve from 0.19843\n",
      "139/139 [==============================] - 29s 210ms/step - loss: 0.1587 - dice_coef: 0.9233 - val_loss: 0.2028 - val_dice_coef: 0.8955 - lr: 1.9531e-06\n",
      "Epoch 166/300\n",
      "139/139 [==============================] - ETA: 0s - loss: 0.1562 - dice_coef: 0.9240\n",
      "Epoch 00166: val_loss did not improve from 0.19843\n",
      "139/139 [==============================] - 29s 210ms/step - loss: 0.1562 - dice_coef: 0.9240 - val_loss: 0.2023 - val_dice_coef: 0.8958 - lr: 1.9531e-06\n",
      "Epoch 167/300\n",
      "139/139 [==============================] - ETA: 0s - loss: 0.1565 - dice_coef: 0.9254\n",
      "Epoch 00167: val_loss did not improve from 0.19843\n",
      "139/139 [==============================] - 29s 210ms/step - loss: 0.1565 - dice_coef: 0.9254 - val_loss: 0.2015 - val_dice_coef: 0.8956 - lr: 1.9531e-06\n",
      "Epoch 168/300\n",
      "139/139 [==============================] - ETA: 0s - loss: 0.1557 - dice_coef: 0.9255\n",
      "Epoch 00168: val_loss did not improve from 0.19843\n",
      "139/139 [==============================] - 29s 210ms/step - loss: 0.1557 - dice_coef: 0.9255 - val_loss: 0.2019 - val_dice_coef: 0.8956 - lr: 1.9531e-06\n",
      "Epoch 169/300\n",
      "139/139 [==============================] - ETA: 0s - loss: 0.1553 - dice_coef: 0.9227\n",
      "Epoch 00169: val_loss did not improve from 0.19843\n",
      "139/139 [==============================] - 29s 210ms/step - loss: 0.1553 - dice_coef: 0.9227 - val_loss: 0.2004 - val_dice_coef: 0.8957 - lr: 9.7656e-07\n",
      "Epoch 170/300\n",
      "139/139 [==============================] - ETA: 0s - loss: 0.1561 - dice_coef: 0.9257\n",
      "Epoch 00170: val_loss did not improve from 0.19843\n",
      "139/139 [==============================] - 29s 210ms/step - loss: 0.1561 - dice_coef: 0.9257 - val_loss: 0.2022 - val_dice_coef: 0.8959 - lr: 9.7656e-07\n",
      "Epoch 171/300\n",
      "139/139 [==============================] - ETA: 0s - loss: 0.1551 - dice_coef: 0.9234\n",
      "Epoch 00171: val_loss did not improve from 0.19843\n",
      "139/139 [==============================] - 29s 211ms/step - loss: 0.1551 - dice_coef: 0.9234 - val_loss: 0.2018 - val_dice_coef: 0.8959 - lr: 9.7656e-07\n",
      "Epoch 172/300\n",
      "139/139 [==============================] - ETA: 0s - loss: 0.1578 - dice_coef: 0.9226\n",
      "Epoch 00172: val_loss did not improve from 0.19843\n",
      "139/139 [==============================] - 29s 211ms/step - loss: 0.1578 - dice_coef: 0.9226 - val_loss: 0.2046 - val_dice_coef: 0.8960 - lr: 9.7656e-07\n",
      "Epoch 173/300\n",
      "139/139 [==============================] - ETA: 0s - loss: 0.1564 - dice_coef: 0.9248\n",
      "Epoch 00173: val_loss did not improve from 0.19843\n",
      "139/139 [==============================] - 29s 210ms/step - loss: 0.1564 - dice_coef: 0.9248 - val_loss: 0.2019 - val_dice_coef: 0.8959 - lr: 9.7656e-07\n",
      "Epoch 174/300\n",
      "139/139 [==============================] - ETA: 0s - loss: 0.1557 - dice_coef: 0.9248\n",
      "Epoch 00174: val_loss did not improve from 0.19843\n",
      "139/139 [==============================] - 29s 211ms/step - loss: 0.1557 - dice_coef: 0.9248 - val_loss: 0.2006 - val_dice_coef: 0.8952 - lr: 9.7656e-07\n",
      "Epoch 175/300\n",
      "139/139 [==============================] - ETA: 0s - loss: 0.1549 - dice_coef: 0.9244\n",
      "Epoch 00175: val_loss did not improve from 0.19843\n",
      "139/139 [==============================] - 29s 210ms/step - loss: 0.1549 - dice_coef: 0.9244 - val_loss: 0.2014 - val_dice_coef: 0.8957 - lr: 9.7656e-07\n",
      "Epoch 176/300\n",
      "139/139 [==============================] - ETA: 0s - loss: 0.1551 - dice_coef: 0.9268\n",
      "Epoch 00176: val_loss did not improve from 0.19843\n",
      "139/139 [==============================] - 29s 210ms/step - loss: 0.1551 - dice_coef: 0.9268 - val_loss: 0.2002 - val_dice_coef: 0.8958 - lr: 9.7656e-07\n",
      "Epoch 177/300\n",
      "139/139 [==============================] - ETA: 0s - loss: 0.1543 - dice_coef: 0.9269\n",
      "Epoch 00177: val_loss did not improve from 0.19843\n",
      "139/139 [==============================] - 29s 210ms/step - loss: 0.1543 - dice_coef: 0.9269 - val_loss: 0.2001 - val_dice_coef: 0.8954 - lr: 4.8828e-07\n",
      "Epoch 178/300\n",
      "139/139 [==============================] - ETA: 0s - loss: 0.1562 - dice_coef: 0.9267\n",
      "Epoch 00178: val_loss did not improve from 0.19843\n",
      "139/139 [==============================] - 29s 210ms/step - loss: 0.1562 - dice_coef: 0.9267 - val_loss: 0.2025 - val_dice_coef: 0.8959 - lr: 4.8828e-07\n",
      "Epoch 179/300\n",
      "139/139 [==============================] - ETA: 0s - loss: 0.1559 - dice_coef: 0.9228\n",
      "Epoch 00179: val_loss did not improve from 0.19843\n",
      "139/139 [==============================] - 29s 210ms/step - loss: 0.1559 - dice_coef: 0.9228 - val_loss: 0.2010 - val_dice_coef: 0.8958 - lr: 4.8828e-07\n",
      "Epoch 180/300\n",
      "139/139 [==============================] - ETA: 0s - loss: 0.1540 - dice_coef: 0.9281\n",
      "Epoch 00180: val_loss did not improve from 0.19843\n",
      "139/139 [==============================] - 29s 210ms/step - loss: 0.1540 - dice_coef: 0.9281 - val_loss: 0.1997 - val_dice_coef: 0.8958 - lr: 4.8828e-07\n",
      "Epoch 181/300\n",
      "139/139 [==============================] - ETA: 0s - loss: 0.1542 - dice_coef: 0.9260\n",
      "Epoch 00181: val_loss did not improve from 0.19843\n",
      "139/139 [==============================] - 29s 209ms/step - loss: 0.1542 - dice_coef: 0.9260 - val_loss: 0.2020 - val_dice_coef: 0.8961 - lr: 4.8828e-07\n",
      "Epoch 182/300\n",
      "139/139 [==============================] - ETA: 0s - loss: 0.1552 - dice_coef: 0.9253\n",
      "Epoch 00182: val_loss did not improve from 0.19843\n",
      "139/139 [==============================] - 29s 209ms/step - loss: 0.1552 - dice_coef: 0.9253 - val_loss: 0.2036 - val_dice_coef: 0.8960 - lr: 4.8828e-07\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 183/300\n",
      "139/139 [==============================] - ETA: 0s - loss: 0.1546 - dice_coef: 0.9271\n",
      "Epoch 00183: val_loss did not improve from 0.19843\n",
      "139/139 [==============================] - 29s 211ms/step - loss: 0.1546 - dice_coef: 0.9271 - val_loss: 0.2030 - val_dice_coef: 0.8960 - lr: 4.8828e-07\n",
      "Epoch 184/300\n",
      "139/139 [==============================] - ETA: 0s - loss: 0.1549 - dice_coef: 0.9250\n",
      "Epoch 00184: val_loss did not improve from 0.19843\n",
      "139/139 [==============================] - 29s 212ms/step - loss: 0.1549 - dice_coef: 0.9250 - val_loss: 0.2012 - val_dice_coef: 0.8960 - lr: 4.8828e-07\n",
      "Epoch 185/300\n",
      "139/139 [==============================] - ETA: 0s - loss: 0.1538 - dice_coef: 0.9261\n",
      "Epoch 00185: val_loss did not improve from 0.19843\n",
      "139/139 [==============================] - 29s 209ms/step - loss: 0.1538 - dice_coef: 0.9261 - val_loss: 0.2011 - val_dice_coef: 0.8959 - lr: 2.4414e-07\n",
      "Epoch 186/300\n",
      "139/139 [==============================] - ETA: 0s - loss: 0.1546 - dice_coef: 0.9249\n",
      "Epoch 00186: val_loss did not improve from 0.19843\n",
      "139/139 [==============================] - 30s 212ms/step - loss: 0.1546 - dice_coef: 0.9249 - val_loss: 0.2001 - val_dice_coef: 0.8956 - lr: 2.4414e-07\n",
      "Epoch 187/300\n",
      "139/139 [==============================] - ETA: 0s - loss: 0.1559 - dice_coef: 0.9263\n",
      "Epoch 00187: val_loss did not improve from 0.19843\n",
      "139/139 [==============================] - 29s 210ms/step - loss: 0.1559 - dice_coef: 0.9263 - val_loss: 0.2015 - val_dice_coef: 0.8958 - lr: 2.4414e-07\n",
      "Epoch 188/300\n",
      "139/139 [==============================] - ETA: 0s - loss: 0.1550 - dice_coef: 0.9252\n",
      "Epoch 00188: val_loss did not improve from 0.19843\n",
      "139/139 [==============================] - 29s 209ms/step - loss: 0.1550 - dice_coef: 0.9252 - val_loss: 0.2007 - val_dice_coef: 0.8958 - lr: 2.4414e-07\n",
      "Epoch 189/300\n",
      "139/139 [==============================] - ETA: 0s - loss: 0.1546 - dice_coef: 0.9275\n",
      "Epoch 00189: val_loss did not improve from 0.19843\n",
      "139/139 [==============================] - 29s 211ms/step - loss: 0.1546 - dice_coef: 0.9275 - val_loss: 0.2013 - val_dice_coef: 0.8958 - lr: 2.4414e-07\n",
      "Epoch 190/300\n",
      "139/139 [==============================] - ETA: 0s - loss: 0.1547 - dice_coef: 0.9249\n",
      "Epoch 00190: val_loss did not improve from 0.19843\n",
      "139/139 [==============================] - 29s 209ms/step - loss: 0.1547 - dice_coef: 0.9249 - val_loss: 0.2027 - val_dice_coef: 0.8961 - lr: 2.4414e-07\n",
      "Epoch 191/300\n",
      "139/139 [==============================] - ETA: 0s - loss: 0.1547 - dice_coef: 0.9244\n",
      "Epoch 00191: val_loss did not improve from 0.19843\n",
      "139/139 [==============================] - 29s 209ms/step - loss: 0.1547 - dice_coef: 0.9244 - val_loss: 0.2007 - val_dice_coef: 0.8960 - lr: 2.4414e-07\n",
      "Epoch 192/300\n",
      "139/139 [==============================] - ETA: 0s - loss: 0.1549 - dice_coef: 0.9259\n",
      "Epoch 00192: val_loss did not improve from 0.19843\n",
      "139/139 [==============================] - 29s 211ms/step - loss: 0.1549 - dice_coef: 0.9259 - val_loss: 0.1990 - val_dice_coef: 0.8956 - lr: 2.4414e-07\n",
      "Epoch 193/300\n",
      "139/139 [==============================] - ETA: 0s - loss: 0.1538 - dice_coef: 0.9262\n",
      "Epoch 00193: val_loss did not improve from 0.19843\n",
      "139/139 [==============================] - 29s 211ms/step - loss: 0.1538 - dice_coef: 0.9262 - val_loss: 0.1994 - val_dice_coef: 0.8956 - lr: 1.2207e-07\n",
      "Epoch 194/300\n",
      "139/139 [==============================] - ETA: 0s - loss: 0.1558 - dice_coef: 0.9246\n",
      "Epoch 00194: val_loss did not improve from 0.19843\n",
      "139/139 [==============================] - 29s 210ms/step - loss: 0.1558 - dice_coef: 0.9246 - val_loss: 0.2023 - val_dice_coef: 0.8961 - lr: 1.2207e-07\n",
      "Epoch 195/300\n",
      "139/139 [==============================] - ETA: 0s - loss: 0.1547 - dice_coef: 0.9262\n",
      "Epoch 00195: val_loss did not improve from 0.19843\n",
      "139/139 [==============================] - 29s 211ms/step - loss: 0.1547 - dice_coef: 0.9262 - val_loss: 0.1992 - val_dice_coef: 0.8956 - lr: 1.2207e-07\n",
      "Epoch 196/300\n",
      "139/139 [==============================] - ETA: 0s - loss: 0.1558 - dice_coef: 0.9261\n",
      "Epoch 00196: val_loss did not improve from 0.19843\n",
      "139/139 [==============================] - 29s 212ms/step - loss: 0.1558 - dice_coef: 0.9261 - val_loss: 0.2026 - val_dice_coef: 0.8962 - lr: 1.2207e-07\n",
      "Epoch 197/300\n",
      "139/139 [==============================] - ETA: 0s - loss: 0.1549 - dice_coef: 0.9247\n",
      "Epoch 00197: val_loss did not improve from 0.19843\n",
      "139/139 [==============================] - 29s 210ms/step - loss: 0.1549 - dice_coef: 0.9247 - val_loss: 0.2002 - val_dice_coef: 0.8959 - lr: 1.2207e-07\n",
      "Epoch 198/300\n",
      "139/139 [==============================] - ETA: 0s - loss: 0.1554 - dice_coef: 0.9229\n",
      "Epoch 00198: val_loss did not improve from 0.19843\n",
      "139/139 [==============================] - 29s 210ms/step - loss: 0.1554 - dice_coef: 0.9229 - val_loss: 0.2034 - val_dice_coef: 0.8961 - lr: 1.2207e-07\n",
      "Epoch 199/300\n",
      "139/139 [==============================] - ETA: 0s - loss: 0.1564 - dice_coef: 0.9254\n",
      "Epoch 00199: val_loss did not improve from 0.19843\n",
      "139/139 [==============================] - 29s 211ms/step - loss: 0.1564 - dice_coef: 0.9254 - val_loss: 0.2036 - val_dice_coef: 0.8961 - lr: 1.2207e-07\n",
      "Epoch 200/300\n",
      "139/139 [==============================] - ETA: 0s - loss: 0.1568 - dice_coef: 0.9258\n",
      "Epoch 00200: val_loss did not improve from 0.19843\n",
      "139/139 [==============================] - 29s 210ms/step - loss: 0.1568 - dice_coef: 0.9258 - val_loss: 0.2019 - val_dice_coef: 0.8960 - lr: 1.2207e-07\n",
      "Epoch 201/300\n",
      "139/139 [==============================] - ETA: 0s - loss: 0.1548 - dice_coef: 0.9238\n",
      "Epoch 00201: val_loss did not improve from 0.19843\n",
      "139/139 [==============================] - 29s 210ms/step - loss: 0.1548 - dice_coef: 0.9238 - val_loss: 0.2003 - val_dice_coef: 0.8958 - lr: 6.1035e-08\n",
      "Epoch 202/300\n",
      "139/139 [==============================] - ETA: 0s - loss: 0.1545 - dice_coef: 0.9265\n",
      "Epoch 00202: val_loss did not improve from 0.19843\n",
      "139/139 [==============================] - 29s 211ms/step - loss: 0.1545 - dice_coef: 0.9265 - val_loss: 0.2016 - val_dice_coef: 0.8959 - lr: 6.1035e-08\n"
     ]
    }
   ],
   "source": [
    "model_name = 'Model10_run4 (300 Epochs)'\n",
    "# Helper.telegram_bot_sendtext(f'Model {model_name} started training')\n",
    "\n",
    "history = model.fit(train_array,\n",
    "                    train_mask_array,\n",
    "                    batch_size=8,\n",
    "                    epochs=300,\n",
    "                    verbose=1,\n",
    "                    shuffle = True,\n",
    "                    validation_data=(valid_array, valid_mask_array),\n",
    "                    callbacks=my_callbacks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9514ab95",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ec1de562",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#plot the training and validation IoU and loss at each epoch\n",
    "loss = history.history['loss']\n",
    "val_loss = history.history['val_loss']\n",
    "epochs = range(1, len(loss) + 1)\n",
    "plt.plot(epochs, loss, 'y', label='Training loss')\n",
    "plt.plot(epochs, val_loss, 'r', label='Validation loss')\n",
    "plt.title('Training and validation loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "acc = history.history['dice_coef']\n",
    "val_acc = history.history['val_dice_coef']\n",
    "\n",
    "plt.plot(epochs, acc, 'y', label='Training Dice')\n",
    "plt.plot(epochs, val_acc, 'r', label='Validation Dice')\n",
    "plt.title('Training and validation Dice')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Dice')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4813f347",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
